{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlexanderCardarasUCSC/EEG-Decoding-to-Images/blob/main/CSE_247_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install packages for running on PRP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install keras\n",
    "!pip -q install google-cloud-storage\n",
    "!pip -q install google-api-python-client oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient import discovery\n",
    "from httplib2 import Http\n",
    "import oauth2client\n",
    "from oauth2client import file, client, tools\n",
    "\n",
    "obj = lambda: None\n",
    "lmao = {\"auth_host_name\":'localhost', 'noauth_local_webserver':'store_true', \n",
    "        'auth_host_port':[8080, 8090], 'logging_level':'ERROR'}\n",
    "for k, v in lmao.items():\n",
    "    setattr(obj, k, v)\n",
    "    \n",
    "# authorization boilerplate code\n",
    "SCOPES = 'https://www.googleapis.com/auth/drive.readonly'\n",
    "store = file.Storage('token.json')\n",
    "creds = store.get()\n",
    "# The following will give you a link if token.json does not exist, the link allows the\n",
    "# user to give this app permission\n",
    "if not creds or creds.invalid:\n",
    "    flow = client.flow_from_clientsecrets('client_secret.json', SCOPES)\n",
    "    creds = tools.run_flow(flow, store, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "DRIVE = discovery.build('drive', 'v3', http=creds.authorize(Http()))\n",
    "\n",
    "def download_file(file_id, filename):\n",
    "    # if you get the shareable link, the link contains this id, replace the file_id below\n",
    "    request = DRIVE.files().get_media(fileId=file_id)\n",
    "    # replace the filename and extension in the first field below\n",
    "    fh = io.FileIO(filename, mode='w')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Image data\n",
    "!mkdir -p data/images\n",
    "!wget -q https://raw.githubusercontent.com/AlexanderCardarasUCSC/EEG-Decoding-to-Images/main/image_utils.py \n",
    "download_file(\"1-2WFgtBn5WdZCnDlJkwaswTTrRtVNrS0\", \"images.npy\")\n",
    "download_file(\"1Lijm9A5Kq4EMub5jO0Bzz3c2VJLm-WH6\", \"image_labels.npy\")\n",
    "\n",
    "!sudo mv images.npy data/images\n",
    "!sudo mv image_labels.npy data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUWoxMgn9TOB"
   },
   "source": [
    "# EEG decoding to images - Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7ZhR8a4LkDU"
   },
   "source": [
    "**Complete the following steps before running the cells below**\n",
    "\n",
    "1.\n",
    "Change runtime to GPU \n",
    "\n",
    ">Runtime > Change runtime type > GPU\n",
    "\n",
    "2.\n",
    "For each of the following link, add a Google Drive shortcut to your Drive root folder(My Drive):\n",
    "\n",
    ">https://drive.google.com/drive/folders/1z8wcew5R7GCeu5s7SSsWw8bUcx1urVmk?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JenuAt1ALqvt"
   },
   "source": [
    "**Mount drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVsVV6U5LtiA",
    "outputId": "0b74228a-c9aa-4da6-ceb2-fd22d1bf1649"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZRqXBBuOR8l"
   },
   "source": [
    "**Run the following cells to load eeg and/or image data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsBmC9XVLTvz"
   },
   "source": [
    "#### Load eeg data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNQaYZ5HyCa7"
   },
   "source": [
    "**Uncomment one of the following blocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4xLacydzSQ4"
   },
   "source": [
    "*5-95Hz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRqC6iVFyCvW"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /content/data/eeg\n",
    "# !cp /content/gdrive/MyDrive/EEG2Image/eeg_5_95_std.pth /content/data/eeg/\n",
    "# eeg_path = \"/content/data/eeg/eeg_5_95_std.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMU8iY61zYB5"
   },
   "source": [
    "14-70Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aru_3JtzNI0"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /content/data/eeg\n",
    "# !cp /content/gdrive/MyDrive/EEG2Image/eeg_14_70_std.pth /content/data/eeg/\n",
    "# eeg_path = \"/content/data/eeg/eeg_14_70_std.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNDc6jwSzYjf"
   },
   "source": [
    "*55-95Hz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nrNCoFSzNbX"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/data/eeg\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/eeg_54_95_std.pth /content/data/eeg/\n",
    "eeg_path = \"/content/data/eeg/eeg_54_95_std.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pzsz5CBrLJQJ"
   },
   "source": [
    "#### Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwepmfU39z3E"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/data/images\n",
    "!wget -q https://raw.githubusercontent.com/AlexanderCardarasUCSC/EEG-Decoding-to-Images/main/image_utils.py /content\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/data/images.npy /content/data/images\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/data/image_labels.npy /content/data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imTyI_FFFpvj"
   },
   "source": [
    "**Example load image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjkR7Orw-ovS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path_root=\"/content/\"):\n",
    "  images = np.load(path_root+\"data/images/images.npy\")\n",
    "  image_labels = np.load(path_root+\"data/images/image_labels.npy\")\n",
    "  return images, image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Dcw0IP7AH9b"
   },
   "outputs": [],
   "source": [
    "images, labels = image_utils.load_images(path_root=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "gBGRRcUB_Aci",
    "outputId": "bced421a-c4b9-4072-e548-4f4ca9d32c4e"
   },
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ7shO6sIN5l"
   },
   "source": [
    "# Baseline GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKBx7cByIds4"
   },
   "source": [
    "**Complete the following steps before running the cells below**\n",
    "\n",
    "1. Follow directions in the Load data section to load image data\n",
    "\n",
    ">Run the cells under the header **Load image data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34oS7RnrPajK"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2hmYEUh2q5R"
   },
   "source": [
    "### Jason's AC-GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZLFNBTbdkrK"
   },
   "outputs": [],
   "source": [
    "# example of fitting an auxiliary classifier gan (ac-gan) on fashion mnsit\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXllC6Y0PiRj"
   },
   "source": [
    "**Discriminator architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUwEvQbeFk-2"
   },
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(64,64,3), n_classes=10):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=in_shape)\n",
    "\t# downsample to 14x14\n",
    "\tfe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# normal\n",
    "\tfe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "\tfe = BatchNormalization()(fe)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# downsample to 7x7\n",
    "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "\tfe = BatchNormalization()(fe)\n",
    "\tfe = LeakyReLU( alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# normal\n",
    "\tfe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "\tfe = BatchNormalization()(fe)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# flatten feature maps\n",
    "\tfe = Flatten()(fe)\n",
    "\t# real/fake output\n",
    "\tout1 = Dense(1, activation='sigmoid')(fe)\n",
    "\t# class label output\n",
    "\tout2 = Dense(n_classes, activation='softmax')(fe)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, [out1, out2])\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9yPgBfEPsdM"
   },
   "source": [
    "**Generator architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtkgzUM3PuXp"
   },
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes=10):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# label input\n",
    "\tin_label = Input(shape=(1,))\n",
    "\t# embedding for categorical input\n",
    "\t# li = Embedding(n_classes, 50)(in_label)\n",
    "\tli = Embedding(n_classes, 50)(in_label)\n",
    "\t# linear multiplication\n",
    "\t# n_nodes = 7 * 7\n",
    "\tn_nodes = 16 * 16 * 3\n",
    "\tli = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "\t# reshape to additional channel\n",
    "\t# li = Reshape((7, 7, 1))(li)\n",
    "\tli = Reshape((16, 16, 3))(li)\n",
    "\t# image generator input\n",
    "\tin_lat = Input(shape=(latent_dim,))\n",
    "\t# foundation for 7x7 image\n",
    "\t# n_nodes = 384 * 7 * 7\n",
    "\tn_nodes = 384 * 16 * 16\n",
    "\tgen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "\tgen = Activation('relu')(gen)\n",
    "\t# gen = Reshape((7, 7, 384))(gen)\n",
    "\tgen = Reshape((16, 16, 384))(gen)\n",
    "\t# merge image gen and label input\n",
    "\tmerge = Concatenate()([gen, li])\n",
    "\t# upsample to 14x14\n",
    "\tgen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "\tgen = BatchNormalization()(gen)\n",
    "\tgen = Activation('relu')(gen)\n",
    "\t# upsample to 28x28\n",
    "\t# gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "\tgen = Conv2DTranspose(3, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "\tout_layer = Activation('tanh')(gen)\n",
    "\t# define model\n",
    "\tmodel = Model([in_lat, in_label], out_layer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbtLd_QgP6T9"
   },
   "source": [
    "**GAN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUP86d-eP6jc"
   },
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tfor layer in d_model.layers:\n",
    "\t\tif not isinstance(layer, BatchNormalization):\n",
    "\t\t\tlayer.trainable = False\n",
    "\t# connect the outputs of the generator to the inputs of the discriminator\n",
    "\tgan_output = d_model(g_model.output)\n",
    "\t# define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "\tmodel = Model(g_model.input, gan_output)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.2)\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHNh232fQxaC"
   },
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t29JjwvnQnGz"
   },
   "outputs": [],
   "source": [
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "# splits image data into training and test sets(currently no randomization)\n",
    "def new_split_data(images, labels, train=0.8, data_per_class=900):\n",
    "  trainX,trainY,testX,testY = [],[],[],[]\n",
    "  for i in range(images.shape[0]):\n",
    "    chunk = i % data_per_class\n",
    "    if chunk < data_per_class*train:\n",
    "      trainX.append(images[i])\n",
    "      trainY.append(labels[i])\n",
    "    else:\n",
    "      testX.append(images[i])\n",
    "      testY.append(labels[i])\n",
    "\n",
    "  return (np.asarray(trainX), np.asarray(trainY)), (np.asarray(testX), np.asarray(testY))\n",
    "\n",
    "# load images\n",
    "def load_real_samples(n_classes):\n",
    "  # load dataset\n",
    "  images, labels = image_utils.load_images(path_root=\"\")\n",
    "  images, labels = trim_classes(images, labels, n_classes)\n",
    "  (trainX, trainY), (testX, testY) = new_split_data(images, labels)\n",
    "\t# expand to 3d, e.g. add channels\n",
    "  X = expand_dims(trainX, axis=-1)\n",
    "\t# convert from ints to floats\n",
    "  X = X.astype('float32')\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "  X = (X - 127.5) / 127.5\n",
    "  print(X.shape, trainY.shape)\n",
    "  return [X, trainY]\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# split into images and labels\n",
    "\timages, labels = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, images.shape[0], n_samples)\n",
    "\t# select images and labels\n",
    "\tX, labels = images[ix], labels[ix]\n",
    "\t# generate class labels\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn [X, labels], y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes):\n",
    "  # generate points in the latent space\n",
    "  x_input = randn(latent_dim * n_samples)\n",
    "  # reshape into a batch of inputs for the network\n",
    "  z_input = x_input.reshape(n_samples, latent_dim)\n",
    "  # generate labels\n",
    "  labels = randint(0, n_classes, n_samples)\n",
    "  return [z_input, labels]\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples, n_classes)\n",
    "\t# predict outputs\n",
    "\timages = generator.predict([z_input, labels_input])\n",
    "\t# create class labels\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn [images, labels_input], y\n",
    "\n",
    "def generate_fake_class_sample(generator, class_label, latent_dim, n_samples):\n",
    "  # generate points in the latent space\n",
    "  x_input = randn(latent_dim * n_samples)\n",
    "  # reshape into a batch of inputs for the network\n",
    "  z_input = x_input.reshape(n_samples, latent_dim)\n",
    "  labels_input = np.asarray([class_label for i in range(n_samples)])\n",
    "  # predict outputs\n",
    "  images = generator.predict([z_input, labels_input])\n",
    "  # create class labels\n",
    "  y = zeros((n_samples, 1))\n",
    "  return [images, labels_input], y\n",
    " \n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "\t# prepare fake examples\n",
    "\t[X, _], _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# scale from [-1,1] to [0,1]\n",
    "\tX = (X + 1) / 2.0\n",
    "\t# plot images\n",
    "\tfor i in range(100):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(10, 10, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\t# pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "\t\tpyplot.imshow(X[i, :, :])\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'generated_plot_%04d.png' % (step+1)\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%04d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
    " \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_classes=10):\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# calculate the size of half a batch of samples\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# get randomly selected 'real' samples\n",
    "\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t# update discriminator model weights\n",
    "\t\t_,d_r1,d_r2 = d_model.train_on_batch(X_real, [y_real, labels_real])\n",
    "\t\t# generate 'fake' examples\n",
    "\t\t[X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator model weights\n",
    "\t\t_,d_f,d_f2 = d_model.train_on_batch(X_fake, [y_fake, labels_fake])\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\t[z_input, z_labels] = generate_latent_points(latent_dim, n_batch, n_classes)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\t_,g_1,g_2 = gan_model.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
    "\t\t# summarize loss on this batch\n",
    "\t\tprint('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % (i+1, d_r1,d_r2, d_f,d_f2, g_1,g_2))\n",
    "\t\t# evaluate the model performance every 'epoch'\n",
    "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-HQexL2SEaB"
   },
   "source": [
    "**Train GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTVMnDL0REYL"
   },
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "# latent_dim = 100\n",
    "latent_dim = 100\n",
    "n_classes=1\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator(n_classes=n_classes)\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim, n_classes)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load image data\n",
    "dataset = load_real_samples(n_classes)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=100, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVFroeQlgCZ1"
   },
   "outputs": [],
   "source": [
    "[X, _], _ = generate_fake_class_sample(generator, class_label=0, latent_dim=latent_dim, n_samples=16)\n",
    "# scale from [-1,1] to [0,1]\n",
    "X = (X + 1) / 2.0\n",
    "\n",
    "f, axs = plt.subplots(4,4,figsize=(15,15))\n",
    "# plot images\n",
    "for i in range(16):\n",
    "  axs[i//4,i%4].imshow(X[i])\n",
    "  axs[i//4,i%4].axis(\"off\")\n",
    "f.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHRHGSU2MOHA"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(generator, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(discriminator, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jason RGB GAN + DC GAN + GAN Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "\n",
    "https://towardsdatascience.com/gan-ways-to-improve-gan-performance-acf37f9f59b\n",
    "\n",
    "More Details\n",
    "\n",
    "https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "https://sthalles.github.io/advanced_gans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir images\n",
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a dcgan on cifar10\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.cifar10 import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "import image_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator64(in_shape=(64,64,3)):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    model.add(Conv2D(128, (4,4), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.00005, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator64(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def trim_classes(images, labels, n_classes):\n",
    "    return images[0:900*n_classes], labels[0:900*n_classes]\n",
    "\n",
    "# load and prepare imnet64 training images\n",
    "def load_real_samples64(n_classes=1):\n",
    "    # load imagenet64 dataset\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, n_classes)\n",
    "    (trainX, _), (_, _) = image_utils.split_data(images, labels, train=1)\n",
    "\n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# create and save a plot of generated images\n",
    "def save_plot(examples, epoch, n=6):\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "    # plot the result\n",
    "    f, axs = pyplot.subplots(n,n,figsize=((n*n)-1,(n*n)-1))\n",
    "    # plot images\n",
    "    for i in range(n*n):\n",
    "        axs[i//n,i%n].imshow(examples[i])\n",
    "        axs[i//n,i%n].axis(\"off\")\n",
    "    # save plot to file\n",
    "    filename = 'images/generated_plot_e%03d.png' % (epoch+1)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "\n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'models/generator_model_%03d.h5' % (epoch+1)\n",
    "    g_model.save(filename)\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1000, n_batch=100):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "                (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "            \n",
    "def augment_images(images):\n",
    "    augmented = []\n",
    "    for image in images:\n",
    "        flipped = tf.image.flip_left_right(image)\n",
    "        \n",
    "        augmented.append(image)\n",
    "        augmented.append(flipped)\n",
    "        \n",
    "    return np.asarray(augmented)\n",
    "\n",
    "\n",
    "n_classes = 1\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator64()\n",
    "# create the generator\n",
    "g_model = define_generator64(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples64(n_classes)\n",
    "dataset = augment_images(dataset)\n",
    "\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the generator model and generating images\n",
    "from keras.models import load_model\n",
    "from numpy.random import randn\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# plot the generated images\n",
    "def create_plot(examples, n):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(examples[i, :, :])\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# load model\n",
    "model = load_model('generator_model_920.h5')\n",
    "# generate images\n",
    "latent_points = generate_latent_points(100, 16)\n",
    "# generate images\n",
    "X = model.predict(latent_points)\n",
    "# scale from [-1,1] to [0,1]\n",
    "X = (X + 1) / 2.0\n",
    "# plot the result\n",
    "f, axs = pyplot.subplots(4,4,figsize=(15,15))\n",
    "# plot images\n",
    "for i in range(16):\n",
    "    axs[i//4,i%4].imshow(X[i])\n",
    "    axs[i//4,i%4].axis(\"off\")\n",
    "f.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVLLUIjU20wy"
   },
   "source": [
    "### ThoughtVIZ's  Baseline AC-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YwfGtq24GBZ"
   },
   "source": [
    "**Download model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ogKkn3n3Wus"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ThoughtViz/models\n",
    "!wget -q https://raw.githubusercontent.com/ptirupat/ThoughtViz/master/training/models/ac_gan.py -P ThoughtViz/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U47otpX3_V5"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-aC8Z_H2_OL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import image_utils\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "from ThoughtViz.models import ac_gan\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3e-xtTX4t-",
    "outputId": "0d2d4a97-8284-47cb-ad4b-fdfc7132d01c"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "MODEL_DIR = '/tmp/imagenet'\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "softmax = None\n",
    "\n",
    "\n",
    "# Call this function with list of images. Each of elements should be a\n",
    "# numpy array with values ranging from 0 to 255.\n",
    "def get_inception_score(images, splits=10):\n",
    "  assert(type(images) == list)\n",
    "  assert(type(images[0]) == np.ndarray)\n",
    "  assert(len(images[0].shape) == 3)\n",
    "  assert(np.max(images[0]) > 10)\n",
    "  assert(np.min(images[0]) >= 0.0)\n",
    "  inps = []\n",
    "  for img in images:\n",
    "    img = img.astype(np.float32)\n",
    "    inps.append(np.expand_dims(img, 0))\n",
    "  bs = 1\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    preds = []\n",
    "    n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n",
    "    for i in range(n_batches):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n",
    "        inp = np.concatenate(inp, 0)\n",
    "        pred = sess.run(softmax, {'ExpandDims:0': inp})\n",
    "        preds.append(pred)\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "      part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
    "      kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "      kl = np.mean(np.sum(kl, 1))\n",
    "      scores.append(np.exp(kl))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "# This function is called automatically.\n",
    "def _init_inception():\n",
    "  global softmax\n",
    "  if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(MODEL_DIR, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  tarfile.open(filepath, 'r:gz').extractall(MODEL_DIR)\n",
    "  with tf.compat.v1.gfile.FastGFile(os.path.join(\n",
    "      MODEL_DIR, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "  # Works with an arbitrary minibatch size.\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    pool3 = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "    ops = pool3.graph.get_operations()\n",
    "    for op_idx, op in enumerate(ops):\n",
    "        for o in op.outputs:\n",
    "            shape = o.get_shape()\n",
    "            shape = [s for s in shape]\n",
    "            new_shape = []\n",
    "            for j, s in enumerate(shape):\n",
    "                if s == 1 and j == 0:\n",
    "                    new_shape.append(None)\n",
    "                else:\n",
    "                    new_shape.append(s)\n",
    "            o.set_shape(tf.TensorShape(new_shape))\n",
    "    w = sess.graph.get_operation_by_name(\"softmax/logits/MatMul\").inputs[1]\n",
    "    logits = tf.matmul(tf.squeeze(pool3, [1, 2]), w)\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "if softmax is None:\n",
    "  _init_inception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrAlxWDP4QuZ"
   },
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKNRqAFF4TEe"
   },
   "outputs": [],
   "source": [
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "def combine_rgb_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1], 3),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1], :] = img[:, :, :]\n",
    "    return image\n",
    "\n",
    "def load_data(num_classes):\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, num_classes)\n",
    "    (x_train, y_train), (x_test, y_test) = image_utils.split_data(images, labels)\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "    x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def train_gan(input_noise_dim, batch_size, epochs, model_save_dir, output_dir, num_classes):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = load_data(num_classes)\n",
    "    print(y_train.shape)\n",
    "    adam_lr = 0.001\n",
    "    adam_beta_1 = 0.5\n",
    "\n",
    "    d = ac_gan.discriminator_model_rgb((64,64), num_classes)\n",
    "    d_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "    d.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=d_optim)\n",
    "    d.trainable = True\n",
    "\n",
    "    g = ac_gan.generator_model_rgb(input_noise_dim + num_classes)\n",
    "    g_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "    g.compile(loss='categorical_crossentropy', optimizer=g_optim)\n",
    "\n",
    "    d_on_g = ac_gan.generator_containing_discriminator(input_noise_dim + num_classes, g, d)\n",
    "    d_on_g.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=g_optim)\n",
    "\n",
    "    g.summary()\n",
    "    d.summary()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is \", epoch)\n",
    "\n",
    "        print(\"Number of batches\", int(x_train.shape[0]/batch_size))\n",
    "\n",
    "        for index in range(int(x_train.shape[0]/batch_size)):\n",
    "            # generate noise from a normal distribution\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "            random_labels = [randint(0, 9) for i in range(batch_size)]\n",
    "\n",
    "            one_hot_vectors = [to_categorical(label, 10) for label in random_labels]\n",
    "\n",
    "            conditioned_noise = []\n",
    "            for i in range(batch_size):\n",
    "                conditioned_noise.append(np.append(noise[i], one_hot_vectors[i]))\n",
    "            conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "            # get real images and corresponding labels\n",
    "            real_images = x_train[index * batch_size:(index + 1) * batch_size]\n",
    "            real_labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            # generate fake images using the generator\n",
    "            generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "            # discriminator loss of real images\n",
    "            d_loss_real = d.train_on_batch(real_images, [np.array([1] * batch_size), np.array(real_labels)])\n",
    "            # discriminator loss of fake images\n",
    "            d_loss_fake = d.train_on_batch(generated_images, [np.array([0] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
    "            d_loss = (d_loss_fake[0] + d_loss_real[0]) * 0.5\n",
    "\n",
    "            # save generated images at intermediate stages of training\n",
    "            if index % 250 == 0:\n",
    "                image = combine_rgb_images(generated_images)\n",
    "                image = image * 255.0\n",
    "                img_save_path = os.path.join(output_dir, str(epoch) + \"_g_\" + str(index) + \".png\")\n",
    "                Image.fromarray(image.astype(np.uint8)).save(img_save_path)\n",
    "\n",
    "            d.trainable = False\n",
    "            # generator loss\n",
    "            g_loss = d_on_g.train_on_batch(conditioned_noise, [np.array([1] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
    "            d.trainable = True\n",
    "\n",
    "        # test_image_count = 50000\n",
    "        test_image_count = x_train.shape[0]\n",
    "        test_noise = np.random.uniform(-1, 1, (test_image_count, input_noise_dim))\n",
    "        test_labels = [randint(0, 9) for i in range(test_image_count)]\n",
    "        one_hot_vectors_test = [to_categorical(label, 10) for label in test_labels]\n",
    "\n",
    "        conditioned_noise_test = []\n",
    "        for i in range(test_image_count):\n",
    "            conditioned_noise_test.append(np.append(test_noise[i], one_hot_vectors_test[i]))\n",
    "        conditioned_noise_test = np.array(conditioned_noise_test)\n",
    "\n",
    "        test_images = g.predict(conditioned_noise_test, verbose=0)\n",
    "        test_images = test_images * 255.0\n",
    "        \n",
    "        if epoch % 50 == 0:        \n",
    "            inception_score = get_inception_score([test_image for test_image in test_images], splits=10)\n",
    "\n",
    "        print(\"Epoch %d d_loss : %f\" % (epoch, d_loss))\n",
    "        print(\"Epoch %d g_loss : %f\" % (epoch, g_loss[0]))\n",
    "        print(\"Epoch %d inception_score : %f\" % (epoch, inception_score[0]))\n",
    "\n",
    "        # save generator and discriminator models along with the weights\n",
    "        g.save(os.path.join(model_save_dir, 'generator_' + str(epoch)), overwrite=True, include_optimizer=True)\n",
    "        d.save(os.path.join(model_save_dir, 'discriminator_' + str(epoch)), overwrite=True, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqtGoD-g4VtZ"
   },
   "source": [
    "**Train GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_5zBRwzZ4cEw",
    "outputId": "456f933e-87a8-404c-969c-ba8fc3a32341"
   },
   "outputs": [],
   "source": [
    "batch_size =50\n",
    "run_id = 1\n",
    "n_epochs = 500\n",
    "input_dim = 100\n",
    "n_classes=10\n",
    "\n",
    "model_save_dir = os.path.join('./saved_models/baseline_acgan/', 'run_' + str(run_id))\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "output_dir = os.path.join('./outputs/baseline_acgan/', 'run_' + str(run_id))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train_gan(input_dim, batch_size, n_epochs, model_save_dir, output_dir, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imWPfhDd9pca"
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "def resize_images(images, size=(28,28)):\n",
    "  temp = []\n",
    "  for img in images:\n",
    "    temp.append(np.array(Image.fromarray(img).resize(size, Image.ANTIALIAS)))\n",
    "  return np.asarray(temp)\n",
    "\n",
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRZLkkFj9fDJ",
    "outputId": "7e6e606f-22d7-4a21-e1ec-3a09f61b7490"
   },
   "outputs": [],
   "source": [
    "images, labels = image_utils.load_images()\n",
    "images = rgb2gray(images)\n",
    "print(images.shape)\n",
    "images = resize_images(images)\n",
    "print(images.shape)\n",
    "(x_train, y_train), (x_test, y_test) = image_utils.split_data(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXagMgJP5RqG"
   },
   "source": [
    "**Download Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQf2da6q5RIY"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/models/ThoughtViz\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/generator_100/saved_model.pb  /content/models/ThoughtViz/\n",
    "!mv /content/models/ThoughtViz/saved_model.pb /content/models/ThoughtViz/generator.pb\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/discriminator_100/saved_model.pb  /content/models/ThoughtViz/\n",
    "!mv /content/models/ThoughtViz/saved_model.pb /content/models/ThoughtViz/discriminator.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AlRT0ZE9QO4"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/models/ThoughtViz\n",
    "!cp -r /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/generator_100 /content/models/ThoughtViz/\n",
    "!cp -r /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/discriminator_100 /content/models/ThoughtViz/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J8CDTsq6mtU"
   },
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlLpMWkz7u_6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "WllfTTeJ_NI4",
    "outputId": "f37faaff-24e2-4542-d245-5a4955e28dce"
   },
   "outputs": [],
   "source": [
    "K.set_learning_phase(False)\n",
    "\n",
    "input_noise_dim = 100\n",
    "batch_size = 50\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "conditioned_noise = []\n",
    "for i in range(batch_size):\n",
    "    conditioned_noise.append(np.append(noise[i], to_categorical(0, 10)))\n",
    "conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "g = load_model(\"/content/models/ThoughtViz/generator_100\")\n",
    "\n",
    "# generate images using the generator\n",
    "generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "image = combine_rgb_images(generated_images)\n",
    "image = image * 127.5 + 127.5\n",
    "plt.imshow(image)\n",
    "img = Image.fromarray(image.astype(np.uint8))\n",
    "img.show()\n",
    "img.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmZV825L9kv1",
    "outputId": "268a9579-52af-49b7-811e-fd88451ed846"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from layers.mog_layer import *\n",
    "from utils.image_utils import *\n",
    "\n",
    "\n",
    "class Tests():\n",
    "\n",
    "    def test_deligan_baseline(self, generator_model):\n",
    "        K.set_learning_phase(False)\n",
    "\n",
    "        input_noise_dim = 100\n",
    "        batch_size = 50\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "        random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "        conditioned_noise = []\n",
    "        for i in range(batch_size):\n",
    "            conditioned_noise.append(np.append(noise[i], to_categorical(random_labels[i], 10)))\n",
    "        conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
    "\n",
    "        # generate images using the generator\n",
    "        generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "        image = combine_rgb_images(generated_images)\n",
    "        image = image * 127.5 + 127.5\n",
    "        img = Image.fromarray(image.astype(np.uint8))\n",
    "        img.show()\n",
    "\n",
    "    def test_deligan_final(self, generator_model, classifier_model, eeg_pkl_file):\n",
    "        K.set_learning_phase(False)\n",
    "\n",
    "        # load EEG data\n",
    "        eeg_data = pickle.load(open(eeg_pkl_file, \"rb\"), encoding='bytes')\n",
    "        classifier = load_model(classifier_model)\n",
    "\n",
    "        x_test = eeg_data[b'x_test']\n",
    "        y_test = eeg_data[b'y_test']\n",
    "        y_test = [np.argmax(y) for y in y_test]\n",
    "        layer_index = 9\n",
    "\n",
    "        # keras way of getting the output from an intermediate layer\n",
    "        get_nth_layer_output = K.function([classifier.layers[0].input], [classifier.layers[layer_index].output])\n",
    "\n",
    "        layer_output = get_nth_layer_output([x_test])[0]\n",
    "\n",
    "        input_noise_dim = 100\n",
    "        batch_size = 50\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "        random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "        eeg_feature_vectors = [layer_output[random.choice(np.where(y_test == random_label)[0])] for random_label in random_labels]\n",
    "\n",
    "        noises, conditionings = [], []\n",
    "        for i in range(batch_size):\n",
    "            noises.append(noise[i])\n",
    "            conditionings.append(eeg_feature_vectors[i])\n",
    "\n",
    "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
    "\n",
    "        # generate images using the generator\n",
    "        generated_images = g.predict([np.array(noises), np.array(conditionings)], verbose=0)\n",
    "\n",
    "        image = combine_rgb_images(generated_images)\n",
    "        image = image * 127.5 + 127.5\n",
    "        img = Image.fromarray(image.astype(np.uint8))\n",
    "        img.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tests = Tests()\n",
    "    #tests.test_deligan_baseline('../models/gan_models/baseline/deligan/image/generator.model')\n",
    "    tests.test_deligan_final('../models/gan_models/final/image/generator.model',\n",
    "                       '../models/eeg_models/image/run_final.h5',\n",
    "                       '../data/eeg/image/data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUOI8Pnt1pzr"
   },
   "source": [
    "# ThoughtViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTpkEupEF4S4"
   },
   "source": [
    "**Complete the following steps before running the cells below**\n",
    "\n",
    "1.\n",
    "Change runtime to GPU \n",
    "\n",
    ">Runtime > Change runtime type > GPU\n",
    "\n",
    "2.\n",
    "For each of the following links, add a Google Drive shortcut to your Drive root folder(My Drive):\n",
    "\n",
    ">https://drive.google.com/file/d/1atP9CsjWIT-hg3fX--fcC1hg0uvg9bEH/view\n",
    "\n",
    ">https://drive.google.com/file/d/1x32IulYeBVmkshEKweijMX3DK1Wu8odx/view\n",
    "\n",
    ">https://drive.google.com/file/d/1cq8RTBiwqO-Jo0TZjBNlRHZEhjKDknKP/view\n",
    "\n",
    ">https://drive.google.com/file/d/1U9qtN1SlOS3dzd2BwWWHhJiMz_0yNW9U/view\n",
    "\n",
    ">https://drive.google.com/file/d/1uFFhvTsU2nmdaecR2WPWsiGJfgI3as1_/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TItDNCT0N9q"
   },
   "source": [
    "**Mount drive and download project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUz-xkGirmGA"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfeFyd97TzfV"
   },
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/ptirupat/ThoughtViz.git\" \"/content/ThoughtViz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXJSE0mZGRI7"
   },
   "source": [
    "**Unzip project files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo9VRjm_soez"
   },
   "outputs": [],
   "source": [
    "!unzip -q \"/content/gdrive/My Drive/data.zip\" -d \"/content/ThoughtViz\" \n",
    "!unzip -q \"/content/gdrive/My Drive/images.zip\" -d \"/content/ThoughtViz/training\"\n",
    "!unzip -q \"/content/gdrive/My Drive/eeg_models.zip\" -d \"/content/ThoughtViz/models\" \n",
    "!unzip -q \"/content/gdrive/My Drive/trained_classifier_models.zip\" -d \"/content/ThoughtViz/training\"\n",
    "!unzip -q \"/content/gdrive/My Drive/gan_models.zip\" -d \"/content/ThoughtViz/models\"\n",
    "!echo \"Download Finished!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a79LZR2kDeaJ"
   },
   "source": [
    "**Update outdated module calls and make code compatible with colab environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8qiLZ7nApzh"
   },
   "outputs": [],
   "source": [
    "# Update Keras module structure\n",
    "!sed -i \"s/keras.layers/keras/\" ThoughtViz/layers/mog_layer.py\n",
    "\n",
    "# Change System Path\n",
    "!sed -i \"1i import sys\\nsys.path.insert(0,'/content/ThoughtViz')\\n\" /content/ThoughtViz/testing/test.py\n",
    "!sed -i \"s/\\..\\//\\/content\\/ThoughtViz\\//\" ThoughtViz/testing/test.py\n",
    "\n",
    "# Rename GAN model directory to match code\n",
    "!mv /content/ThoughtViz/models/gan_models/thoughtviz/ /content/ThoughtViz/models/gan_models/final/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3Hr0y71HRvh"
   },
   "source": [
    "**Test code**\n",
    "\n",
    "Note: Images are not saved by default. \n",
    ">To save the output image, you must edit the *ThoughtViz/testing/test.py* file and add the line **img.save('output.png')** after *img.show()* in the *test_deligan_final* function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov05zjNP70zG"
   },
   "outputs": [],
   "source": [
    "!python3 ThoughtViz/testing/test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtGN9xbZ39lE"
   },
   "source": [
    "# EEG decoding to images - Converting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoP7kwRQKHCm"
   },
   "source": [
    "**Convert to NPY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAbmc-734gXr"
   },
   "source": [
    "Code provided by: *https://github.com/perceivelab/eeg_visual_classification/blob/main/eeg_signal_classification.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fB7_4GgI4UkY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pth_to_npy(path):\n",
    "      torch_file = torch.load(path)\n",
    "\n",
    "      torch_dataset = torch_file[\"dataset\"]\n",
    "      torch_labels = torch_file[\"labels\"]\n",
    "      torch_images = torch_file[\"images\"]\n",
    "      torch_size = len(torch_dataset)\n",
    "\n",
    "      return torch_dataset, torch_labels, torch_images, torch_size\n",
    "\n",
    "torch_dataset, torch_labels, torch_images, torch_size = pth_to_npy(data_path)\n",
    "# # Dataset class\n",
    "# class EEGDataset:\n",
    "    \n",
    "#     # Constructor\n",
    "#     def __init__(self, eeg_signals_path):\n",
    "#         # Load EEG signals\n",
    "#         loaded = torch.load(eeg_signals_path)\n",
    "#         if opt.subject!=0:\n",
    "#             self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==opt.subject]\n",
    "#         else:\n",
    "#             self.data=loaded['dataset']        \n",
    "#         self.labels = loaded[\"labels\"]\n",
    "#         self.images = loaded[\"images\"]\n",
    "        \n",
    "#         # Compute size\n",
    "#         self.size = len(self.data)\n",
    "\n",
    "#     # Get size\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     # Get item\n",
    "#     def __getitem__(self, i):\n",
    "#         # Process EEG\n",
    "#         eeg = self.data[i][\"eeg\"].float().t()\n",
    "#         eeg = eeg[opt.time_low:opt.time_high,:]\n",
    "\n",
    "#         if opt.model_type == \"model10\":\n",
    "#             eeg = eeg.t()\n",
    "#             eeg = eeg.view(1,128,opt.time_high-opt.time_low)\n",
    "#         # Get label\n",
    "#         label = self.data[i][\"label\"]\n",
    "#         # Return\n",
    "#         return eeg, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hb5xFyEpQBaj",
    "outputId": "55157747-30fa-4d7c-9d04-49861e6fdd94"
   },
   "outputs": [],
   "source": [
    "print(\"size:\", torch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRPIXFBGdVpp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjWLx1doROMW",
    "outputId": "c50823c4-7ab9-4e88-aec4-d44d4af8d873"
   },
   "outputs": [],
   "source": [
    "# print(\"dataset 0:\", torch_dataset[0])\n",
    "eeg = torch_dataset[0][\"eeg\"]\n",
    "image_num = torch_dataset[0][\"image\"]\n",
    "label = torch_dataset[0][\"label\"]\n",
    "subject = torch_dataset[0][\"subject\"]\n",
    "\n",
    "print(\"eeg data:\", eeg)\n",
    "print(\"image number:\", image_num)\n",
    "print(\"image label:\", label)\n",
    "print(\"participant number:\", subject)\n",
    "\n",
    "trimmed_eeg = eeg.float().t()[40:460]\n",
    "print(\"eeg data trimmed:\", trimmed_eeg)\n",
    "\n",
    "numpy_eeg = trimmed_eeg.numpy()\n",
    "print(\"eeg data trimmed+numpy:\", numpy_eeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uI9xMywlQx7o",
    "outputId": "84bf189c-cbd1-4c98-9b06-d47cef86fe37"
   },
   "outputs": [],
   "source": [
    "print(\"label 0:\", torch_labels[0])\n",
    "print(\"label 1:\", torch_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVKjoP9MQ7eO",
    "outputId": "12cabec5-9fde-4b64-c2a7-9998174e2139"
   },
   "outputs": [],
   "source": [
    "print(\"image 0:\", torch_images[0])\n",
    "print(\"image 1:\", torch_images[1])\n",
    "\n",
    "for i in range(0,10):\n",
    "  print(torch_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kwdKppi4K0Z"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlOHKmR49Psy"
   },
   "source": [
    "**Download and Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0hHATgNyvPC",
    "outputId": "aca4bd0a-ecf2-4e07-9047-cfbcf9f57109"
   },
   "outputs": [],
   "source": [
    "!pip install -U mne\n",
    "!pip install pyEDFlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zs8lBY6j9B5_"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "from pyedflib import highlevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fZb1Gzn9UzE"
   },
   "source": [
    "**Define Visualization Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-BOnV8T8lmx"
   },
   "outputs": [],
   "source": [
    "def create_edf(npy_path, name):\n",
    "  data = np.load(npy_path)\n",
    "  # [128,440,11965]\n",
    "  signals = list(data.T[:,:,:])\n",
    "  channel_names = list('ch'+str(i) for i in range(0,128))\n",
    "  signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "  header = highlevel.make_header()\n",
    "  highlevel.write_edf(name, signals, signal_headers, header)\n",
    "\n",
    "def create_edf2(npy_path, name):\n",
    "  data = np.load(npy_path)\n",
    "  # [128,11965,440]\n",
    "  signals = list(data.transpose(2,0,1)[0:ch,0:100,:])\n",
    "  channel_names = list('ch'+str(i) for i in range(0,128))\n",
    "  signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "  header = highlevel.make_header()\n",
    "  highlevel.write_edf(name, signals, signal_headers, header)\n",
    "\n",
    "def visualizeData(path):\n",
    "  raw = mne.io.read_raw_edf(path,preload=True);\n",
    "  print(raw)\n",
    "  raw.plot_psd(area_mode='range', fmin=15, fmax=70.0, show=False, average=True);\n",
    "  raw.plot(duration=2, scalings=0.000002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EL43khtN0b-"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/gdrive/MyDrive/EEG2Image/data/eeg_14_70.npy'\n",
    "test_path = '/content/test_file.edf'\n",
    "rand_path = '/content/edf_file.edf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AAoglAZY3JR",
    "outputId": "b5272e3b-7fa6-47ea-e476-2c2a734fc415"
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data[::1,1].shape)\n",
    "print(data[::1,2].shape)\n",
    "print(data[1::].shape)\n",
    "print(data[2::].shape)\n",
    "print(data.T[0,0].shape)\n",
    "print(data.T.shape)\n",
    "print(data.T[:,:,0:10].shape)\n",
    "print(data.T[:,:,0:10].reshape(128,440*10).shape)\n",
    "print(data.transpose(2,0,1).shape)\n",
    "print(data.transpose(2,0,1)[:,0:10,:].shape)\n",
    "data.transpose(2,0,1)[0:10,0:10,:].reshape((128,440*10)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "eepeZbubOJVk",
    "outputId": "37f4d124-3512-47f7-c397-c5f1322f75fc"
   },
   "outputs": [],
   "source": [
    "# write an edf file\n",
    "ch = 5\n",
    "signals = list(data.T[0:ch,:,0:100].reshape(ch,440*100))\n",
    "channel_names = list('ch'+str(i) for i in range(0,ch))\n",
    "signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "header = highlevel.make_header(patientname='patient_x', gender='Female')\n",
    "highlevel.write_edf('test_file.edf', signals, signal_headers, header)\n",
    "\n",
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "J0m0kkUuNIXb",
    "outputId": "d78821b7-7916-47c4-cdd5-0cff76fc4c70"
   },
   "outputs": [],
   "source": [
    "# write an edf file\n",
    "ch = 5\n",
    "signals = list(data.transpose(2,0,1)[0:ch,0:100,:])\n",
    "channel_names = list('ch'+str(i) for i in range(0,ch))\n",
    "signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "header = highlevel.make_header()\n",
    "highlevel.write_edf('test_file.edf', signals, signal_headers, header)\n",
    "\n",
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "atyHW3BWnS9J",
    "outputId": "d32f1654-12cb-4e54-a1e6-f0ddf0e45817"
   },
   "outputs": [],
   "source": [
    "# write an edf file\n",
    "ch = 50\n",
    "signals = list(data.transpose(2,0,1)[0:ch,0:100,:])\n",
    "channel_names = list('ch'+str(i) for i in range(0,ch))\n",
    "signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "header = highlevel.make_header()\n",
    "highlevel.write_edf('test_file.edf', signals, signal_headers, header)\n",
    "\n",
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "NOQdtLEINQGy",
    "outputId": "56dcc711-ef5c-4d58-def9-6dc01dc45546"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "0wsi8UGJlflJ",
    "outputId": "62b590c0-0d04-45ce-ecec-af507f177a3e"
   },
   "outputs": [],
   "source": [
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "xZzJCRhgnV1d",
    "outputId": "6d9597e4-97d0-4973-c916-bfc0ad877648"
   },
   "outputs": [],
   "source": [
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC2ITzzILMDk"
   },
   "source": [
    "**LSTM stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxsizMvPLUUQ"
   },
   "outputs": [],
   "source": [
    "# Code goes here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz0lue81LaON"
   },
   "source": [
    "**GAN stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJIEr4gVLgCY"
   },
   "outputs": [],
   "source": [
    "# Code goes here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXagbmapLnsY"
   },
   "source": [
    "**Test Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIc1nb7BLpsC"
   },
   "outputs": [],
   "source": [
    "# Code goes here #"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPqkXqJUAQ2NK5Q/RgvUhXg",
   "collapsed_sections": [
    "dsBmC9XVLTvz",
    "mUOI8Pnt1pzr",
    "LtGN9xbZ39lE",
    "_kwdKppi4K0Z"
   ],
   "include_colab_link": true,
   "name": "CSE 247 Project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
