{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlexanderCardarasUCSC/EEG-Decoding-to-Images/blob/main/CSE_247_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install packages for running on PRP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install keras\n",
    "!pip -q install google-cloud-storage\n",
    "!pip -q install google-api-python-client oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/oauth2client/_helpers.py:255: UserWarning: Cannot access token.json: No such file or directory\n",
      "  warnings.warn(_MISSING_FILE_MESSAGE.format(filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=690369448729-2hvau5npdbarjcoenoqb4nr2g7mvih5e.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.readonly&access_type=offline&response_type=code\n",
      "\n",
      "Enter verification code: 4/1AY0e-g4-WoAqxi9kGuJTGuvI1RTERIdNhNtWpIdbW3J9hvTWmdl2jU2H3B4\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from apiclient import discovery\n",
    "from httplib2 import Http\n",
    "import oauth2client\n",
    "from oauth2client import file, client, tools\n",
    "\n",
    "obj = lambda: None\n",
    "lmao = {\"auth_host_name\":'localhost', 'noauth_local_webserver':'store_true', \n",
    "        'auth_host_port':[8080, 8090], 'logging_level':'ERROR'}\n",
    "for k, v in lmao.items():\n",
    "    setattr(obj, k, v)\n",
    "    \n",
    "# authorization boilerplate code\n",
    "SCOPES = 'https://www.googleapis.com/auth/drive.readonly'\n",
    "store = file.Storage('token.json')\n",
    "creds = store.get()\n",
    "# The following will give you a link if token.json does not exist, the link allows the\n",
    "# user to give this app permission\n",
    "if not creds or creds.invalid:\n",
    "    flow = client.flow_from_clientsecrets('client_secret.json', SCOPES)\n",
    "    creds = tools.run_flow(flow, store, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "DRIVE = discovery.build('drive', 'v3', http=creds.authorize(Http()))\n",
    "\n",
    "def download_file(file_id, filename):\n",
    "    # if you get the shareable link, the link contains this id, replace the file_id below\n",
    "    request = DRIVE.files().get_media(fileId=file_id)\n",
    "    # replace the filename and extension in the first field below\n",
    "    fh = io.FileIO(filename, mode='w')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 23%.\n",
      "Download 47%.\n",
      "Download 71%.\n",
      "Download 94%.\n",
      "Download 100%.\n",
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "# Download Image data\n",
    "!mkdir -p data/images\n",
    "!wget -q https://raw.githubusercontent.com/AlexanderCardarasUCSC/EEG-Decoding-to-Images/main/image_utils.py \n",
    "download_file(\"1-2WFgtBn5WdZCnDlJkwaswTTrRtVNrS0\", \"images.npy\")\n",
    "download_file(\"1Lijm9A5Kq4EMub5jO0Bzz3c2VJLm-WH6\", \"image_labels.npy\")\n",
    "\n",
    "!sudo mv images.npy data/images\n",
    "!sudo mv image_labels.npy data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUWoxMgn9TOB"
   },
   "source": [
    "# EEG decoding to images - Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7ZhR8a4LkDU"
   },
   "source": [
    "**Complete the following steps before running the cells below**\n",
    "\n",
    "1.\n",
    "Change runtime to GPU \n",
    "\n",
    ">Runtime > Change runtime type > GPU\n",
    "\n",
    "2.\n",
    "For each of the following link, add a Google Drive shortcut to your Drive root folder(My Drive):\n",
    "\n",
    ">https://drive.google.com/drive/folders/1z8wcew5R7GCeu5s7SSsWw8bUcx1urVmk?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JenuAt1ALqvt"
   },
   "source": [
    "**Mount drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVsVV6U5LtiA",
    "outputId": "0b74228a-c9aa-4da6-ceb2-fd22d1bf1649"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZRqXBBuOR8l"
   },
   "source": [
    "**Run the following cells to load eeg and/or image data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsBmC9XVLTvz"
   },
   "source": [
    "#### Load eeg data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNQaYZ5HyCa7"
   },
   "source": [
    "**Uncomment one of the following blocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4xLacydzSQ4"
   },
   "source": [
    "*5-95Hz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRqC6iVFyCvW"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /content/data/eeg\n",
    "# !cp /content/gdrive/MyDrive/EEG2Image/eeg_5_95_std.pth /content/data/eeg/\n",
    "# eeg_path = \"/content/data/eeg/eeg_5_95_std.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMU8iY61zYB5"
   },
   "source": [
    "14-70Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aru_3JtzNI0"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /content/data/eeg\n",
    "# !cp /content/gdrive/MyDrive/EEG2Image/eeg_14_70_std.pth /content/data/eeg/\n",
    "# eeg_path = \"/content/data/eeg/eeg_14_70_std.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNDc6jwSzYjf"
   },
   "source": [
    "*55-95Hz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nrNCoFSzNbX"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/data/eeg\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/eeg_54_95_std.pth /content/data/eeg/\n",
    "eeg_path = \"/content/data/eeg/eeg_54_95_std.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pzsz5CBrLJQJ"
   },
   "source": [
    "#### Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwepmfU39z3E"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/data/images\n",
    "!wget -q https://raw.githubusercontent.com/AlexanderCardarasUCSC/EEG-Decoding-to-Images/main/image_utils.py /content\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/data/images.npy /content/data/images\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/data/image_labels.npy /content/data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imTyI_FFFpvj"
   },
   "source": [
    "**Example load image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjkR7Orw-ovS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path_root=\"/content/\"):\n",
    "  images = np.load(path_root+\"data/images/images.npy\")\n",
    "  image_labels = np.load(path_root+\"data/images/image_labels.npy\")\n",
    "  return images, image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Dcw0IP7AH9b"
   },
   "outputs": [],
   "source": [
    "images, labels = image_utils.load_images(path_root=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "gBGRRcUB_Aci",
    "outputId": "bced421a-c4b9-4072-e548-4f4ca9d32c4e"
   },
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ7shO6sIN5l"
   },
   "source": [
    "# Baseline GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKBx7cByIds4"
   },
   "source": [
    "**Complete the following steps before running the cells below**\n",
    "\n",
    "1. Follow directions in the Load data section to load image data\n",
    "\n",
    ">Run the cells under the header **Load image data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34oS7RnrPajK"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2hmYEUh2q5R"
   },
   "source": [
    "### Jason's AC-GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZLFNBTbdkrK"
   },
   "outputs": [],
   "source": [
    "# example of fitting an auxiliary classifier gan (ac-gan) on fashion mnsit\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXllC6Y0PiRj"
   },
   "source": [
    "**Discriminator architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUwEvQbeFk-2"
   },
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(64,64,3), n_classes=10):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=in_shape)\n",
    "\t# downsample to 14x14\n",
    "\tfe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# normal\n",
    "\tfe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "\tfe = BatchNormalization()(fe)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# downsample to 7x7\n",
    "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "\tfe = BatchNormalization()(fe)\n",
    "\tfe = LeakyReLU( alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# normal\n",
    "\tfe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "\tfe = BatchNormalization()(fe)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\tfe = Dropout(0.5)(fe)\n",
    "\t# flatten feature maps\n",
    "\tfe = Flatten()(fe)\n",
    "\t# real/fake output\n",
    "\tout1 = Dense(1, activation='sigmoid')(fe)\n",
    "\t# class label output\n",
    "\tout2 = Dense(n_classes, activation='softmax')(fe)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, [out1, out2])\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9yPgBfEPsdM"
   },
   "source": [
    "**Generator architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtkgzUM3PuXp"
   },
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes=10):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# label input\n",
    "\tin_label = Input(shape=(1,))\n",
    "\t# embedding for categorical input\n",
    "\t# li = Embedding(n_classes, 50)(in_label)\n",
    "\tli = Embedding(n_classes, 50)(in_label)\n",
    "\t# linear multiplication\n",
    "\t# n_nodes = 7 * 7\n",
    "\tn_nodes = 16 * 16 * 3\n",
    "\tli = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "\t# reshape to additional channel\n",
    "\t# li = Reshape((7, 7, 1))(li)\n",
    "\tli = Reshape((16, 16, 3))(li)\n",
    "\t# image generator input\n",
    "\tin_lat = Input(shape=(latent_dim,))\n",
    "\t# foundation for 7x7 image\n",
    "\t# n_nodes = 384 * 7 * 7\n",
    "\tn_nodes = 384 * 16 * 16\n",
    "\tgen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "\tgen = Activation('relu')(gen)\n",
    "\t# gen = Reshape((7, 7, 384))(gen)\n",
    "\tgen = Reshape((16, 16, 384))(gen)\n",
    "\t# merge image gen and label input\n",
    "\tmerge = Concatenate()([gen, li])\n",
    "\t# upsample to 14x14\n",
    "\tgen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "\tgen = BatchNormalization()(gen)\n",
    "\tgen = Activation('relu')(gen)\n",
    "\t# upsample to 28x28\n",
    "\t# gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "\tgen = Conv2DTranspose(3, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "\tout_layer = Activation('tanh')(gen)\n",
    "\t# define model\n",
    "\tmodel = Model([in_lat, in_label], out_layer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbtLd_QgP6T9"
   },
   "source": [
    "**GAN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUP86d-eP6jc"
   },
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tfor layer in d_model.layers:\n",
    "\t\tif not isinstance(layer, BatchNormalization):\n",
    "\t\t\tlayer.trainable = False\n",
    "\t# connect the outputs of the generator to the inputs of the discriminator\n",
    "\tgan_output = d_model(g_model.output)\n",
    "\t# define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "\tmodel = Model(g_model.input, gan_output)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.2)\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHNh232fQxaC"
   },
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t29JjwvnQnGz"
   },
   "outputs": [],
   "source": [
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "# splits image data into training and test sets(currently no randomization)\n",
    "def new_split_data(images, labels, train=0.8, data_per_class=900):\n",
    "  trainX,trainY,testX,testY = [],[],[],[]\n",
    "  for i in range(images.shape[0]):\n",
    "    chunk = i % data_per_class\n",
    "    if chunk < data_per_class*train:\n",
    "      trainX.append(images[i])\n",
    "      trainY.append(labels[i])\n",
    "    else:\n",
    "      testX.append(images[i])\n",
    "      testY.append(labels[i])\n",
    "\n",
    "  return (np.asarray(trainX), np.asarray(trainY)), (np.asarray(testX), np.asarray(testY))\n",
    "\n",
    "# load images\n",
    "def load_real_samples(n_classes):\n",
    "  # load dataset\n",
    "  images, labels = image_utils.load_images(path_root=\"\")\n",
    "  images, labels = trim_classes(images, labels, n_classes)\n",
    "  (trainX, trainY), (testX, testY) = new_split_data(images, labels)\n",
    "\t# expand to 3d, e.g. add channels\n",
    "  X = expand_dims(trainX, axis=-1)\n",
    "\t# convert from ints to floats\n",
    "  X = X.astype('float32')\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "  X = (X - 127.5) / 127.5\n",
    "  print(X.shape, trainY.shape)\n",
    "  return [X, trainY]\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# split into images and labels\n",
    "\timages, labels = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, images.shape[0], n_samples)\n",
    "\t# select images and labels\n",
    "\tX, labels = images[ix], labels[ix]\n",
    "\t# generate class labels\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn [X, labels], y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes):\n",
    "  # generate points in the latent space\n",
    "  x_input = randn(latent_dim * n_samples)\n",
    "  # reshape into a batch of inputs for the network\n",
    "  z_input = x_input.reshape(n_samples, latent_dim)\n",
    "  # generate labels\n",
    "  labels = randint(0, n_classes, n_samples)\n",
    "  return [z_input, labels]\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples, n_classes)\n",
    "\t# predict outputs\n",
    "\timages = generator.predict([z_input, labels_input])\n",
    "\t# create class labels\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn [images, labels_input], y\n",
    "\n",
    "def generate_fake_class_sample(generator, class_label, latent_dim, n_samples):\n",
    "  # generate points in the latent space\n",
    "  x_input = randn(latent_dim * n_samples)\n",
    "  # reshape into a batch of inputs for the network\n",
    "  z_input = x_input.reshape(n_samples, latent_dim)\n",
    "  labels_input = np.asarray([class_label for i in range(n_samples)])\n",
    "  # predict outputs\n",
    "  images = generator.predict([z_input, labels_input])\n",
    "  # create class labels\n",
    "  y = zeros((n_samples, 1))\n",
    "  return [images, labels_input], y\n",
    " \n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "\t# prepare fake examples\n",
    "\t[X, _], _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# scale from [-1,1] to [0,1]\n",
    "\tX = (X + 1) / 2.0\n",
    "\t# plot images\n",
    "\tfor i in range(100):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(10, 10, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\t# pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "\t\tpyplot.imshow(X[i, :, :])\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'generated_plot_%04d.png' % (step+1)\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%04d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
    " \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_classes=10):\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# calculate the size of half a batch of samples\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# get randomly selected 'real' samples\n",
    "\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t# update discriminator model weights\n",
    "\t\t_,d_r1,d_r2 = d_model.train_on_batch(X_real, [y_real, labels_real])\n",
    "\t\t# generate 'fake' examples\n",
    "\t\t[X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator model weights\n",
    "\t\t_,d_f,d_f2 = d_model.train_on_batch(X_fake, [y_fake, labels_fake])\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\t[z_input, z_labels] = generate_latent_points(latent_dim, n_batch, n_classes)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\t_,g_1,g_2 = gan_model.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
    "\t\t# summarize loss on this batch\n",
    "\t\tprint('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % (i+1, d_r1,d_r2, d_f,d_f2, g_1,g_2))\n",
    "\t\t# evaluate the model performance every 'epoch'\n",
    "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-HQexL2SEaB"
   },
   "source": [
    "**Train GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTVMnDL0REYL"
   },
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "# latent_dim = 100\n",
    "latent_dim = 100\n",
    "n_classes=1\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator(n_classes=n_classes)\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim, n_classes)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load image data\n",
    "dataset = load_real_samples(n_classes)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=100, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVFroeQlgCZ1"
   },
   "outputs": [],
   "source": [
    "[X, _], _ = generate_fake_class_sample(generator, class_label=0, latent_dim=latent_dim, n_samples=16)\n",
    "# scale from [-1,1] to [0,1]\n",
    "X = (X + 1) / 2.0\n",
    "\n",
    "f, axs = plt.subplots(4,4,figsize=(15,15))\n",
    "# plot images\n",
    "for i in range(16):\n",
    "  axs[i//4,i%4].imshow(X[i])\n",
    "  axs[i//4,i%4].axis(\"off\")\n",
    "f.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHRHGSU2MOHA"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(generator, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(discriminator, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 real=12% fake=0%\n",
      ">2 real=100% fake=0%\n",
      ">3 real=100% fake=0%\n",
      ">4 real=100% fake=0%\n",
      ">5 real=100% fake=0%\n",
      ">6 real=100% fake=20%\n",
      ">7 real=100% fake=84%\n",
      ">8 real=100% fake=100%\n",
      ">9 real=100% fake=100%\n",
      ">10 real=100% fake=100%\n",
      ">11 real=100% fake=100%\n",
      ">12 real=100% fake=100%\n",
      ">13 real=100% fake=100%\n",
      ">14 real=100% fake=100%\n",
      ">15 real=100% fake=100%\n",
      ">16 real=100% fake=100%\n",
      ">17 real=100% fake=100%\n",
      ">18 real=100% fake=100%\n",
      ">19 real=100% fake=100%\n",
      ">20 real=97% fake=100%\n"
     ]
    }
   ],
   "source": [
    "# example of training the discriminator model on real and random cifar10 images\n",
    "from numpy import expand_dims\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from keras.datasets.cifar10 import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "import image_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    " \n",
    "# define the standalone discriminator model\n",
    "def define_discriminator32(in_shape=(32,32,3)):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator64(in_shape=(64,64,3)):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    model.add(Conv2D(128, (3,3), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(512, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator32(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator64(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# load and prepare cifar10 training images\n",
    "def load_real_samples_cifar10():\n",
    "    # load cifar10 dataset\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    \n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "def trim_classes(images, labels, max_classes=10):\n",
    "    return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "\n",
    "\n",
    "# load and prepare cifar10 training images\n",
    "def load_real_samples_imagenet64(n_classes=10):\n",
    "    # load imagenet64 dataset\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, n_classes)\n",
    "    (trainX, _), (_, _) = image_utils.split_data(images, labels)\n",
    "    \n",
    "    \n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "#     X = (X - 127.5) / 127.5\n",
    "    return X\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# train the discriminator model\n",
    "def train_discriminator(model, dataset, n_iter=20, n_batch=128):\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_iter):\n",
    "        # get randomly selected 'real' samples\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        # update discriminator on real samples\n",
    "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
    "        # generate 'fake' examples\n",
    "#         X_fake, y_fake = generate_fake_samples32(half_batch)\n",
    "        X_fake, y_fake = generate_fake_samples(half_batch)\n",
    "        # update discriminator on fake samples\n",
    "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "        # summarize performance\n",
    "        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))\n",
    "        \n",
    "# define the discriminator model\n",
    "# model = define_discriminator32()\n",
    "model = define_discriminator64()\n",
    "# load image data\n",
    "# dataset = load_real_samples_cifar10()\n",
    "dataset = load_real_samples_imagenet64()\n",
    "\n",
    "# fit the model\n",
    "train_discriminator(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/9, d1=0.679, d2=0.695 g=0.692\n",
      ">1, 2/9, d1=0.641, d2=0.696 g=0.691\n",
      ">1, 3/9, d1=0.600, d2=0.697 g=0.690\n",
      ">1, 4/9, d1=0.557, d2=0.700 g=0.687\n",
      ">1, 5/9, d1=0.503, d2=0.707 g=0.680\n",
      ">1, 6/9, d1=0.442, d2=0.726 g=0.662\n",
      ">1, 7/9, d1=0.392, d2=0.769 g=0.628\n",
      ">1, 8/9, d1=0.326, d2=0.863 g=0.569\n",
      ">1, 9/9, d1=0.281, d2=0.998 g=0.516\n",
      ">2, 1/9, d1=0.295, d2=1.094 g=0.510\n",
      ">2, 2/9, d1=0.262, d2=0.970 g=0.609\n",
      ">2, 3/9, d1=0.313, d2=0.754 g=0.789\n",
      ">2, 4/9, d1=0.215, d2=0.567 g=0.988\n",
      ">2, 5/9, d1=0.232, d2=0.498 g=1.079\n",
      ">2, 6/9, d1=0.220, d2=0.535 g=0.990\n",
      ">2, 7/9, d1=0.189, d2=0.631 g=0.862\n",
      ">2, 8/9, d1=0.132, d2=0.707 g=0.782\n",
      ">2, 9/9, d1=0.142, d2=0.779 g=0.712\n",
      ">3, 1/9, d1=0.208, d2=0.892 g=0.647\n",
      ">3, 2/9, d1=0.221, d2=1.050 g=0.583\n",
      ">3, 3/9, d1=0.266, d2=1.311 g=0.528\n",
      ">3, 4/9, d1=0.250, d2=1.409 g=0.568\n",
      ">3, 5/9, d1=0.331, d2=1.178 g=0.769\n",
      ">3, 6/9, d1=0.491, d2=0.771 g=1.180\n",
      ">3, 7/9, d1=0.525, d2=0.391 g=1.645\n",
      ">3, 8/9, d1=0.694, d2=0.352 g=1.438\n",
      ">3, 9/9, d1=0.664, d2=0.483 g=1.053\n",
      ">4, 1/9, d1=0.581, d2=0.579 g=0.850\n",
      ">4, 2/9, d1=0.576, d2=0.630 g=0.784\n",
      ">4, 3/9, d1=0.522, d2=0.666 g=0.742\n",
      ">4, 4/9, d1=0.433, d2=0.697 g=0.716\n",
      ">4, 5/9, d1=0.438, d2=0.720 g=0.696\n",
      ">4, 6/9, d1=0.417, d2=0.737 g=0.673\n",
      ">4, 7/9, d1=0.356, d2=0.757 g=0.673\n",
      ">4, 8/9, d1=0.296, d2=0.747 g=0.683\n",
      ">4, 9/9, d1=0.263, d2=0.745 g=0.701\n",
      ">5, 1/9, d1=0.278, d2=0.712 g=0.726\n",
      ">5, 2/9, d1=0.227, d2=0.691 g=0.746\n",
      ">5, 3/9, d1=0.280, d2=0.710 g=0.724\n",
      ">5, 4/9, d1=0.254, d2=0.732 g=0.688\n",
      ">5, 5/9, d1=0.182, d2=0.765 g=0.655\n",
      ">5, 6/9, d1=0.191, d2=0.809 g=0.614\n",
      ">5, 7/9, d1=0.209, d2=0.878 g=0.567\n",
      ">5, 8/9, d1=0.103, d2=0.937 g=0.546\n",
      ">5, 9/9, d1=0.089, d2=0.995 g=0.515\n",
      ">6, 1/9, d1=0.119, d2=1.088 g=0.496\n",
      ">6, 2/9, d1=0.130, d2=1.104 g=0.503\n",
      ">6, 3/9, d1=0.131, d2=1.098 g=0.562\n",
      ">6, 4/9, d1=0.186, d2=0.944 g=0.657\n",
      ">6, 5/9, d1=0.251, d2=0.799 g=0.791\n",
      ">6, 6/9, d1=0.237, d2=0.665 g=0.889\n",
      ">6, 7/9, d1=0.244, d2=0.638 g=0.872\n",
      ">6, 8/9, d1=0.284, d2=0.734 g=0.788\n",
      ">6, 9/9, d1=0.238, d2=0.838 g=0.686\n",
      ">7, 1/9, d1=0.312, d2=0.930 g=0.609\n",
      ">7, 2/9, d1=0.234, d2=0.982 g=0.593\n",
      ">7, 3/9, d1=0.229, d2=0.984 g=0.631\n",
      ">7, 4/9, d1=0.305, d2=0.946 g=0.729\n",
      ">7, 5/9, d1=0.256, d2=0.790 g=0.984\n",
      ">7, 6/9, d1=0.373, d2=0.473 g=1.439\n",
      ">7, 7/9, d1=0.343, d2=0.280 g=1.881\n",
      ">7, 8/9, d1=0.450, d2=0.209 g=1.956\n",
      ">7, 9/9, d1=0.430, d2=0.262 g=1.645\n",
      ">8, 1/9, d1=0.392, d2=0.346 g=1.274\n",
      ">8, 2/9, d1=0.293, d2=0.446 g=1.058\n",
      ">8, 3/9, d1=0.329, d2=0.514 g=0.926\n",
      ">8, 4/9, d1=0.232, d2=0.548 g=0.868\n",
      ">8, 5/9, d1=0.186, d2=0.573 g=0.835\n",
      ">8, 6/9, d1=0.221, d2=0.588 g=0.816\n",
      ">8, 7/9, d1=0.151, d2=0.597 g=0.805\n",
      ">8, 8/9, d1=0.079, d2=0.605 g=0.797\n",
      ">8, 9/9, d1=0.102, d2=0.619 g=0.785\n",
      ">9, 1/9, d1=0.054, d2=0.638 g=0.767\n",
      ">9, 2/9, d1=0.081, d2=0.681 g=0.726\n",
      ">9, 3/9, d1=0.062, d2=0.783 g=0.662\n",
      ">9, 4/9, d1=0.063, d2=0.987 g=0.593\n",
      ">9, 5/9, d1=0.138, d2=1.066 g=0.671\n",
      ">9, 6/9, d1=0.165, d2=0.805 g=0.961\n",
      ">9, 7/9, d1=0.181, d2=0.537 g=1.083\n",
      ">9, 8/9, d1=0.240, d2=0.594 g=0.943\n",
      ">9, 9/9, d1=0.332, d2=0.700 g=0.791\n",
      ">10, 1/9, d1=0.166, d2=0.764 g=0.769\n",
      ">10, 2/9, d1=0.284, d2=0.745 g=0.800\n",
      ">10, 3/9, d1=0.192, d2=0.668 g=0.933\n",
      ">10, 4/9, d1=0.122, d2=0.523 g=1.111\n",
      ">10, 5/9, d1=0.091, d2=0.438 g=1.185\n",
      ">10, 6/9, d1=0.117, d2=0.538 g=0.977\n",
      ">10, 7/9, d1=0.127, d2=0.959 g=0.621\n",
      ">10, 8/9, d1=0.140, d2=1.615 g=0.418\n",
      ">10, 9/9, d1=0.121, d2=1.650 g=0.459\n",
      ">Accuracy real: 100%, fake: 0%\n",
      ">11, 1/9, d1=1.154, d2=1.206 g=0.664\n",
      ">11, 2/9, d1=0.253, d2=0.787 g=0.862\n",
      ">11, 3/9, d1=0.294, d2=0.546 g=1.068\n",
      ">11, 4/9, d1=0.274, d2=0.418 g=1.315\n",
      ">11, 5/9, d1=0.314, d2=0.327 g=1.514\n",
      ">11, 6/9, d1=0.231, d2=0.245 g=1.734\n",
      ">11, 7/9, d1=0.323, d2=0.214 g=1.836\n",
      ">11, 8/9, d1=0.219, d2=0.216 g=1.774\n",
      ">11, 9/9, d1=0.189, d2=0.230 g=1.739\n",
      ">12, 1/9, d1=0.182, d2=0.243 g=1.697\n",
      ">12, 2/9, d1=0.192, d2=0.264 g=1.548\n",
      ">12, 3/9, d1=0.173, d2=0.312 g=1.401\n",
      ">12, 4/9, d1=0.174, d2=0.351 g=1.283\n",
      ">12, 5/9, d1=0.166, d2=0.381 g=1.192\n",
      ">12, 6/9, d1=0.121, d2=0.402 g=1.143\n",
      ">12, 7/9, d1=0.090, d2=0.421 g=1.107\n",
      ">12, 8/9, d1=0.125, d2=0.437 g=1.070\n",
      ">12, 9/9, d1=0.067, d2=0.450 g=1.057\n",
      ">13, 1/9, d1=0.099, d2=0.478 g=1.037\n",
      ">13, 2/9, d1=0.104, d2=0.546 g=1.007\n",
      ">13, 3/9, d1=0.050, d2=0.765 g=1.028\n",
      ">13, 4/9, d1=0.081, d2=0.898 g=1.375\n",
      ">13, 5/9, d1=0.135, d2=0.500 g=2.700\n",
      ">13, 6/9, d1=0.561, d2=0.124 g=2.731\n",
      ">13, 7/9, d1=0.474, d2=0.281 g=1.935\n",
      ">13, 8/9, d1=0.287, d2=0.387 g=1.864\n",
      ">13, 9/9, d1=0.422, d2=0.545 g=1.596\n",
      ">14, 1/9, d1=0.417, d2=0.610 g=1.576\n",
      ">14, 2/9, d1=0.331, d2=0.509 g=1.788\n",
      ">14, 3/9, d1=0.278, d2=0.261 g=1.967\n",
      ">14, 4/9, d1=0.313, d2=0.295 g=1.512\n",
      ">14, 5/9, d1=0.166, d2=0.436 g=1.144\n",
      ">14, 6/9, d1=0.075, d2=0.807 g=0.752\n",
      ">14, 7/9, d1=0.078, d2=1.804 g=0.596\n",
      ">14, 8/9, d1=0.132, d2=1.507 g=0.874\n",
      ">14, 9/9, d1=0.249, d2=0.619 g=1.602\n",
      ">15, 1/9, d1=0.252, d2=0.242 g=2.258\n",
      ">15, 2/9, d1=0.333, d2=0.137 g=2.544\n",
      ">15, 3/9, d1=0.233, d2=0.118 g=2.619\n",
      ">15, 4/9, d1=0.227, d2=0.113 g=2.784\n",
      ">15, 5/9, d1=0.245, d2=0.091 g=2.753\n",
      ">15, 6/9, d1=0.282, d2=0.125 g=2.591\n",
      ">15, 7/9, d1=0.252, d2=0.134 g=2.513\n",
      ">15, 8/9, d1=0.216, d2=0.168 g=2.540\n",
      ">15, 9/9, d1=0.174, d2=0.158 g=2.289\n",
      ">16, 1/9, d1=0.182, d2=0.250 g=1.916\n",
      ">16, 2/9, d1=0.226, d2=0.411 g=1.355\n",
      ">16, 3/9, d1=0.232, d2=0.417 g=1.206\n",
      ">16, 4/9, d1=0.268, d2=0.406 g=1.227\n",
      ">16, 5/9, d1=0.075, d2=0.384 g=1.298\n",
      ">16, 6/9, d1=0.149, d2=0.353 g=1.351\n",
      ">16, 7/9, d1=0.129, d2=0.316 g=1.436\n",
      ">16, 8/9, d1=0.150, d2=0.310 g=1.461\n",
      ">16, 9/9, d1=0.248, d2=0.322 g=1.402\n",
      ">17, 1/9, d1=0.113, d2=0.381 g=1.362\n",
      ">17, 2/9, d1=0.264, d2=0.523 g=1.608\n",
      ">17, 3/9, d1=0.133, d2=0.386 g=2.180\n",
      ">17, 4/9, d1=0.352, d2=0.799 g=2.413\n",
      ">17, 5/9, d1=0.260, d2=0.462 g=3.701\n",
      ">17, 6/9, d1=0.493, d2=0.059 g=3.869\n",
      ">17, 7/9, d1=0.595, d2=0.161 g=4.257\n",
      ">17, 8/9, d1=0.520, d2=0.111 g=5.131\n",
      ">17, 9/9, d1=0.308, d2=0.053 g=4.874\n",
      ">18, 1/9, d1=0.502, d2=0.860 g=14.469\n",
      ">18, 2/9, d1=1.528, d2=0.000 g=16.036\n",
      ">18, 3/9, d1=1.888, d2=0.000 g=8.178\n",
      ">18, 4/9, d1=0.558, d2=0.250 g=6.730\n",
      ">18, 5/9, d1=0.479, d2=0.030 g=5.270\n",
      ">18, 6/9, d1=0.500, d2=0.559 g=11.489\n",
      ">18, 7/9, d1=0.711, d2=0.000 g=10.252\n",
      ">18, 8/9, d1=1.399, d2=1.936 g=9.889\n",
      ">18, 9/9, d1=2.252, d2=0.000 g=11.136\n",
      ">19, 1/9, d1=2.145, d2=0.000 g=7.529\n",
      ">19, 2/9, d1=1.556, d2=0.010 g=4.112\n",
      ">19, 3/9, d1=0.559, d2=0.212 g=3.438\n",
      ">19, 4/9, d1=0.571, d2=0.131 g=3.621\n",
      ">19, 5/9, d1=0.465, d2=0.080 g=3.388\n",
      ">19, 6/9, d1=0.578, d2=0.186 g=3.323\n",
      ">19, 7/9, d1=0.291, d2=0.281 g=3.013\n",
      ">19, 8/9, d1=0.337, d2=0.687 g=2.684\n",
      ">19, 9/9, d1=0.545, d2=0.448 g=2.688\n",
      ">20, 1/9, d1=0.858, d2=0.559 g=2.723\n",
      ">20, 2/9, d1=0.770, d2=0.561 g=2.832\n",
      ">20, 3/9, d1=0.748, d2=0.489 g=2.067\n",
      ">20, 4/9, d1=1.083, d2=0.905 g=1.415\n",
      ">20, 5/9, d1=1.104, d2=0.951 g=1.519\n",
      ">20, 6/9, d1=1.079, d2=0.628 g=2.082\n",
      ">20, 7/9, d1=1.062, d2=0.524 g=2.357\n",
      ">20, 8/9, d1=1.273, d2=0.266 g=2.323\n",
      ">20, 9/9, d1=0.842, d2=0.368 g=1.933\n",
      ">Accuracy real: 46%, fake: 71%\n",
      ">21, 1/9, d1=0.696, d2=0.599 g=1.543\n",
      ">21, 2/9, d1=1.138, d2=0.881 g=1.269\n",
      ">21, 3/9, d1=0.762, d2=0.835 g=1.184\n",
      ">21, 4/9, d1=0.732, d2=0.554 g=1.331\n",
      ">21, 5/9, d1=0.569, d2=0.407 g=1.515\n",
      ">21, 6/9, d1=0.656, d2=0.319 g=1.591\n",
      ">21, 7/9, d1=0.747, d2=0.309 g=1.660\n",
      ">21, 8/9, d1=0.540, d2=0.292 g=1.699\n",
      ">21, 9/9, d1=0.405, d2=0.292 g=1.676\n",
      ">22, 1/9, d1=0.518, d2=0.351 g=1.475\n",
      ">22, 2/9, d1=0.393, d2=0.368 g=1.324\n",
      ">22, 3/9, d1=0.443, d2=0.449 g=1.111\n",
      ">22, 4/9, d1=0.246, d2=0.514 g=0.987\n",
      ">22, 5/9, d1=0.375, d2=0.512 g=0.968\n",
      ">22, 6/9, d1=0.381, d2=0.535 g=0.930\n",
      ">22, 7/9, d1=0.249, d2=0.606 g=0.901\n",
      ">22, 8/9, d1=0.280, d2=0.629 g=0.847\n",
      ">22, 9/9, d1=0.269, d2=0.654 g=0.838\n",
      ">23, 1/9, d1=0.324, d2=0.651 g=0.887\n",
      ">23, 2/9, d1=0.333, d2=0.643 g=0.878\n",
      ">23, 3/9, d1=0.453, d2=0.635 g=0.875\n",
      ">23, 4/9, d1=0.442, d2=0.633 g=0.889\n",
      ">23, 5/9, d1=0.266, d2=0.618 g=0.879\n",
      ">23, 6/9, d1=0.215, d2=0.642 g=0.863\n",
      ">23, 7/9, d1=0.319, d2=0.649 g=0.855\n",
      ">23, 8/9, d1=0.461, d2=0.676 g=0.866\n",
      ">23, 9/9, d1=0.301, d2=0.582 g=0.914\n",
      ">24, 1/9, d1=0.360, d2=0.569 g=0.979\n",
      ">24, 2/9, d1=0.243, d2=0.548 g=1.077\n",
      ">24, 3/9, d1=0.354, d2=0.505 g=1.112\n",
      ">24, 4/9, d1=0.366, d2=0.523 g=1.087\n",
      ">24, 5/9, d1=0.259, d2=0.569 g=1.021\n",
      ">24, 6/9, d1=0.360, d2=0.628 g=1.031\n",
      ">24, 7/9, d1=0.368, d2=0.590 g=1.110\n",
      ">24, 8/9, d1=0.392, d2=0.493 g=1.213\n",
      ">24, 9/9, d1=0.404, d2=0.432 g=1.424\n",
      ">25, 1/9, d1=0.481, d2=0.346 g=1.508\n",
      ">25, 2/9, d1=0.453, d2=0.358 g=1.535\n",
      ">25, 3/9, d1=0.352, d2=0.369 g=1.500\n",
      ">25, 4/9, d1=0.451, d2=0.418 g=1.362\n",
      ">25, 5/9, d1=0.469, d2=0.544 g=1.124\n",
      ">25, 6/9, d1=0.374, d2=0.743 g=1.045\n",
      ">25, 7/9, d1=0.456, d2=1.205 g=1.413\n",
      ">25, 8/9, d1=0.462, d2=0.690 g=2.584\n",
      ">25, 9/9, d1=0.732, d2=0.143 g=3.327\n",
      ">26, 1/9, d1=0.747, d2=0.166 g=3.549\n",
      ">26, 2/9, d1=0.700, d2=0.150 g=3.796\n",
      ">26, 3/9, d1=0.568, d2=0.070 g=3.856\n",
      ">26, 4/9, d1=0.510, d2=0.060 g=3.455\n",
      ">26, 5/9, d1=0.480, d2=0.207 g=3.716\n",
      ">26, 6/9, d1=0.544, d2=0.250 g=3.379\n",
      ">26, 7/9, d1=0.458, d2=0.606 g=2.245\n",
      ">26, 8/9, d1=0.795, d2=1.479 g=0.884\n",
      ">26, 9/9, d1=0.757, d2=1.302 g=1.056\n",
      ">27, 1/9, d1=0.982, d2=0.625 g=1.771\n",
      ">27, 2/9, d1=0.773, d2=0.210 g=2.412\n",
      ">27, 3/9, d1=0.807, d2=0.151 g=2.556\n",
      ">27, 4/9, d1=0.667, d2=0.132 g=2.481\n",
      ">27, 5/9, d1=0.478, d2=0.178 g=2.419\n",
      ">27, 6/9, d1=0.445, d2=0.179 g=2.520\n",
      ">27, 7/9, d1=0.392, d2=0.206 g=2.642\n",
      ">27, 8/9, d1=0.478, d2=0.289 g=2.307\n",
      ">27, 9/9, d1=0.529, d2=0.408 g=1.931\n",
      ">28, 1/9, d1=0.433, d2=0.442 g=1.734\n",
      ">28, 2/9, d1=0.487, d2=0.690 g=1.403\n",
      ">28, 3/9, d1=0.742, d2=1.066 g=0.846\n",
      ">28, 4/9, d1=0.417, d2=1.114 g=0.721\n",
      ">28, 5/9, d1=0.618, d2=0.913 g=0.846\n",
      ">28, 6/9, d1=0.678, d2=0.699 g=1.047\n",
      ">28, 7/9, d1=0.576, d2=0.588 g=1.213\n",
      ">28, 8/9, d1=0.494, d2=0.601 g=1.288\n",
      ">28, 9/9, d1=0.473, d2=0.651 g=1.353\n",
      ">29, 1/9, d1=0.539, d2=0.821 g=1.210\n",
      ">29, 2/9, d1=0.594, d2=1.037 g=0.956\n",
      ">29, 3/9, d1=0.679, d2=1.033 g=0.977\n",
      ">29, 4/9, d1=0.731, d2=0.457 g=1.551\n",
      ">29, 5/9, d1=0.669, d2=0.313 g=1.608\n",
      ">29, 6/9, d1=0.524, d2=0.305 g=1.661\n",
      ">29, 7/9, d1=0.578, d2=0.322 g=1.591\n",
      ">29, 8/9, d1=0.478, d2=0.359 g=1.585\n",
      ">29, 9/9, d1=0.493, d2=0.333 g=1.633\n",
      ">30, 1/9, d1=0.423, d2=0.356 g=1.666\n",
      ">30, 2/9, d1=0.423, d2=0.326 g=1.719\n",
      ">30, 3/9, d1=0.549, d2=0.423 g=1.571\n",
      ">30, 4/9, d1=0.500, d2=0.439 g=1.602\n",
      ">30, 5/9, d1=0.520, d2=0.402 g=1.457\n",
      ">30, 6/9, d1=0.485, d2=0.369 g=1.493\n",
      ">30, 7/9, d1=0.454, d2=0.423 g=1.511\n",
      ">30, 8/9, d1=0.569, d2=0.396 g=1.458\n",
      ">30, 9/9, d1=0.518, d2=0.444 g=1.305\n",
      ">Accuracy real: 63%, fake: 89%\n",
      ">31, 1/9, d1=0.567, d2=0.535 g=1.093\n",
      ">31, 2/9, d1=0.683, d2=0.652 g=0.876\n",
      ">31, 3/9, d1=0.518, d2=0.764 g=0.820\n",
      ">31, 4/9, d1=0.436, d2=0.725 g=0.886\n",
      ">31, 5/9, d1=0.408, d2=0.596 g=1.047\n",
      ">31, 6/9, d1=0.402, d2=0.471 g=1.237\n",
      ">31, 7/9, d1=0.394, d2=0.367 g=1.409\n",
      ">31, 8/9, d1=0.433, d2=0.311 g=1.483\n",
      ">31, 9/9, d1=0.359, d2=0.276 g=1.599\n",
      ">32, 1/9, d1=0.387, d2=0.264 g=1.587\n",
      ">32, 2/9, d1=0.453, d2=0.274 g=1.573\n",
      ">32, 3/9, d1=0.342, d2=0.280 g=1.524\n",
      ">32, 4/9, d1=0.343, d2=0.296 g=1.453\n",
      ">32, 5/9, d1=0.333, d2=0.305 g=1.417\n",
      ">32, 6/9, d1=0.317, d2=0.323 g=1.356\n",
      ">32, 7/9, d1=0.286, d2=0.340 g=1.290\n",
      ">32, 8/9, d1=0.327, d2=0.370 g=1.217\n",
      ">32, 9/9, d1=0.297, d2=0.528 g=1.116\n",
      ">33, 1/9, d1=0.255, d2=0.715 g=0.957\n",
      ">33, 2/9, d1=0.258, d2=0.985 g=0.940\n",
      ">33, 3/9, d1=0.377, d2=0.691 g=1.367\n",
      ">33, 4/9, d1=0.406, d2=0.499 g=1.770\n",
      ">33, 5/9, d1=0.623, d2=0.424 g=1.609\n",
      ">33, 6/9, d1=0.848, d2=0.572 g=1.231\n",
      ">33, 7/9, d1=0.960, d2=0.959 g=0.805\n",
      ">33, 8/9, d1=0.720, d2=1.033 g=0.790\n",
      ">33, 9/9, d1=1.020, d2=1.092 g=0.774\n",
      ">34, 1/9, d1=1.036, d2=1.004 g=0.803\n",
      ">34, 2/9, d1=0.832, d2=0.893 g=0.805\n",
      ">34, 3/9, d1=0.865, d2=0.853 g=0.765\n",
      ">34, 4/9, d1=0.703, d2=0.896 g=0.693\n",
      ">34, 5/9, d1=0.644, d2=0.984 g=0.643\n",
      ">34, 6/9, d1=0.650, d2=0.958 g=0.631\n",
      ">34, 7/9, d1=0.606, d2=1.088 g=0.633\n",
      ">34, 8/9, d1=0.598, d2=1.040 g=0.646\n",
      ">34, 9/9, d1=0.618, d2=0.871 g=0.725\n",
      ">35, 1/9, d1=0.685, d2=0.848 g=0.811\n",
      ">35, 2/9, d1=0.674, d2=0.648 g=0.927\n",
      ">35, 3/9, d1=0.639, d2=0.536 g=1.084\n",
      ">35, 4/9, d1=0.660, d2=0.456 g=1.166\n",
      ">35, 5/9, d1=0.605, d2=0.394 g=1.286\n",
      ">35, 6/9, d1=0.475, d2=0.343 g=1.449\n",
      ">35, 7/9, d1=0.603, d2=0.314 g=1.482\n",
      ">35, 8/9, d1=0.486, d2=0.298 g=1.501\n",
      ">35, 9/9, d1=0.400, d2=0.286 g=1.520\n",
      ">36, 1/9, d1=0.360, d2=0.296 g=1.567\n",
      ">36, 2/9, d1=0.392, d2=0.338 g=1.470\n",
      ">36, 3/9, d1=0.440, d2=0.401 g=1.305\n",
      ">36, 4/9, d1=0.391, d2=0.556 g=1.148\n",
      ">36, 5/9, d1=0.469, d2=0.808 g=0.996\n",
      ">36, 6/9, d1=0.621, d2=1.021 g=0.974\n",
      ">36, 7/9, d1=0.520, d2=1.334 g=1.256\n",
      ">36, 8/9, d1=0.854, d2=1.330 g=1.448\n",
      ">36, 9/9, d1=1.111, d2=0.893 g=2.547\n",
      ">37, 1/9, d1=1.344, d2=0.091 g=3.393\n",
      ">37, 2/9, d1=1.311, d2=0.066 g=3.026\n",
      ">37, 3/9, d1=1.075, d2=0.106 g=2.643\n",
      ">37, 4/9, d1=0.811, d2=0.154 g=2.690\n",
      ">37, 5/9, d1=0.763, d2=0.155 g=2.894\n",
      ">37, 6/9, d1=0.664, d2=0.116 g=2.865\n",
      ">37, 7/9, d1=0.623, d2=0.131 g=2.891\n",
      ">37, 8/9, d1=0.479, d2=0.152 g=2.662\n",
      ">37, 9/9, d1=0.504, d2=0.164 g=2.547\n",
      ">38, 1/9, d1=0.509, d2=0.166 g=2.220\n",
      ">38, 2/9, d1=0.482, d2=0.239 g=2.032\n",
      ">38, 3/9, d1=0.475, d2=0.337 g=1.869\n",
      ">38, 4/9, d1=0.461, d2=0.374 g=1.566\n",
      ">38, 5/9, d1=0.461, d2=0.494 g=1.391\n",
      ">38, 6/9, d1=0.455, d2=0.442 g=1.375\n",
      ">38, 7/9, d1=0.596, d2=0.546 g=1.469\n",
      ">38, 8/9, d1=0.665, d2=0.542 g=1.510\n",
      ">38, 9/9, d1=0.621, d2=0.676 g=1.272\n",
      ">39, 1/9, d1=0.653, d2=1.192 g=0.921\n",
      ">39, 2/9, d1=0.802, d2=1.462 g=0.969\n",
      ">39, 3/9, d1=0.846, d2=0.821 g=1.588\n",
      ">39, 4/9, d1=1.002, d2=0.376 g=1.966\n",
      ">39, 5/9, d1=1.017, d2=0.333 g=1.885\n",
      ">39, 6/9, d1=0.996, d2=0.201 g=2.251\n",
      ">39, 7/9, d1=0.882, d2=0.197 g=2.273\n",
      ">39, 8/9, d1=0.782, d2=0.170 g=2.338\n",
      ">39, 9/9, d1=0.758, d2=0.183 g=2.263\n",
      ">40, 1/9, d1=0.725, d2=0.221 g=2.208\n",
      ">40, 2/9, d1=0.689, d2=0.182 g=2.299\n",
      ">40, 3/9, d1=0.793, d2=0.169 g=2.244\n",
      ">40, 4/9, d1=0.619, d2=0.185 g=2.149\n",
      ">40, 5/9, d1=0.548, d2=0.198 g=2.131\n",
      ">40, 6/9, d1=0.531, d2=0.256 g=1.958\n",
      ">40, 7/9, d1=0.466, d2=0.575 g=1.605\n",
      ">40, 8/9, d1=0.707, d2=0.996 g=1.530\n",
      ">40, 9/9, d1=0.667, d2=0.361 g=2.380\n",
      ">Accuracy real: 46%, fake: 100%\n",
      ">41, 1/9, d1=0.314, d2=0.103 g=2.956\n",
      ">41, 2/9, d1=0.860, d2=0.077 g=2.893\n",
      ">41, 3/9, d1=0.769, d2=0.135 g=2.613\n",
      ">41, 4/9, d1=0.563, d2=0.166 g=2.657\n",
      ">41, 5/9, d1=0.452, d2=0.250 g=2.620\n",
      ">41, 6/9, d1=0.595, d2=0.557 g=2.759\n",
      ">41, 7/9, d1=0.727, d2=0.852 g=3.559\n",
      ">41, 8/9, d1=0.715, d2=0.503 g=4.157\n",
      ">41, 9/9, d1=1.084, d2=0.055 g=4.021\n",
      ">42, 1/9, d1=0.998, d2=0.126 g=2.911\n",
      ">42, 2/9, d1=1.015, d2=0.224 g=2.367\n",
      ">42, 3/9, d1=0.886, d2=0.260 g=2.303\n",
      ">42, 4/9, d1=0.700, d2=0.222 g=2.396\n",
      ">42, 5/9, d1=0.681, d2=0.311 g=2.015\n",
      ">42, 6/9, d1=0.650, d2=0.274 g=1.992\n",
      ">42, 7/9, d1=0.668, d2=0.284 g=1.806\n",
      ">42, 8/9, d1=0.562, d2=0.317 g=1.536\n",
      ">42, 9/9, d1=0.536, d2=0.375 g=1.415\n",
      ">43, 1/9, d1=0.633, d2=0.399 g=1.300\n",
      ">43, 2/9, d1=0.507, d2=0.437 g=1.210\n",
      ">43, 3/9, d1=0.540, d2=0.486 g=1.124\n",
      ">43, 4/9, d1=0.550, d2=0.518 g=1.048\n",
      ">43, 5/9, d1=0.429, d2=0.538 g=1.037\n",
      ">43, 6/9, d1=0.525, d2=0.593 g=0.960\n",
      ">43, 7/9, d1=0.542, d2=0.611 g=0.938\n",
      ">43, 8/9, d1=0.567, d2=0.670 g=0.883\n",
      ">43, 9/9, d1=0.443, d2=0.707 g=0.867\n",
      ">44, 1/9, d1=0.571, d2=0.754 g=0.842\n",
      ">44, 2/9, d1=0.699, d2=0.815 g=0.901\n",
      ">44, 3/9, d1=0.612, d2=0.771 g=0.971\n",
      ">44, 4/9, d1=0.958, d2=0.773 g=1.008\n",
      ">44, 5/9, d1=0.747, d2=0.678 g=1.111\n",
      ">44, 6/9, d1=0.998, d2=0.665 g=1.120\n",
      ">44, 7/9, d1=0.934, d2=0.569 g=1.186\n",
      ">44, 8/9, d1=1.075, d2=0.522 g=1.184\n",
      ">44, 9/9, d1=1.066, d2=0.502 g=1.121\n",
      ">45, 1/9, d1=0.979, d2=0.504 g=1.116\n",
      ">45, 2/9, d1=0.828, d2=0.480 g=1.160\n",
      ">45, 3/9, d1=0.885, d2=0.486 g=1.103\n",
      ">45, 4/9, d1=0.825, d2=0.503 g=1.125\n",
      ">45, 5/9, d1=0.702, d2=0.480 g=1.157\n",
      ">45, 6/9, d1=0.724, d2=0.449 g=1.227\n",
      ">45, 7/9, d1=0.709, d2=0.423 g=1.206\n",
      ">45, 8/9, d1=0.602, d2=0.443 g=1.180\n",
      ">45, 9/9, d1=0.578, d2=0.428 g=1.231\n",
      ">46, 1/9, d1=0.507, d2=0.416 g=1.220\n",
      ">46, 2/9, d1=0.599, d2=0.426 g=1.223\n",
      ">46, 3/9, d1=0.601, d2=0.468 g=1.135\n",
      ">46, 4/9, d1=0.589, d2=0.465 g=1.104\n",
      ">46, 5/9, d1=0.572, d2=0.526 g=1.018\n",
      ">46, 6/9, d1=0.495, d2=0.522 g=0.982\n",
      ">46, 7/9, d1=0.422, d2=0.552 g=0.961\n",
      ">46, 8/9, d1=0.442, d2=0.576 g=0.942\n",
      ">46, 9/9, d1=0.411, d2=0.595 g=0.933\n",
      ">47, 1/9, d1=0.443, d2=0.559 g=0.957\n",
      ">47, 2/9, d1=0.443, d2=0.551 g=0.952\n",
      ">47, 3/9, d1=0.422, d2=0.549 g=0.974\n",
      ">47, 4/9, d1=0.503, d2=0.552 g=0.962\n",
      ">47, 5/9, d1=0.394, d2=0.529 g=1.002\n",
      ">47, 6/9, d1=0.354, d2=0.500 g=1.033\n",
      ">47, 7/9, d1=0.348, d2=0.455 g=1.082\n",
      ">47, 8/9, d1=0.308, d2=0.429 g=1.142\n",
      ">47, 9/9, d1=0.442, d2=0.410 g=1.169\n",
      ">48, 1/9, d1=0.322, d2=0.405 g=1.211\n",
      ">48, 2/9, d1=0.309, d2=0.374 g=1.249\n",
      ">48, 3/9, d1=0.354, d2=0.373 g=1.238\n",
      ">48, 4/9, d1=0.262, d2=0.352 g=1.307\n",
      ">48, 5/9, d1=0.358, d2=0.349 g=1.316\n",
      ">48, 6/9, d1=0.319, d2=0.344 g=1.309\n",
      ">48, 7/9, d1=0.266, d2=0.365 g=1.271\n",
      ">48, 8/9, d1=0.265, d2=0.358 g=1.271\n",
      ">48, 9/9, d1=0.257, d2=0.367 g=1.207\n",
      ">49, 1/9, d1=0.313, d2=0.382 g=1.179\n",
      ">49, 2/9, d1=0.143, d2=0.386 g=1.162\n",
      ">49, 3/9, d1=0.264, d2=0.416 g=1.117\n",
      ">49, 4/9, d1=0.248, d2=0.432 g=1.090\n",
      ">49, 5/9, d1=0.266, d2=0.454 g=1.070\n",
      ">49, 6/9, d1=0.143, d2=0.492 g=0.961\n",
      ">49, 7/9, d1=0.154, d2=0.578 g=0.896\n",
      ">49, 8/9, d1=0.150, d2=0.805 g=0.708\n",
      ">49, 9/9, d1=0.189, d2=1.584 g=0.399\n",
      ">50, 1/9, d1=0.249, d2=2.587 g=0.374\n",
      ">50, 2/9, d1=0.405, d2=2.624 g=0.472\n",
      ">50, 3/9, d1=0.362, d2=1.576 g=0.751\n",
      ">50, 4/9, d1=0.500, d2=0.948 g=1.494\n",
      ">50, 5/9, d1=0.829, d2=0.315 g=2.074\n",
      ">50, 6/9, d1=0.768, d2=0.206 g=2.255\n",
      ">50, 7/9, d1=0.822, d2=0.208 g=2.154\n",
      ">50, 8/9, d1=0.677, d2=0.218 g=2.119\n",
      ">50, 9/9, d1=0.739, d2=0.250 g=2.160\n",
      ">Accuracy real: 60%, fake: 95%\n",
      ">51, 1/9, d1=0.324, d2=0.304 g=2.347\n",
      ">51, 2/9, d1=0.683, d2=0.253 g=2.377\n",
      ">51, 3/9, d1=0.569, d2=0.342 g=1.992\n",
      ">51, 4/9, d1=0.644, d2=0.394 g=1.446\n",
      ">51, 5/9, d1=0.604, d2=0.632 g=1.242\n",
      ">51, 6/9, d1=0.707, d2=0.583 g=1.204\n",
      ">51, 7/9, d1=0.683, d2=0.469 g=1.278\n",
      ">51, 8/9, d1=0.660, d2=0.422 g=1.393\n",
      ">51, 9/9, d1=0.701, d2=0.390 g=1.399\n",
      ">52, 1/9, d1=0.643, d2=0.390 g=1.417\n",
      ">52, 2/9, d1=0.579, d2=0.370 g=1.423\n",
      ">52, 3/9, d1=0.636, d2=0.472 g=1.469\n",
      ">52, 4/9, d1=0.572, d2=0.388 g=1.514\n",
      ">52, 5/9, d1=0.534, d2=0.385 g=1.512\n",
      ">52, 6/9, d1=0.565, d2=0.495 g=1.365\n",
      ">52, 7/9, d1=0.608, d2=0.528 g=1.217\n",
      ">52, 8/9, d1=0.575, d2=0.519 g=1.212\n",
      ">52, 9/9, d1=0.596, d2=0.431 g=1.303\n",
      ">53, 1/9, d1=0.460, d2=0.364 g=1.482\n",
      ">53, 2/9, d1=0.537, d2=0.307 g=1.509\n",
      ">53, 3/9, d1=0.408, d2=0.325 g=1.518\n",
      ">53, 4/9, d1=0.460, d2=0.337 g=1.418\n",
      ">53, 5/9, d1=0.430, d2=0.382 g=1.354\n",
      ">53, 6/9, d1=0.305, d2=0.400 g=1.365\n",
      ">53, 7/9, d1=0.374, d2=0.427 g=1.390\n",
      ">53, 8/9, d1=0.349, d2=0.392 g=1.455\n",
      ">53, 9/9, d1=0.405, d2=0.381 g=1.497\n",
      ">54, 1/9, d1=0.400, d2=0.354 g=1.519\n",
      ">54, 2/9, d1=0.415, d2=0.345 g=1.690\n",
      ">54, 3/9, d1=0.354, d2=0.320 g=1.782\n",
      ">54, 4/9, d1=0.363, d2=0.275 g=1.909\n",
      ">54, 5/9, d1=0.475, d2=0.275 g=1.880\n",
      ">54, 6/9, d1=0.400, d2=0.318 g=1.816\n",
      ">54, 7/9, d1=0.411, d2=0.315 g=1.785\n",
      ">54, 8/9, d1=0.347, d2=0.340 g=1.606\n",
      ">54, 9/9, d1=0.599, d2=0.398 g=1.420\n",
      ">55, 1/9, d1=0.653, d2=0.515 g=1.298\n",
      ">55, 2/9, d1=0.427, d2=0.493 g=1.233\n",
      ">55, 3/9, d1=0.384, d2=0.455 g=1.213\n",
      ">55, 4/9, d1=0.392, d2=0.466 g=1.233\n",
      ">55, 5/9, d1=0.394, d2=0.465 g=1.196\n",
      ">55, 6/9, d1=0.506, d2=0.528 g=1.117\n",
      ">55, 7/9, d1=0.438, d2=0.623 g=1.001\n",
      ">55, 8/9, d1=0.344, d2=0.729 g=1.016\n",
      ">55, 9/9, d1=0.509, d2=0.835 g=1.009\n",
      ">56, 1/9, d1=0.539, d2=0.840 g=1.114\n",
      ">56, 2/9, d1=0.742, d2=0.674 g=1.218\n",
      ">56, 3/9, d1=0.536, d2=0.662 g=1.282\n",
      ">56, 4/9, d1=0.573, d2=0.595 g=1.227\n",
      ">56, 5/9, d1=0.701, d2=0.617 g=1.118\n",
      ">56, 6/9, d1=0.542, d2=0.800 g=1.008\n",
      ">56, 7/9, d1=0.592, d2=0.819 g=0.880\n",
      ">56, 8/9, d1=0.556, d2=1.120 g=0.781\n",
      ">56, 9/9, d1=0.713, d2=1.129 g=0.750\n",
      ">57, 1/9, d1=0.738, d2=0.876 g=0.831\n",
      ">57, 2/9, d1=0.772, d2=0.700 g=1.012\n",
      ">57, 3/9, d1=0.747, d2=0.487 g=1.252\n",
      ">57, 4/9, d1=0.776, d2=0.360 g=1.484\n",
      ">57, 5/9, d1=0.677, d2=0.307 g=1.604\n",
      ">57, 6/9, d1=0.572, d2=0.244 g=1.724\n",
      ">57, 7/9, d1=0.546, d2=0.225 g=1.806\n",
      ">57, 8/9, d1=0.555, d2=0.255 g=1.728\n",
      ">57, 9/9, d1=0.519, d2=0.252 g=1.728\n",
      ">58, 1/9, d1=0.436, d2=0.304 g=1.577\n",
      ">58, 2/9, d1=0.449, d2=0.371 g=1.331\n",
      ">58, 3/9, d1=0.408, d2=0.762 g=0.945\n",
      ">58, 4/9, d1=0.495, d2=1.258 g=0.774\n",
      ">58, 5/9, d1=0.628, d2=1.381 g=0.980\n",
      ">58, 6/9, d1=0.711, d2=0.880 g=1.610\n",
      ">58, 7/9, d1=0.975, d2=0.414 g=1.969\n",
      ">58, 8/9, d1=1.022, d2=0.315 g=2.082\n",
      ">58, 9/9, d1=1.002, d2=0.354 g=1.867\n",
      ">59, 1/9, d1=0.997, d2=0.386 g=1.697\n",
      ">59, 2/9, d1=1.014, d2=0.521 g=1.546\n",
      ">59, 3/9, d1=1.125, d2=0.545 g=1.290\n",
      ">59, 4/9, d1=0.891, d2=0.651 g=1.247\n",
      ">59, 5/9, d1=0.995, d2=0.594 g=1.101\n",
      ">59, 6/9, d1=0.901, d2=0.681 g=1.035\n",
      ">59, 7/9, d1=0.868, d2=0.637 g=0.996\n",
      ">59, 8/9, d1=0.884, d2=0.663 g=0.996\n",
      ">59, 9/9, d1=1.003, d2=0.653 g=0.964\n",
      ">60, 1/9, d1=0.982, d2=0.609 g=0.946\n",
      ">60, 2/9, d1=0.860, d2=0.616 g=0.979\n",
      ">60, 3/9, d1=0.675, d2=0.599 g=0.976\n",
      ">60, 4/9, d1=0.708, d2=0.583 g=0.970\n",
      ">60, 5/9, d1=0.740, d2=0.588 g=0.925\n",
      ">60, 6/9, d1=0.704, d2=0.627 g=0.878\n",
      ">60, 7/9, d1=0.680, d2=0.640 g=0.832\n",
      ">60, 8/9, d1=0.629, d2=0.664 g=0.816\n",
      ">60, 9/9, d1=0.620, d2=0.672 g=0.857\n",
      ">Accuracy real: 60%, fake: 65%\n",
      ">61, 1/9, d1=0.645, d2=0.656 g=0.871\n",
      ">61, 2/9, d1=0.628, d2=0.617 g=0.905\n",
      ">61, 3/9, d1=0.595, d2=0.594 g=0.929\n",
      ">61, 4/9, d1=0.606, d2=0.561 g=0.969\n",
      ">61, 5/9, d1=0.698, d2=0.559 g=0.968\n",
      ">61, 6/9, d1=0.561, d2=0.573 g=0.987\n",
      ">61, 7/9, d1=0.561, d2=0.576 g=0.995\n",
      ">61, 8/9, d1=0.514, d2=0.546 g=0.984\n",
      ">61, 9/9, d1=0.547, d2=0.626 g=0.999\n",
      ">62, 1/9, d1=0.593, d2=0.567 g=1.075\n",
      ">62, 2/9, d1=0.526, d2=0.487 g=1.192\n",
      ">62, 3/9, d1=0.527, d2=0.402 g=1.312\n",
      ">62, 4/9, d1=0.506, d2=0.378 g=1.462\n",
      ">62, 5/9, d1=0.488, d2=0.346 g=1.552\n",
      ">62, 6/9, d1=0.538, d2=0.301 g=1.619\n",
      ">62, 7/9, d1=0.483, d2=0.294 g=1.626\n",
      ">62, 8/9, d1=0.437, d2=0.284 g=1.663\n",
      ">62, 9/9, d1=0.455, d2=0.323 g=1.565\n",
      ">63, 1/9, d1=0.411, d2=0.398 g=1.388\n",
      ">63, 2/9, d1=0.411, d2=0.461 g=1.208\n",
      ">63, 3/9, d1=0.427, d2=0.624 g=0.967\n",
      ">63, 4/9, d1=0.393, d2=0.792 g=0.822\n",
      ">63, 5/9, d1=0.458, d2=0.923 g=0.730\n",
      ">63, 6/9, d1=0.507, d2=0.878 g=0.707\n",
      ">63, 7/9, d1=0.490, d2=0.879 g=0.685\n",
      ">63, 8/9, d1=0.582, d2=0.977 g=0.736\n",
      ">63, 9/9, d1=0.603, d2=0.825 g=0.800\n",
      ">64, 1/9, d1=0.641, d2=0.733 g=0.893\n",
      ">64, 2/9, d1=0.639, d2=0.579 g=0.960\n",
      ">64, 3/9, d1=0.736, d2=0.543 g=1.043\n",
      ">64, 4/9, d1=0.666, d2=0.474 g=1.109\n",
      ">64, 5/9, d1=0.645, d2=0.444 g=1.219\n",
      ">64, 6/9, d1=0.601, d2=0.414 g=1.257\n",
      ">64, 7/9, d1=0.593, d2=0.410 g=1.305\n",
      ">64, 8/9, d1=0.545, d2=0.377 g=1.352\n",
      ">64, 9/9, d1=0.534, d2=0.352 g=1.287\n",
      ">65, 1/9, d1=0.534, d2=0.390 g=1.221\n",
      ">65, 2/9, d1=0.438, d2=0.439 g=1.170\n",
      ">65, 3/9, d1=0.471, d2=0.452 g=1.147\n",
      ">65, 4/9, d1=0.455, d2=0.501 g=1.115\n",
      ">65, 5/9, d1=0.450, d2=0.533 g=1.024\n",
      ">65, 6/9, d1=0.465, d2=0.583 g=0.940\n",
      ">65, 7/9, d1=0.415, d2=0.618 g=0.965\n",
      ">65, 8/9, d1=0.410, d2=0.597 g=0.947\n",
      ">65, 9/9, d1=0.457, d2=0.561 g=1.012\n",
      ">66, 1/9, d1=0.329, d2=0.564 g=1.094\n",
      ">66, 2/9, d1=0.388, d2=0.536 g=1.136\n",
      ">66, 3/9, d1=0.369, d2=0.446 g=1.117\n",
      ">66, 4/9, d1=0.446, d2=0.456 g=1.191\n",
      ">66, 5/9, d1=0.264, d2=0.496 g=1.161\n",
      ">66, 6/9, d1=0.302, d2=0.537 g=1.208\n",
      ">66, 7/9, d1=0.429, d2=0.512 g=1.216\n",
      ">66, 8/9, d1=0.346, d2=0.560 g=1.255\n",
      ">66, 9/9, d1=0.363, d2=0.502 g=1.482\n",
      ">67, 1/9, d1=0.441, d2=0.509 g=1.624\n",
      ">67, 2/9, d1=0.550, d2=0.485 g=1.754\n",
      ">67, 3/9, d1=0.499, d2=0.439 g=1.888\n",
      ">67, 4/9, d1=0.611, d2=0.375 g=2.239\n",
      ">67, 5/9, d1=0.723, d2=0.320 g=2.430\n",
      ">67, 6/9, d1=0.732, d2=0.317 g=2.026\n",
      ">67, 7/9, d1=0.818, d2=0.531 g=1.954\n",
      ">67, 8/9, d1=0.536, d2=0.463 g=1.779\n",
      ">67, 9/9, d1=0.730, d2=0.565 g=1.366\n",
      ">68, 1/9, d1=0.658, d2=0.586 g=1.313\n",
      ">68, 2/9, d1=0.872, d2=0.616 g=1.044\n",
      ">68, 3/9, d1=0.736, d2=0.649 g=0.991\n",
      ">68, 4/9, d1=0.688, d2=0.661 g=0.903\n",
      ">68, 5/9, d1=0.634, d2=0.689 g=0.917\n",
      ">68, 6/9, d1=0.608, d2=0.715 g=0.937\n",
      ">68, 7/9, d1=0.740, d2=0.768 g=0.865\n",
      ">68, 8/9, d1=0.597, d2=0.840 g=0.827\n",
      ">68, 9/9, d1=0.622, d2=0.764 g=0.850\n",
      ">69, 1/9, d1=0.630, d2=0.731 g=0.913\n",
      ">69, 2/9, d1=0.528, d2=0.670 g=1.045\n",
      ">69, 3/9, d1=0.612, d2=0.613 g=1.190\n",
      ">69, 4/9, d1=0.587, d2=0.527 g=1.318\n",
      ">69, 5/9, d1=0.487, d2=0.475 g=1.379\n",
      ">69, 6/9, d1=0.591, d2=0.495 g=1.544\n",
      ">69, 7/9, d1=0.712, d2=0.403 g=1.617\n",
      ">69, 8/9, d1=0.621, d2=0.307 g=1.622\n",
      ">69, 9/9, d1=0.577, d2=0.301 g=1.691\n",
      ">70, 1/9, d1=0.595, d2=0.319 g=1.580\n",
      ">70, 2/9, d1=0.485, d2=0.329 g=1.525\n",
      ">70, 3/9, d1=0.508, d2=0.327 g=1.591\n",
      ">70, 4/9, d1=0.458, d2=0.293 g=1.552\n",
      ">70, 5/9, d1=0.590, d2=0.345 g=1.349\n",
      ">70, 6/9, d1=0.550, d2=0.372 g=1.302\n",
      ">70, 7/9, d1=0.478, d2=0.427 g=1.257\n",
      ">70, 8/9, d1=0.398, d2=0.427 g=1.230\n",
      ">70, 9/9, d1=0.434, d2=0.446 g=1.196\n",
      ">Accuracy real: 81%, fake: 97%\n",
      ">71, 1/9, d1=0.461, d2=0.496 g=1.160\n",
      ">71, 2/9, d1=0.497, d2=0.503 g=1.090\n",
      ">71, 3/9, d1=0.536, d2=0.598 g=1.050\n",
      ">71, 4/9, d1=0.211, d2=0.690 g=1.128\n",
      ">71, 5/9, d1=0.561, d2=0.823 g=1.155\n",
      ">71, 6/9, d1=0.578, d2=0.925 g=1.362\n",
      ">71, 7/9, d1=0.710, d2=0.642 g=2.112\n",
      ">71, 8/9, d1=0.781, d2=0.224 g=2.878\n",
      ">71, 9/9, d1=0.667, d2=0.143 g=3.004\n",
      ">72, 1/9, d1=0.621, d2=0.284 g=3.440\n",
      ">72, 2/9, d1=0.651, d2=0.238 g=3.778\n",
      ">72, 3/9, d1=0.635, d2=0.314 g=4.165\n",
      ">72, 4/9, d1=0.702, d2=0.253 g=4.600\n",
      ">72, 5/9, d1=0.742, d2=0.229 g=4.011\n",
      ">72, 6/9, d1=0.551, d2=0.553 g=3.662\n",
      ">72, 7/9, d1=0.627, d2=0.499 g=3.643\n",
      ">72, 8/9, d1=0.852, d2=0.647 g=2.863\n",
      ">72, 9/9, d1=1.060, d2=1.492 g=2.033\n",
      ">73, 1/9, d1=1.409, d2=1.034 g=1.383\n",
      ">73, 2/9, d1=1.692, d2=0.351 g=2.085\n",
      ">73, 3/9, d1=1.354, d2=0.203 g=2.283\n",
      ">73, 4/9, d1=1.139, d2=0.177 g=2.440\n",
      ">73, 5/9, d1=1.228, d2=0.210 g=2.035\n",
      ">73, 6/9, d1=1.065, d2=0.389 g=1.662\n",
      ">73, 7/9, d1=0.983, d2=0.578 g=1.378\n",
      ">73, 8/9, d1=0.994, d2=0.334 g=1.913\n",
      ">73, 9/9, d1=0.855, d2=0.195 g=2.208\n",
      ">74, 1/9, d1=0.787, d2=0.183 g=2.135\n",
      ">74, 2/9, d1=0.701, d2=0.266 g=1.918\n",
      ">74, 3/9, d1=0.632, d2=0.352 g=1.757\n",
      ">74, 4/9, d1=0.705, d2=0.458 g=1.466\n",
      ">74, 5/9, d1=0.651, d2=0.539 g=1.471\n",
      ">74, 6/9, d1=0.757, d2=0.565 g=1.324\n",
      ">74, 7/9, d1=0.764, d2=0.489 g=1.377\n",
      ">74, 8/9, d1=0.905, d2=0.482 g=1.299\n",
      ">74, 9/9, d1=0.679, d2=0.411 g=1.439\n",
      ">75, 1/9, d1=0.651, d2=0.467 g=1.635\n",
      ">75, 2/9, d1=0.620, d2=0.364 g=1.516\n",
      ">75, 3/9, d1=0.593, d2=0.382 g=1.484\n",
      ">75, 4/9, d1=0.564, d2=0.367 g=1.605\n",
      ">75, 5/9, d1=0.563, d2=0.332 g=1.653\n",
      ">75, 6/9, d1=0.507, d2=0.344 g=1.712\n",
      ">75, 7/9, d1=0.517, d2=0.374 g=1.676\n",
      ">75, 8/9, d1=0.484, d2=0.375 g=1.574\n",
      ">75, 9/9, d1=0.417, d2=0.397 g=1.460\n",
      ">76, 1/9, d1=0.449, d2=0.338 g=1.480\n",
      ">76, 2/9, d1=0.389, d2=0.419 g=1.447\n",
      ">76, 3/9, d1=0.412, d2=0.405 g=1.348\n",
      ">76, 4/9, d1=0.432, d2=0.600 g=1.299\n",
      ">76, 5/9, d1=0.456, d2=0.550 g=1.123\n",
      ">76, 6/9, d1=0.410, d2=0.640 g=0.978\n",
      ">76, 7/9, d1=0.482, d2=0.707 g=0.957\n",
      ">76, 8/9, d1=0.530, d2=0.681 g=0.962\n",
      ">76, 9/9, d1=0.717, d2=0.748 g=0.947\n",
      ">77, 1/9, d1=0.674, d2=0.808 g=0.998\n",
      ">77, 2/9, d1=0.622, d2=0.669 g=1.168\n",
      ">77, 3/9, d1=0.779, d2=0.598 g=1.513\n",
      ">77, 4/9, d1=0.700, d2=0.423 g=1.646\n",
      ">77, 5/9, d1=0.943, d2=0.377 g=1.670\n",
      ">77, 6/9, d1=0.900, d2=0.412 g=1.860\n",
      ">77, 7/9, d1=0.827, d2=0.329 g=1.841\n",
      ">77, 8/9, d1=0.886, d2=0.417 g=1.690\n",
      ">77, 9/9, d1=0.962, d2=0.468 g=1.361\n",
      ">78, 1/9, d1=0.886, d2=0.593 g=1.184\n",
      ">78, 2/9, d1=0.797, d2=0.620 g=1.034\n",
      ">78, 3/9, d1=0.819, d2=0.699 g=0.906\n",
      ">78, 4/9, d1=0.786, d2=0.698 g=0.824\n",
      ">78, 5/9, d1=0.716, d2=0.673 g=0.826\n",
      ">78, 6/9, d1=0.696, d2=0.652 g=0.830\n",
      ">78, 7/9, d1=0.580, d2=0.644 g=0.857\n",
      ">78, 8/9, d1=0.608, d2=0.621 g=0.875\n",
      ">78, 9/9, d1=0.512, d2=0.631 g=0.881\n",
      ">79, 1/9, d1=0.434, d2=0.601 g=0.890\n",
      ">79, 2/9, d1=0.540, d2=0.617 g=0.894\n",
      ">79, 3/9, d1=0.479, d2=0.598 g=0.891\n",
      ">79, 4/9, d1=0.475, d2=0.635 g=0.848\n",
      ">79, 5/9, d1=0.389, d2=0.644 g=0.845\n",
      ">79, 6/9, d1=0.434, d2=0.669 g=0.839\n",
      ">79, 7/9, d1=0.481, d2=0.676 g=0.819\n",
      ">79, 8/9, d1=0.445, d2=0.686 g=0.824\n",
      ">79, 9/9, d1=0.404, d2=0.696 g=0.831\n",
      ">80, 1/9, d1=0.385, d2=0.648 g=0.875\n",
      ">80, 2/9, d1=0.426, d2=0.607 g=0.948\n",
      ">80, 3/9, d1=0.413, d2=0.558 g=1.014\n",
      ">80, 4/9, d1=0.414, d2=0.525 g=1.050\n",
      ">80, 5/9, d1=0.426, d2=0.474 g=1.101\n",
      ">80, 6/9, d1=0.422, d2=0.437 g=1.165\n",
      ">80, 7/9, d1=0.391, d2=0.444 g=1.206\n",
      ">80, 8/9, d1=0.437, d2=0.469 g=1.195\n",
      ">80, 9/9, d1=0.369, d2=0.456 g=1.182\n",
      ">Accuracy real: 94%, fake: 95%\n",
      ">81, 1/9, d1=0.461, d2=0.475 g=1.145\n",
      ">81, 2/9, d1=0.350, d2=0.534 g=1.101\n",
      ">81, 3/9, d1=0.394, d2=0.527 g=1.089\n",
      ">81, 4/9, d1=0.405, d2=0.529 g=1.076\n",
      ">81, 5/9, d1=0.387, d2=0.516 g=1.144\n",
      ">81, 6/9, d1=0.399, d2=0.478 g=1.169\n",
      ">81, 7/9, d1=0.428, d2=0.457 g=1.242\n",
      ">81, 8/9, d1=0.391, d2=0.405 g=1.283\n",
      ">81, 9/9, d1=0.341, d2=0.417 g=1.311\n",
      ">82, 1/9, d1=0.397, d2=0.423 g=1.254\n",
      ">82, 2/9, d1=0.403, d2=0.481 g=1.158\n",
      ">82, 3/9, d1=0.302, d2=0.471 g=1.175\n",
      ">82, 4/9, d1=0.302, d2=0.421 g=1.314\n",
      ">82, 5/9, d1=0.281, d2=0.319 g=1.594\n",
      ">82, 6/9, d1=0.262, d2=0.294 g=1.699\n",
      ">82, 7/9, d1=0.342, d2=0.479 g=1.603\n",
      ">82, 8/9, d1=0.234, d2=0.434 g=1.552\n",
      ">82, 9/9, d1=0.246, d2=0.356 g=1.462\n",
      ">83, 1/9, d1=0.311, d2=0.363 g=1.345\n",
      ">83, 2/9, d1=0.255, d2=0.475 g=1.292\n",
      ">83, 3/9, d1=0.240, d2=0.391 g=1.178\n",
      ">83, 4/9, d1=0.245, d2=0.457 g=1.193\n",
      ">83, 5/9, d1=0.161, d2=0.623 g=1.125\n",
      ">83, 6/9, d1=0.296, d2=0.630 g=1.087\n",
      ">83, 7/9, d1=0.310, d2=0.636 g=1.038\n",
      ">83, 8/9, d1=0.253, d2=0.639 g=0.976\n",
      ">83, 9/9, d1=0.253, d2=0.621 g=0.828\n",
      ">84, 1/9, d1=0.261, d2=0.814 g=0.845\n",
      ">84, 2/9, d1=0.329, d2=1.058 g=0.813\n",
      ">84, 3/9, d1=0.300, d2=0.878 g=0.739\n",
      ">84, 4/9, d1=0.310, d2=0.987 g=0.762\n",
      ">84, 5/9, d1=0.504, d2=0.944 g=0.889\n",
      ">84, 6/9, d1=0.479, d2=0.844 g=0.839\n",
      ">84, 7/9, d1=0.690, d2=0.668 g=0.982\n",
      ">84, 8/9, d1=0.509, d2=0.653 g=1.061\n",
      ">84, 9/9, d1=0.649, d2=0.662 g=1.041\n",
      ">85, 1/9, d1=0.659, d2=0.641 g=1.114\n",
      ">85, 2/9, d1=0.678, d2=0.592 g=1.140\n",
      ">85, 3/9, d1=0.642, d2=0.523 g=1.199\n",
      ">85, 4/9, d1=0.639, d2=0.473 g=1.328\n",
      ">85, 5/9, d1=0.718, d2=0.490 g=1.207\n",
      ">85, 6/9, d1=0.597, d2=0.476 g=1.246\n",
      ">85, 7/9, d1=0.609, d2=0.491 g=1.322\n",
      ">85, 8/9, d1=0.484, d2=0.507 g=1.306\n",
      ">85, 9/9, d1=0.530, d2=0.452 g=1.304\n",
      ">86, 1/9, d1=0.562, d2=0.434 g=1.329\n",
      ">86, 2/9, d1=0.615, d2=0.508 g=1.238\n",
      ">86, 3/9, d1=0.497, d2=0.571 g=1.172\n",
      ">86, 4/9, d1=0.565, d2=0.633 g=1.096\n",
      ">86, 5/9, d1=0.611, d2=0.671 g=1.097\n",
      ">86, 6/9, d1=0.537, d2=0.584 g=1.182\n",
      ">86, 7/9, d1=0.600, d2=0.549 g=1.329\n",
      ">86, 8/9, d1=0.536, d2=0.401 g=1.561\n",
      ">86, 9/9, d1=0.543, d2=0.335 g=1.726\n",
      ">87, 1/9, d1=0.664, d2=0.409 g=1.757\n",
      ">87, 2/9, d1=0.472, d2=0.363 g=1.743\n",
      ">87, 3/9, d1=0.541, d2=0.376 g=1.729\n",
      ">87, 4/9, d1=0.574, d2=0.401 g=1.462\n",
      ">87, 5/9, d1=0.563, d2=0.591 g=1.467\n",
      ">87, 6/9, d1=0.680, d2=0.462 g=1.488\n",
      ">87, 7/9, d1=0.658, d2=0.343 g=1.762\n",
      ">87, 8/9, d1=0.543, d2=0.313 g=1.969\n",
      ">87, 9/9, d1=0.708, d2=0.298 g=1.894\n",
      ">88, 1/9, d1=0.696, d2=0.376 g=1.741\n",
      ">88, 2/9, d1=0.528, d2=0.409 g=1.743\n",
      ">88, 3/9, d1=0.613, d2=0.397 g=1.770\n",
      ">88, 4/9, d1=0.576, d2=0.573 g=1.411\n",
      ">88, 5/9, d1=0.648, d2=1.115 g=1.130\n",
      ">88, 6/9, d1=0.765, d2=0.931 g=1.058\n",
      ">88, 7/9, d1=0.791, d2=0.688 g=1.324\n",
      ">88, 8/9, d1=0.956, d2=0.537 g=1.372\n",
      ">88, 9/9, d1=0.893, d2=0.639 g=1.158\n",
      ">89, 1/9, d1=0.660, d2=0.839 g=1.127\n",
      ">89, 2/9, d1=0.733, d2=0.948 g=1.166\n",
      ">89, 3/9, d1=0.904, d2=1.012 g=1.298\n",
      ">89, 4/9, d1=0.917, d2=1.275 g=1.449\n",
      ">89, 5/9, d1=1.070, d2=0.470 g=2.176\n",
      ">89, 6/9, d1=1.131, d2=0.166 g=2.617\n",
      ">89, 7/9, d1=1.123, d2=0.171 g=2.495\n",
      ">89, 8/9, d1=0.846, d2=0.185 g=2.440\n",
      ">89, 9/9, d1=0.772, d2=0.180 g=2.329\n",
      ">90, 1/9, d1=0.739, d2=0.214 g=2.654\n",
      ">90, 2/9, d1=0.697, d2=0.152 g=2.765\n",
      ">90, 3/9, d1=0.630, d2=0.247 g=2.574\n",
      ">90, 4/9, d1=0.609, d2=0.371 g=2.193\n",
      ">90, 5/9, d1=0.610, d2=0.669 g=1.273\n",
      ">90, 6/9, d1=0.724, d2=0.912 g=1.166\n",
      ">90, 7/9, d1=0.737, d2=0.622 g=1.090\n",
      ">90, 8/9, d1=0.637, d2=0.526 g=1.252\n",
      ">90, 9/9, d1=0.698, d2=0.371 g=1.546\n",
      ">Accuracy real: 45%, fake: 100%\n",
      ">91, 1/9, d1=0.431, d2=0.273 g=1.877\n",
      ">91, 2/9, d1=0.699, d2=0.203 g=1.975\n",
      ">91, 3/9, d1=0.746, d2=0.192 g=2.020\n",
      ">91, 4/9, d1=0.714, d2=0.185 g=2.013\n",
      ">91, 5/9, d1=0.576, d2=0.189 g=1.967\n",
      ">91, 6/9, d1=0.526, d2=0.215 g=1.830\n",
      ">91, 7/9, d1=0.495, d2=0.276 g=1.715\n",
      ">91, 8/9, d1=0.492, d2=0.318 g=1.674\n",
      ">91, 9/9, d1=0.572, d2=0.287 g=1.668\n",
      ">92, 1/9, d1=0.527, d2=0.299 g=1.725\n",
      ">92, 2/9, d1=0.504, d2=0.262 g=1.764\n",
      ">92, 3/9, d1=0.447, d2=0.264 g=1.774\n",
      ">92, 4/9, d1=0.431, d2=0.306 g=1.640\n",
      ">92, 5/9, d1=0.489, d2=0.377 g=1.573\n",
      ">92, 6/9, d1=0.487, d2=0.470 g=1.500\n",
      ">92, 7/9, d1=0.416, d2=0.566 g=1.525\n",
      ">92, 8/9, d1=0.585, d2=0.479 g=1.778\n",
      ">92, 9/9, d1=0.529, d2=0.381 g=2.028\n",
      ">93, 1/9, d1=0.681, d2=0.311 g=2.064\n",
      ">93, 2/9, d1=0.881, d2=0.401 g=1.815\n",
      ">93, 3/9, d1=0.828, d2=0.673 g=1.471\n",
      ">93, 4/9, d1=0.789, d2=0.438 g=1.345\n",
      ">93, 5/9, d1=0.812, d2=0.602 g=1.329\n",
      ">93, 6/9, d1=0.783, d2=0.457 g=1.372\n",
      ">93, 7/9, d1=0.699, d2=0.428 g=1.374\n",
      ">93, 8/9, d1=0.619, d2=0.444 g=1.428\n",
      ">93, 9/9, d1=0.575, d2=0.538 g=1.440\n",
      ">94, 1/9, d1=0.549, d2=0.515 g=1.530\n",
      ">94, 2/9, d1=0.581, d2=0.443 g=1.397\n",
      ">94, 3/9, d1=0.543, d2=0.480 g=1.313\n",
      ">94, 4/9, d1=0.538, d2=0.456 g=1.311\n",
      ">94, 5/9, d1=0.607, d2=0.393 g=1.463\n",
      ">94, 6/9, d1=0.563, d2=0.361 g=1.493\n",
      ">94, 7/9, d1=0.592, d2=0.333 g=1.537\n",
      ">94, 8/9, d1=0.528, d2=0.311 g=1.472\n",
      ">94, 9/9, d1=0.418, d2=0.318 g=1.407\n",
      ">95, 1/9, d1=0.465, d2=0.340 g=1.412\n",
      ">95, 2/9, d1=0.489, d2=0.366 g=1.296\n",
      ">95, 3/9, d1=0.420, d2=0.431 g=1.201\n",
      ">95, 4/9, d1=0.473, d2=0.493 g=1.047\n",
      ">95, 5/9, d1=0.445, d2=0.565 g=0.961\n",
      ">95, 6/9, d1=0.388, d2=0.614 g=0.913\n",
      ">95, 7/9, d1=0.354, d2=0.766 g=0.786\n",
      ">95, 8/9, d1=0.387, d2=0.808 g=0.774\n",
      ">95, 9/9, d1=0.373, d2=0.736 g=0.703\n",
      ">96, 1/9, d1=0.402, d2=0.812 g=0.684\n",
      ">96, 2/9, d1=0.410, d2=0.855 g=0.616\n",
      ">96, 3/9, d1=0.315, d2=0.866 g=0.615\n",
      ">96, 4/9, d1=0.362, d2=0.850 g=0.615\n",
      ">96, 5/9, d1=0.329, d2=0.859 g=0.622\n",
      ">96, 6/9, d1=0.335, d2=0.946 g=0.652\n",
      ">96, 7/9, d1=0.325, d2=0.926 g=0.649\n",
      ">96, 8/9, d1=0.397, d2=0.857 g=0.675\n",
      ">96, 9/9, d1=0.392, d2=0.805 g=0.652\n",
      ">97, 1/9, d1=0.412, d2=0.802 g=0.695\n",
      ">97, 2/9, d1=0.381, d2=0.842 g=0.717\n",
      ">97, 3/9, d1=0.307, d2=0.754 g=0.793\n",
      ">97, 4/9, d1=0.437, d2=0.706 g=0.825\n",
      ">97, 5/9, d1=0.427, d2=0.651 g=0.844\n",
      ">97, 6/9, d1=0.442, d2=0.657 g=0.854\n",
      ">97, 7/9, d1=0.411, d2=0.657 g=0.947\n",
      ">97, 8/9, d1=0.389, d2=0.618 g=1.058\n",
      ">97, 9/9, d1=0.494, d2=0.505 g=1.116\n",
      ">98, 1/9, d1=0.460, d2=0.529 g=1.139\n",
      ">98, 2/9, d1=0.478, d2=0.496 g=1.189\n",
      ">98, 3/9, d1=0.465, d2=0.463 g=1.290\n",
      ">98, 4/9, d1=0.497, d2=0.460 g=1.271\n",
      ">98, 5/9, d1=0.470, d2=0.450 g=1.362\n",
      ">98, 6/9, d1=0.522, d2=0.390 g=1.487\n",
      ">98, 7/9, d1=0.461, d2=0.399 g=1.450\n",
      ">98, 8/9, d1=0.446, d2=0.366 g=1.486\n",
      ">98, 9/9, d1=0.397, d2=0.378 g=1.434\n",
      ">99, 1/9, d1=0.377, d2=0.481 g=1.542\n",
      ">99, 2/9, d1=0.341, d2=0.451 g=1.325\n",
      ">99, 3/9, d1=0.395, d2=0.443 g=1.270\n",
      ">99, 4/9, d1=0.481, d2=0.527 g=1.226\n",
      ">99, 5/9, d1=0.371, d2=0.603 g=1.138\n",
      ">99, 6/9, d1=0.536, d2=0.636 g=1.068\n",
      ">99, 7/9, d1=0.671, d2=0.633 g=0.906\n",
      ">99, 8/9, d1=0.601, d2=0.818 g=0.984\n",
      ">99, 9/9, d1=0.568, d2=0.700 g=1.111\n",
      ">100, 1/9, d1=0.635, d2=0.538 g=1.161\n",
      ">100, 2/9, d1=0.566, d2=0.560 g=1.184\n",
      ">100, 3/9, d1=0.572, d2=0.480 g=1.431\n",
      ">100, 4/9, d1=0.548, d2=0.389 g=1.526\n",
      ">100, 5/9, d1=0.639, d2=0.401 g=1.459\n",
      ">100, 6/9, d1=0.491, d2=0.401 g=1.400\n",
      ">100, 7/9, d1=0.571, d2=0.503 g=1.317\n",
      ">100, 8/9, d1=0.522, d2=0.473 g=1.188\n",
      ">100, 9/9, d1=0.519, d2=0.618 g=1.057\n",
      ">Accuracy real: 76%, fake: 71%\n",
      ">101, 1/9, d1=0.583, d2=0.575 g=1.027\n",
      ">101, 2/9, d1=0.398, d2=0.638 g=1.068\n",
      ">101, 3/9, d1=0.554, d2=0.529 g=1.079\n",
      ">101, 4/9, d1=0.568, d2=0.562 g=1.051\n",
      ">101, 5/9, d1=0.472, d2=0.580 g=1.120\n",
      ">101, 6/9, d1=0.482, d2=0.550 g=1.161\n",
      ">101, 7/9, d1=0.519, d2=0.507 g=1.249\n",
      ">101, 8/9, d1=0.495, d2=0.468 g=1.281\n",
      ">101, 9/9, d1=0.558, d2=0.406 g=1.345\n",
      ">102, 1/9, d1=0.546, d2=0.412 g=1.279\n",
      ">102, 2/9, d1=0.399, d2=0.437 g=1.219\n",
      ">102, 3/9, d1=0.396, d2=0.459 g=1.244\n",
      ">102, 4/9, d1=0.440, d2=0.469 g=1.192\n",
      ">102, 5/9, d1=0.443, d2=0.534 g=1.155\n",
      ">102, 6/9, d1=0.454, d2=0.559 g=1.186\n",
      ">102, 7/9, d1=0.483, d2=0.547 g=1.313\n",
      ">102, 8/9, d1=0.370, d2=0.461 g=1.422\n",
      ">102, 9/9, d1=0.422, d2=0.404 g=1.547\n",
      ">103, 1/9, d1=0.489, d2=0.401 g=1.555\n",
      ">103, 2/9, d1=0.412, d2=0.383 g=1.390\n",
      ">103, 3/9, d1=0.469, d2=0.450 g=1.290\n",
      ">103, 4/9, d1=0.394, d2=0.617 g=1.184\n",
      ">103, 5/9, d1=0.362, d2=0.597 g=1.164\n",
      ">103, 6/9, d1=0.462, d2=0.687 g=1.007\n",
      ">103, 7/9, d1=0.424, d2=0.675 g=0.967\n",
      ">103, 8/9, d1=0.389, d2=0.672 g=0.879\n",
      ">103, 9/9, d1=0.424, d2=0.769 g=0.948\n",
      ">104, 1/9, d1=0.524, d2=0.828 g=0.969\n",
      ">104, 2/9, d1=0.655, d2=0.598 g=0.941\n",
      ">104, 3/9, d1=0.545, d2=0.660 g=1.078\n",
      ">104, 4/9, d1=0.644, d2=0.520 g=1.079\n",
      ">104, 5/9, d1=0.499, d2=0.642 g=1.191\n",
      ">104, 6/9, d1=0.738, d2=0.551 g=1.316\n",
      ">104, 7/9, d1=0.550, d2=0.501 g=1.285\n",
      ">104, 8/9, d1=0.629, d2=0.502 g=1.268\n",
      ">104, 9/9, d1=0.614, d2=0.534 g=1.173\n",
      ">105, 1/9, d1=0.672, d2=0.651 g=1.141\n",
      ">105, 2/9, d1=0.631, d2=0.721 g=1.186\n",
      ">105, 3/9, d1=0.647, d2=0.791 g=1.486\n",
      ">105, 4/9, d1=0.737, d2=0.658 g=1.520\n",
      ">105, 5/9, d1=0.771, d2=0.814 g=1.728\n",
      ">105, 6/9, d1=0.900, d2=0.465 g=2.157\n",
      ">105, 7/9, d1=0.899, d2=0.418 g=1.946\n",
      ">105, 8/9, d1=1.074, d2=0.424 g=1.589\n",
      ">105, 9/9, d1=0.823, d2=0.574 g=1.275\n",
      ">106, 1/9, d1=0.689, d2=0.665 g=1.058\n",
      ">106, 2/9, d1=0.725, d2=0.787 g=1.000\n",
      ">106, 3/9, d1=0.700, d2=0.789 g=0.828\n",
      ">106, 4/9, d1=0.651, d2=0.988 g=0.735\n",
      ">106, 5/9, d1=0.546, d2=1.259 g=0.793\n",
      ">106, 6/9, d1=0.645, d2=0.883 g=0.895\n",
      ">106, 7/9, d1=0.653, d2=0.711 g=0.948\n",
      ">106, 8/9, d1=0.755, d2=0.552 g=1.067\n",
      ">106, 9/9, d1=0.582, d2=0.541 g=1.155\n",
      ">107, 1/9, d1=0.686, d2=0.480 g=1.196\n",
      ">107, 2/9, d1=0.566, d2=0.442 g=1.179\n",
      ">107, 3/9, d1=0.608, d2=0.539 g=1.162\n",
      ">107, 4/9, d1=0.514, d2=0.619 g=1.087\n",
      ">107, 5/9, d1=0.494, d2=0.652 g=1.091\n",
      ">107, 6/9, d1=0.530, d2=0.663 g=1.136\n",
      ">107, 7/9, d1=0.622, d2=0.545 g=1.113\n",
      ">107, 8/9, d1=0.543, d2=0.519 g=1.196\n",
      ">107, 9/9, d1=0.593, d2=0.447 g=1.149\n",
      ">108, 1/9, d1=0.523, d2=0.534 g=1.053\n",
      ">108, 2/9, d1=0.521, d2=0.601 g=1.065\n",
      ">108, 3/9, d1=0.478, d2=0.534 g=1.065\n",
      ">108, 4/9, d1=0.593, d2=0.581 g=1.060\n",
      ">108, 5/9, d1=0.501, d2=0.550 g=1.075\n",
      ">108, 6/9, d1=0.618, d2=0.536 g=1.102\n",
      ">108, 7/9, d1=0.587, d2=0.480 g=1.095\n",
      ">108, 8/9, d1=0.488, d2=0.486 g=1.127\n",
      ">108, 9/9, d1=0.624, d2=0.529 g=1.195\n",
      ">109, 1/9, d1=0.550, d2=0.454 g=1.217\n",
      ">109, 2/9, d1=0.562, d2=0.439 g=1.198\n",
      ">109, 3/9, d1=0.518, d2=0.439 g=1.192\n",
      ">109, 4/9, d1=0.546, d2=0.408 g=1.208\n",
      ">109, 5/9, d1=0.525, d2=0.460 g=1.210\n",
      ">109, 6/9, d1=0.490, d2=0.506 g=1.232\n",
      ">109, 7/9, d1=0.592, d2=0.502 g=1.165\n",
      ">109, 8/9, d1=0.454, d2=0.583 g=1.193\n",
      ">109, 9/9, d1=0.452, d2=0.661 g=1.236\n",
      ">110, 1/9, d1=0.503, d2=0.475 g=1.247\n",
      ">110, 2/9, d1=0.595, d2=0.468 g=1.194\n",
      ">110, 3/9, d1=0.589, d2=0.529 g=1.233\n",
      ">110, 4/9, d1=0.588, d2=0.596 g=1.222\n",
      ">110, 5/9, d1=0.636, d2=0.663 g=1.173\n",
      ">110, 6/9, d1=0.728, d2=0.666 g=1.195\n",
      ">110, 7/9, d1=0.744, d2=0.617 g=1.450\n",
      ">110, 8/9, d1=0.765, d2=0.489 g=1.576\n",
      ">110, 9/9, d1=0.700, d2=0.412 g=1.616\n",
      ">Accuracy real: 53%, fake: 93%\n",
      ">111, 1/9, d1=0.551, d2=0.431 g=1.396\n",
      ">111, 2/9, d1=0.650, d2=0.584 g=1.239\n",
      ">111, 3/9, d1=0.850, d2=0.695 g=1.026\n",
      ">111, 4/9, d1=0.884, d2=0.899 g=0.831\n",
      ">111, 5/9, d1=0.770, d2=0.903 g=0.795\n",
      ">111, 6/9, d1=0.778, d2=0.878 g=0.793\n",
      ">111, 7/9, d1=0.769, d2=0.839 g=0.748\n",
      ">111, 8/9, d1=0.771, d2=0.782 g=0.826\n",
      ">111, 9/9, d1=0.745, d2=0.751 g=0.907\n",
      ">112, 1/9, d1=0.767, d2=0.691 g=1.044\n",
      ">112, 2/9, d1=0.751, d2=0.610 g=1.157\n",
      ">112, 3/9, d1=0.862, d2=0.518 g=1.175\n",
      ">112, 4/9, d1=0.769, d2=0.428 g=1.293\n",
      ">112, 5/9, d1=0.745, d2=0.420 g=1.282\n",
      ">112, 6/9, d1=0.777, d2=0.440 g=1.293\n",
      ">112, 7/9, d1=0.724, d2=0.414 g=1.280\n",
      ">112, 8/9, d1=0.730, d2=0.466 g=1.348\n",
      ">112, 9/9, d1=0.684, d2=0.394 g=1.403\n",
      ">113, 1/9, d1=0.675, d2=0.544 g=1.393\n",
      ">113, 2/9, d1=0.610, d2=0.353 g=1.426\n",
      ">113, 3/9, d1=0.648, d2=0.426 g=1.377\n",
      ">113, 4/9, d1=0.596, d2=0.403 g=1.430\n",
      ">113, 5/9, d1=0.622, d2=0.365 g=1.438\n",
      ">113, 6/9, d1=0.639, d2=0.353 g=1.347\n",
      ">113, 7/9, d1=0.679, d2=0.449 g=1.276\n",
      ">113, 8/9, d1=0.539, d2=0.364 g=1.269\n",
      ">113, 9/9, d1=0.542, d2=0.449 g=1.296\n",
      ">114, 1/9, d1=0.558, d2=0.388 g=1.263\n",
      ">114, 2/9, d1=0.483, d2=0.420 g=1.223\n",
      ">114, 3/9, d1=0.558, d2=0.369 g=1.251\n",
      ">114, 4/9, d1=0.449, d2=0.550 g=1.201\n",
      ">114, 5/9, d1=0.533, d2=0.447 g=1.154\n",
      ">114, 6/9, d1=0.515, d2=0.459 g=1.171\n",
      ">114, 7/9, d1=0.558, d2=0.438 g=1.020\n",
      ">114, 8/9, d1=0.490, d2=0.501 g=1.065\n",
      ">114, 9/9, d1=0.484, d2=0.624 g=1.048\n",
      ">115, 1/9, d1=0.493, d2=0.637 g=0.948\n",
      ">115, 2/9, d1=0.487, d2=0.585 g=0.885\n",
      ">115, 3/9, d1=0.547, d2=0.688 g=0.798\n",
      ">115, 4/9, d1=0.529, d2=0.976 g=0.795\n",
      ">115, 5/9, d1=0.548, d2=0.911 g=0.791\n",
      ">115, 6/9, d1=0.577, d2=0.854 g=1.103\n",
      ">115, 7/9, d1=0.657, d2=0.552 g=1.519\n",
      ">115, 8/9, d1=0.687, d2=0.288 g=1.835\n",
      ">115, 9/9, d1=0.545, d2=0.233 g=1.936\n",
      ">116, 1/9, d1=0.594, d2=0.231 g=1.918\n",
      ">116, 2/9, d1=0.624, d2=0.295 g=1.787\n",
      ">116, 3/9, d1=0.543, d2=0.335 g=1.600\n",
      ">116, 4/9, d1=0.465, d2=0.473 g=1.507\n",
      ">116, 5/9, d1=0.415, d2=0.453 g=1.440\n",
      ">116, 6/9, d1=0.521, d2=0.666 g=1.128\n",
      ">116, 7/9, d1=0.489, d2=0.739 g=1.018\n",
      ">116, 8/9, d1=0.517, d2=0.805 g=0.852\n",
      ">116, 9/9, d1=0.677, d2=0.838 g=0.774\n",
      ">117, 1/9, d1=0.578, d2=0.859 g=0.857\n",
      ">117, 2/9, d1=0.703, d2=0.720 g=1.018\n",
      ">117, 3/9, d1=0.746, d2=0.639 g=1.132\n",
      ">117, 4/9, d1=0.719, d2=0.573 g=1.243\n",
      ">117, 5/9, d1=0.721, d2=0.552 g=1.211\n",
      ">117, 6/9, d1=0.791, d2=0.553 g=1.284\n",
      ">117, 7/9, d1=0.652, d2=0.470 g=1.264\n",
      ">117, 8/9, d1=0.660, d2=0.433 g=1.374\n",
      ">117, 9/9, d1=0.671, d2=0.452 g=1.268\n",
      ">118, 1/9, d1=0.577, d2=0.426 g=1.249\n",
      ">118, 2/9, d1=0.531, d2=0.454 g=1.294\n",
      ">118, 3/9, d1=0.562, d2=0.442 g=1.201\n",
      ">118, 4/9, d1=0.553, d2=0.444 g=1.192\n",
      ">118, 5/9, d1=0.520, d2=0.444 g=1.188\n",
      ">118, 6/9, d1=0.535, d2=0.468 g=1.148\n",
      ">118, 7/9, d1=0.478, d2=0.471 g=1.148\n",
      ">118, 8/9, d1=0.535, d2=0.481 g=1.106\n",
      ">118, 9/9, d1=0.526, d2=0.493 g=1.134\n",
      ">119, 1/9, d1=0.475, d2=0.474 g=1.118\n",
      ">119, 2/9, d1=0.494, d2=0.487 g=1.094\n",
      ">119, 3/9, d1=0.459, d2=0.488 g=1.189\n",
      ">119, 4/9, d1=0.653, d2=0.486 g=1.117\n",
      ">119, 5/9, d1=0.447, d2=0.485 g=1.163\n",
      ">119, 6/9, d1=0.456, d2=0.496 g=1.140\n",
      ">119, 7/9, d1=0.455, d2=0.495 g=1.094\n",
      ">119, 8/9, d1=0.466, d2=0.542 g=1.017\n",
      ">119, 9/9, d1=0.641, d2=0.573 g=1.020\n",
      ">120, 1/9, d1=0.434, d2=0.560 g=0.931\n",
      ">120, 2/9, d1=0.557, d2=0.601 g=0.933\n",
      ">120, 3/9, d1=0.506, d2=0.654 g=0.930\n",
      ">120, 4/9, d1=0.489, d2=0.625 g=0.937\n",
      ">120, 5/9, d1=0.525, d2=0.646 g=0.929\n",
      ">120, 6/9, d1=0.568, d2=0.622 g=0.913\n",
      ">120, 7/9, d1=0.567, d2=0.641 g=0.910\n",
      ">120, 8/9, d1=0.553, d2=0.669 g=0.950\n",
      ">120, 9/9, d1=0.605, d2=0.594 g=0.987\n",
      ">Accuracy real: 65%, fake: 78%\n",
      ">121, 1/9, d1=0.612, d2=0.598 g=0.981\n",
      ">121, 2/9, d1=0.638, d2=0.645 g=0.957\n",
      ">121, 3/9, d1=0.657, d2=0.610 g=1.044\n",
      ">121, 4/9, d1=0.594, d2=0.560 g=1.094\n",
      ">121, 5/9, d1=0.721, d2=0.577 g=1.107\n",
      ">121, 6/9, d1=0.618, d2=0.548 g=1.145\n",
      ">121, 7/9, d1=0.631, d2=0.517 g=1.237\n",
      ">121, 8/9, d1=0.584, d2=0.428 g=1.321\n",
      ">121, 9/9, d1=0.671, d2=0.407 g=1.314\n",
      ">122, 1/9, d1=0.737, d2=0.395 g=1.434\n",
      ">122, 2/9, d1=0.589, d2=0.417 g=1.367\n",
      ">122, 3/9, d1=0.703, d2=0.363 g=1.411\n",
      ">122, 4/9, d1=0.651, d2=0.448 g=1.339\n",
      ">122, 5/9, d1=0.623, d2=0.478 g=1.330\n",
      ">122, 6/9, d1=0.579, d2=0.455 g=1.203\n",
      ">122, 7/9, d1=0.508, d2=0.497 g=1.128\n",
      ">122, 8/9, d1=0.513, d2=0.521 g=1.108\n",
      ">122, 9/9, d1=0.644, d2=0.573 g=1.074\n",
      ">123, 1/9, d1=0.576, d2=0.588 g=1.147\n",
      ">123, 2/9, d1=0.549, d2=0.460 g=1.300\n",
      ">123, 3/9, d1=0.524, d2=0.434 g=1.462\n",
      ">123, 4/9, d1=0.610, d2=0.425 g=1.301\n",
      ">123, 5/9, d1=0.445, d2=0.408 g=1.288\n",
      ">123, 6/9, d1=0.358, d2=0.500 g=1.442\n",
      ">123, 7/9, d1=0.429, d2=0.418 g=1.382\n",
      ">123, 8/9, d1=0.368, d2=0.525 g=1.390\n",
      ">123, 9/9, d1=0.388, d2=0.500 g=1.272\n",
      ">124, 1/9, d1=0.469, d2=0.518 g=1.339\n",
      ">124, 2/9, d1=0.476, d2=0.754 g=1.349\n",
      ">124, 3/9, d1=0.582, d2=0.553 g=1.376\n",
      ">124, 4/9, d1=0.568, d2=0.569 g=1.395\n",
      ">124, 5/9, d1=0.609, d2=0.632 g=1.463\n",
      ">124, 6/9, d1=0.574, d2=0.605 g=1.688\n",
      ">124, 7/9, d1=0.669, d2=0.587 g=1.376\n",
      ">124, 8/9, d1=0.706, d2=0.645 g=1.205\n",
      ">124, 9/9, d1=0.605, d2=0.631 g=1.120\n",
      ">125, 1/9, d1=0.602, d2=0.647 g=0.994\n",
      ">125, 2/9, d1=0.543, d2=0.706 g=0.909\n",
      ">125, 3/9, d1=0.645, d2=0.799 g=0.775\n",
      ">125, 4/9, d1=0.604, d2=0.861 g=0.807\n",
      ">125, 5/9, d1=0.505, d2=0.737 g=0.837\n",
      ">125, 6/9, d1=0.556, d2=0.678 g=0.908\n",
      ">125, 7/9, d1=0.572, d2=0.638 g=1.020\n",
      ">125, 8/9, d1=0.479, d2=0.591 g=1.050\n",
      ">125, 9/9, d1=0.495, d2=0.629 g=1.051\n",
      ">126, 1/9, d1=0.484, d2=0.714 g=1.088\n",
      ">126, 2/9, d1=0.626, d2=0.732 g=1.318\n",
      ">126, 3/9, d1=0.642, d2=0.537 g=1.532\n",
      ">126, 4/9, d1=0.826, d2=0.607 g=1.592\n",
      ">126, 5/9, d1=0.674, d2=0.367 g=1.606\n",
      ">126, 6/9, d1=0.742, d2=0.541 g=1.536\n",
      ">126, 7/9, d1=0.651, d2=0.540 g=1.400\n",
      ">126, 8/9, d1=0.590, d2=0.508 g=1.465\n",
      ">126, 9/9, d1=0.653, d2=0.603 g=1.238\n",
      ">127, 1/9, d1=0.631, d2=0.535 g=1.146\n",
      ">127, 2/9, d1=0.584, d2=0.538 g=1.080\n",
      ">127, 3/9, d1=0.628, d2=0.628 g=1.050\n",
      ">127, 4/9, d1=0.669, d2=0.663 g=0.966\n",
      ">127, 5/9, d1=0.656, d2=0.734 g=0.952\n",
      ">127, 6/9, d1=0.703, d2=0.726 g=0.940\n",
      ">127, 7/9, d1=0.690, d2=0.720 g=1.009\n",
      ">127, 8/9, d1=0.752, d2=0.598 g=1.008\n",
      ">127, 9/9, d1=0.636, d2=0.610 g=1.068\n",
      ">128, 1/9, d1=0.688, d2=0.593 g=1.004\n",
      ">128, 2/9, d1=0.679, d2=0.599 g=0.963\n",
      ">128, 3/9, d1=0.665, d2=0.631 g=0.949\n",
      ">128, 4/9, d1=0.633, d2=0.645 g=0.976\n",
      ">128, 5/9, d1=0.673, d2=0.581 g=1.065\n",
      ">128, 6/9, d1=0.723, d2=0.572 g=1.063\n",
      ">128, 7/9, d1=0.718, d2=0.581 g=1.098\n",
      ">128, 8/9, d1=0.694, d2=0.495 g=1.174\n",
      ">128, 9/9, d1=0.826, d2=0.468 g=1.222\n",
      ">129, 1/9, d1=0.681, d2=0.453 g=1.248\n",
      ">129, 2/9, d1=0.634, d2=0.448 g=1.250\n",
      ">129, 3/9, d1=0.762, d2=0.500 g=1.187\n",
      ">129, 4/9, d1=0.773, d2=0.541 g=1.042\n",
      ">129, 5/9, d1=0.650, d2=0.557 g=1.035\n",
      ">129, 6/9, d1=0.602, d2=0.546 g=1.044\n",
      ">129, 7/9, d1=0.621, d2=0.598 g=1.042\n",
      ">129, 8/9, d1=0.713, d2=0.552 g=0.955\n",
      ">129, 9/9, d1=0.614, d2=0.700 g=0.978\n",
      ">130, 1/9, d1=0.631, d2=0.632 g=0.935\n",
      ">130, 2/9, d1=0.722, d2=0.685 g=0.880\n",
      ">130, 3/9, d1=0.735, d2=0.717 g=0.965\n",
      ">130, 4/9, d1=0.757, d2=0.634 g=0.983\n",
      ">130, 5/9, d1=0.690, d2=0.618 g=0.979\n",
      ">130, 6/9, d1=0.689, d2=0.647 g=1.069\n",
      ">130, 7/9, d1=0.758, d2=0.610 g=1.102\n",
      ">130, 8/9, d1=0.688, d2=0.597 g=1.004\n",
      ">130, 9/9, d1=0.710, d2=0.678 g=1.012\n",
      ">Accuracy real: 52%, fake: 67%\n",
      ">131, 1/9, d1=0.671, d2=0.683 g=0.968\n",
      ">131, 2/9, d1=0.744, d2=0.629 g=0.979\n",
      ">131, 3/9, d1=0.686, d2=0.641 g=1.039\n",
      ">131, 4/9, d1=0.711, d2=0.624 g=1.212\n",
      ">131, 5/9, d1=0.625, d2=0.460 g=1.301\n",
      ">131, 6/9, d1=0.679, d2=0.368 g=1.450\n",
      ">131, 7/9, d1=0.683, d2=0.331 g=1.625\n",
      ">131, 8/9, d1=0.658, d2=0.373 g=1.677\n",
      ">131, 9/9, d1=0.588, d2=0.319 g=1.673\n",
      ">132, 1/9, d1=0.562, d2=0.337 g=1.499\n",
      ">132, 2/9, d1=0.498, d2=0.462 g=1.261\n",
      ">132, 3/9, d1=0.557, d2=0.555 g=1.088\n",
      ">132, 4/9, d1=0.441, d2=0.579 g=0.935\n",
      ">132, 5/9, d1=0.532, d2=0.602 g=0.910\n",
      ">132, 6/9, d1=0.585, d2=0.626 g=0.910\n",
      ">132, 7/9, d1=0.512, d2=0.639 g=0.952\n",
      ">132, 8/9, d1=0.522, d2=0.525 g=0.981\n",
      ">132, 9/9, d1=0.530, d2=0.576 g=1.044\n",
      ">133, 1/9, d1=0.493, d2=0.501 g=1.082\n",
      ">133, 2/9, d1=0.564, d2=0.466 g=1.096\n",
      ">133, 3/9, d1=0.554, d2=0.488 g=1.110\n",
      ">133, 4/9, d1=0.522, d2=0.524 g=1.077\n",
      ">133, 5/9, d1=0.474, d2=0.498 g=1.046\n",
      ">133, 6/9, d1=0.468, d2=0.527 g=1.047\n",
      ">133, 7/9, d1=0.404, d2=0.502 g=1.088\n",
      ">133, 8/9, d1=0.444, d2=0.480 g=1.043\n",
      ">133, 9/9, d1=0.412, d2=0.497 g=1.044\n",
      ">134, 1/9, d1=0.577, d2=0.526 g=1.025\n",
      ">134, 2/9, d1=0.511, d2=0.565 g=0.982\n",
      ">134, 3/9, d1=0.518, d2=0.607 g=0.971\n",
      ">134, 4/9, d1=0.452, d2=0.658 g=0.989\n",
      ">134, 5/9, d1=0.541, d2=0.638 g=0.981\n",
      ">134, 6/9, d1=0.446, d2=0.795 g=0.972\n",
      ">134, 7/9, d1=0.630, d2=0.807 g=0.995\n",
      ">134, 8/9, d1=0.574, d2=0.670 g=1.063\n",
      ">134, 9/9, d1=0.681, d2=0.763 g=1.245\n",
      ">135, 1/9, d1=0.768, d2=0.471 g=1.528\n",
      ">135, 2/9, d1=0.956, d2=0.502 g=1.557\n",
      ">135, 3/9, d1=0.884, d2=0.460 g=1.426\n",
      ">135, 4/9, d1=1.038, d2=0.569 g=1.269\n",
      ">135, 5/9, d1=0.999, d2=0.717 g=1.033\n",
      ">135, 6/9, d1=0.840, d2=0.716 g=0.873\n",
      ">135, 7/9, d1=0.908, d2=0.884 g=0.813\n",
      ">135, 8/9, d1=0.860, d2=0.809 g=0.795\n",
      ">135, 9/9, d1=0.860, d2=0.772 g=0.863\n",
      ">136, 1/9, d1=0.829, d2=0.677 g=0.945\n",
      ">136, 2/9, d1=0.764, d2=0.559 g=1.071\n",
      ">136, 3/9, d1=0.774, d2=0.468 g=1.196\n",
      ">136, 4/9, d1=0.787, d2=0.424 g=1.264\n",
      ">136, 5/9, d1=0.705, d2=0.413 g=1.295\n",
      ">136, 6/9, d1=0.621, d2=0.406 g=1.245\n",
      ">136, 7/9, d1=0.600, d2=0.429 g=1.202\n",
      ">136, 8/9, d1=0.565, d2=0.480 g=1.106\n",
      ">136, 9/9, d1=0.622, d2=0.516 g=1.047\n",
      ">137, 1/9, d1=0.671, d2=0.595 g=0.917\n",
      ">137, 2/9, d1=0.490, d2=0.657 g=0.828\n",
      ">137, 3/9, d1=0.618, d2=0.782 g=0.822\n",
      ">137, 4/9, d1=0.567, d2=0.825 g=0.796\n",
      ">137, 5/9, d1=0.623, d2=0.791 g=0.819\n",
      ">137, 6/9, d1=0.655, d2=0.691 g=0.961\n",
      ">137, 7/9, d1=0.564, d2=0.578 g=1.165\n",
      ">137, 8/9, d1=0.569, d2=0.491 g=1.427\n",
      ">137, 9/9, d1=0.635, d2=0.379 g=1.649\n",
      ">138, 1/9, d1=0.559, d2=0.317 g=1.743\n",
      ">138, 2/9, d1=0.649, d2=0.328 g=1.731\n",
      ">138, 3/9, d1=0.656, d2=0.362 g=1.490\n",
      ">138, 4/9, d1=0.636, d2=0.496 g=1.273\n",
      ">138, 5/9, d1=0.583, d2=0.590 g=1.106\n",
      ">138, 6/9, d1=0.600, d2=0.659 g=0.958\n",
      ">138, 7/9, d1=0.477, d2=0.692 g=0.849\n",
      ">138, 8/9, d1=0.534, d2=0.862 g=0.765\n",
      ">138, 9/9, d1=0.585, d2=0.862 g=0.741\n",
      ">139, 1/9, d1=0.622, d2=0.835 g=0.758\n",
      ">139, 2/9, d1=0.607, d2=0.744 g=0.784\n",
      ">139, 3/9, d1=0.698, d2=0.803 g=0.795\n",
      ">139, 4/9, d1=0.743, d2=0.768 g=0.740\n",
      ">139, 5/9, d1=0.662, d2=0.825 g=0.810\n",
      ">139, 6/9, d1=0.727, d2=0.765 g=0.912\n",
      ">139, 7/9, d1=0.751, d2=0.681 g=0.969\n",
      ">139, 8/9, d1=0.707, d2=0.578 g=1.045\n",
      ">139, 9/9, d1=0.715, d2=0.531 g=1.105\n",
      ">140, 1/9, d1=0.715, d2=0.545 g=1.127\n",
      ">140, 2/9, d1=0.761, d2=0.493 g=1.163\n",
      ">140, 3/9, d1=0.752, d2=0.403 g=1.210\n",
      ">140, 4/9, d1=0.695, d2=0.491 g=1.222\n",
      ">140, 5/9, d1=0.708, d2=0.576 g=1.172\n",
      ">140, 6/9, d1=0.691, d2=0.567 g=1.091\n",
      ">140, 7/9, d1=0.663, d2=0.643 g=1.160\n",
      ">140, 8/9, d1=0.696, d2=0.610 g=1.112\n",
      ">140, 9/9, d1=0.607, d2=0.488 g=1.048\n",
      ">Accuracy real: 57%, fake: 78%\n",
      ">141, 1/9, d1=0.614, d2=0.596 g=0.992\n",
      ">141, 2/9, d1=0.660, d2=0.617 g=0.951\n",
      ">141, 3/9, d1=0.647, d2=0.589 g=0.952\n",
      ">141, 4/9, d1=0.672, d2=0.639 g=0.899\n",
      ">141, 5/9, d1=0.563, d2=0.629 g=0.946\n",
      ">141, 6/9, d1=0.662, d2=0.584 g=0.943\n",
      ">141, 7/9, d1=0.608, d2=0.549 g=0.953\n",
      ">141, 8/9, d1=0.556, d2=0.575 g=0.921\n",
      ">141, 9/9, d1=0.609, d2=0.574 g=0.912\n",
      ">142, 1/9, d1=0.691, d2=0.547 g=0.935\n",
      ">142, 2/9, d1=0.621, d2=0.601 g=0.908\n",
      ">142, 3/9, d1=0.503, d2=0.549 g=0.929\n",
      ">142, 4/9, d1=0.587, d2=0.606 g=0.889\n",
      ">142, 5/9, d1=0.606, d2=0.607 g=0.928\n",
      ">142, 6/9, d1=0.540, d2=0.610 g=0.904\n",
      ">142, 7/9, d1=0.565, d2=0.589 g=0.898\n",
      ">142, 8/9, d1=0.622, d2=0.630 g=0.895\n",
      ">142, 9/9, d1=0.545, d2=0.622 g=0.896\n",
      ">143, 1/9, d1=0.601, d2=0.648 g=0.834\n",
      ">143, 2/9, d1=0.569, d2=0.671 g=0.852\n",
      ">143, 3/9, d1=0.550, d2=0.647 g=0.882\n",
      ">143, 4/9, d1=0.598, d2=0.697 g=0.873\n",
      ">143, 5/9, d1=0.590, d2=0.676 g=0.889\n",
      ">143, 6/9, d1=0.619, d2=0.679 g=0.913\n",
      ">143, 7/9, d1=0.667, d2=0.633 g=0.914\n",
      ">143, 8/9, d1=0.660, d2=0.654 g=0.879\n",
      ">143, 9/9, d1=0.642, d2=0.667 g=0.882\n",
      ">144, 1/9, d1=0.650, d2=0.690 g=0.867\n",
      ">144, 2/9, d1=0.731, d2=0.696 g=0.904\n",
      ">144, 3/9, d1=0.691, d2=0.655 g=0.981\n",
      ">144, 4/9, d1=0.733, d2=0.647 g=0.984\n",
      ">144, 5/9, d1=0.703, d2=0.594 g=0.996\n",
      ">144, 6/9, d1=0.745, d2=0.604 g=1.010\n",
      ">144, 7/9, d1=0.682, d2=0.587 g=1.092\n",
      ">144, 8/9, d1=0.711, d2=0.547 g=1.160\n",
      ">144, 9/9, d1=0.699, d2=0.468 g=1.255\n",
      ">145, 1/9, d1=0.628, d2=0.401 g=1.376\n",
      ">145, 2/9, d1=0.609, d2=0.346 g=1.485\n",
      ">145, 3/9, d1=0.581, d2=0.311 g=1.626\n",
      ">145, 4/9, d1=0.583, d2=0.276 g=1.598\n",
      ">145, 5/9, d1=0.547, d2=0.298 g=1.530\n",
      ">145, 6/9, d1=0.575, d2=0.357 g=1.445\n",
      ">145, 7/9, d1=0.531, d2=0.396 g=1.236\n",
      ">145, 8/9, d1=0.480, d2=0.473 g=1.086\n",
      ">145, 9/9, d1=0.496, d2=0.614 g=0.925\n",
      ">146, 1/9, d1=0.422, d2=0.725 g=0.857\n",
      ">146, 2/9, d1=0.464, d2=0.687 g=0.879\n",
      ">146, 3/9, d1=0.470, d2=0.742 g=0.944\n",
      ">146, 4/9, d1=0.496, d2=0.617 g=1.062\n",
      ">146, 5/9, d1=0.514, d2=0.608 g=1.154\n",
      ">146, 6/9, d1=0.545, d2=0.789 g=1.163\n",
      ">146, 7/9, d1=0.595, d2=0.869 g=1.260\n",
      ">146, 8/9, d1=0.622, d2=0.626 g=1.247\n",
      ">146, 9/9, d1=0.823, d2=0.711 g=1.200\n",
      ">147, 1/9, d1=0.897, d2=0.641 g=0.965\n",
      ">147, 2/9, d1=0.894, d2=0.605 g=1.008\n",
      ">147, 3/9, d1=0.802, d2=0.573 g=1.015\n",
      ">147, 4/9, d1=0.724, d2=0.480 g=1.136\n",
      ">147, 5/9, d1=0.780, d2=0.437 g=1.166\n",
      ">147, 6/9, d1=0.679, d2=0.457 g=1.171\n",
      ">147, 7/9, d1=0.580, d2=0.435 g=1.231\n",
      ">147, 8/9, d1=0.578, d2=0.415 g=1.183\n",
      ">147, 9/9, d1=0.553, d2=0.458 g=1.149\n",
      ">148, 1/9, d1=0.513, d2=0.471 g=1.120\n",
      ">148, 2/9, d1=0.532, d2=0.479 g=1.137\n",
      ">148, 3/9, d1=0.544, d2=0.535 g=1.070\n",
      ">148, 4/9, d1=0.488, d2=0.543 g=0.963\n",
      ">148, 5/9, d1=0.478, d2=0.596 g=0.939\n",
      ">148, 6/9, d1=0.494, d2=0.684 g=0.930\n",
      ">148, 7/9, d1=0.500, d2=0.672 g=0.893\n",
      ">148, 8/9, d1=0.477, d2=0.707 g=0.973\n",
      ">148, 9/9, d1=0.553, d2=0.598 g=1.026\n",
      ">149, 1/9, d1=0.621, d2=0.484 g=1.109\n",
      ">149, 2/9, d1=0.584, d2=0.434 g=1.254\n",
      ">149, 3/9, d1=0.661, d2=0.433 g=1.405\n",
      ">149, 4/9, d1=0.581, d2=0.498 g=1.279\n",
      ">149, 5/9, d1=0.502, d2=0.478 g=1.273\n",
      ">149, 6/9, d1=0.615, d2=0.381 g=1.239\n",
      ">149, 7/9, d1=0.574, d2=0.468 g=1.162\n",
      ">149, 8/9, d1=0.614, d2=0.503 g=1.131\n",
      ">149, 9/9, d1=0.563, d2=0.560 g=1.066\n",
      ">150, 1/9, d1=0.554, d2=0.578 g=0.902\n",
      ">150, 2/9, d1=0.629, d2=0.698 g=0.813\n",
      ">150, 3/9, d1=0.652, d2=0.788 g=0.801\n",
      ">150, 4/9, d1=0.567, d2=0.687 g=0.888\n",
      ">150, 5/9, d1=0.535, d2=0.591 g=1.044\n",
      ">150, 6/9, d1=0.656, d2=0.543 g=1.033\n",
      ">150, 7/9, d1=0.681, d2=0.536 g=1.015\n",
      ">150, 8/9, d1=0.654, d2=0.605 g=0.984\n",
      ">150, 9/9, d1=0.534, d2=0.590 g=0.948\n",
      ">Accuracy real: 70%, fake: 59%\n",
      ">151, 1/9, d1=0.620, d2=0.707 g=0.892\n",
      ">151, 2/9, d1=0.500, d2=0.621 g=0.924\n",
      ">151, 3/9, d1=0.664, d2=0.612 g=0.924\n",
      ">151, 4/9, d1=0.597, d2=0.654 g=0.985\n",
      ">151, 5/9, d1=0.569, d2=0.522 g=1.021\n",
      ">151, 6/9, d1=0.576, d2=0.531 g=1.102\n",
      ">151, 7/9, d1=0.654, d2=0.519 g=1.169\n",
      ">151, 8/9, d1=0.615, d2=0.477 g=1.258\n",
      ">151, 9/9, d1=0.767, d2=0.455 g=1.273\n",
      ">152, 1/9, d1=0.556, d2=0.406 g=1.326\n",
      ">152, 2/9, d1=0.649, d2=0.454 g=1.260\n",
      ">152, 3/9, d1=0.638, d2=0.461 g=1.283\n",
      ">152, 4/9, d1=0.628, d2=0.445 g=1.287\n",
      ">152, 5/9, d1=0.536, d2=0.460 g=1.275\n",
      ">152, 6/9, d1=0.560, d2=0.485 g=1.282\n",
      ">152, 7/9, d1=0.568, d2=0.451 g=1.311\n",
      ">152, 8/9, d1=0.576, d2=0.483 g=1.200\n",
      ">152, 9/9, d1=0.604, d2=0.543 g=1.129\n",
      ">153, 1/9, d1=0.543, d2=0.613 g=1.100\n",
      ">153, 2/9, d1=0.606, d2=0.693 g=1.107\n",
      ">153, 3/9, d1=0.534, d2=0.568 g=1.123\n",
      ">153, 4/9, d1=0.655, d2=0.562 g=0.978\n",
      ">153, 5/9, d1=0.564, d2=0.852 g=0.918\n",
      ">153, 6/9, d1=0.551, d2=0.737 g=1.010\n",
      ">153, 7/9, d1=0.612, d2=0.788 g=0.958\n",
      ">153, 8/9, d1=0.547, d2=0.676 g=0.983\n",
      ">153, 9/9, d1=0.676, d2=0.646 g=1.006\n",
      ">154, 1/9, d1=0.743, d2=0.766 g=0.988\n",
      ">154, 2/9, d1=0.653, d2=0.670 g=0.961\n",
      ">154, 3/9, d1=0.600, d2=0.701 g=1.067\n",
      ">154, 4/9, d1=0.740, d2=0.592 g=0.999\n",
      ">154, 5/9, d1=0.778, d2=0.609 g=1.043\n",
      ">154, 6/9, d1=0.754, d2=0.625 g=1.076\n",
      ">154, 7/9, d1=0.840, d2=0.743 g=0.971\n",
      ">154, 8/9, d1=0.730, d2=0.646 g=0.953\n",
      ">154, 9/9, d1=0.730, d2=0.776 g=0.991\n",
      ">155, 1/9, d1=0.780, d2=0.696 g=0.952\n",
      ">155, 2/9, d1=0.840, d2=0.702 g=0.996\n",
      ">155, 3/9, d1=0.735, d2=0.633 g=1.086\n",
      ">155, 4/9, d1=0.819, d2=0.559 g=1.174\n",
      ">155, 5/9, d1=0.756, d2=0.539 g=1.183\n",
      ">155, 6/9, d1=0.877, d2=0.479 g=1.303\n",
      ">155, 7/9, d1=0.788, d2=0.445 g=1.205\n",
      ">155, 8/9, d1=0.640, d2=0.407 g=1.376\n",
      ">155, 9/9, d1=0.645, d2=0.373 g=1.611\n",
      ">156, 1/9, d1=0.665, d2=0.340 g=1.788\n",
      ">156, 2/9, d1=0.715, d2=0.354 g=1.676\n",
      ">156, 3/9, d1=0.687, d2=0.455 g=1.465\n",
      ">156, 4/9, d1=0.708, d2=0.595 g=1.278\n",
      ">156, 5/9, d1=0.655, d2=0.836 g=1.080\n",
      ">156, 6/9, d1=0.688, d2=0.639 g=0.972\n",
      ">156, 7/9, d1=0.731, d2=0.657 g=1.015\n",
      ">156, 8/9, d1=0.682, d2=0.662 g=0.978\n",
      ">156, 9/9, d1=0.716, d2=0.642 g=0.963\n",
      ">157, 1/9, d1=0.741, d2=0.609 g=0.982\n",
      ">157, 2/9, d1=0.742, d2=0.556 g=0.980\n",
      ">157, 3/9, d1=0.716, d2=0.567 g=0.901\n",
      ">157, 4/9, d1=0.644, d2=0.573 g=0.908\n",
      ">157, 5/9, d1=0.695, d2=0.689 g=0.801\n",
      ">157, 6/9, d1=0.689, d2=0.709 g=0.789\n",
      ">157, 7/9, d1=0.622, d2=0.777 g=0.765\n",
      ">157, 8/9, d1=0.713, d2=0.790 g=0.733\n",
      ">157, 9/9, d1=0.644, d2=0.790 g=0.731\n",
      ">158, 1/9, d1=0.694, d2=0.829 g=0.704\n",
      ">158, 2/9, d1=0.651, d2=0.857 g=0.734\n",
      ">158, 3/9, d1=0.725, d2=0.714 g=0.816\n",
      ">158, 4/9, d1=0.717, d2=0.731 g=0.890\n",
      ">158, 5/9, d1=0.715, d2=0.684 g=0.986\n",
      ">158, 6/9, d1=0.718, d2=0.606 g=1.059\n",
      ">158, 7/9, d1=0.767, d2=0.663 g=1.118\n",
      ">158, 8/9, d1=0.758, d2=0.551 g=1.176\n",
      ">158, 9/9, d1=0.792, d2=0.553 g=1.041\n",
      ">159, 1/9, d1=0.771, d2=0.640 g=1.077\n",
      ">159, 2/9, d1=0.840, d2=0.620 g=0.989\n",
      ">159, 3/9, d1=0.778, d2=0.661 g=0.972\n",
      ">159, 4/9, d1=0.819, d2=0.663 g=0.898\n",
      ">159, 5/9, d1=0.720, d2=0.722 g=0.907\n",
      ">159, 6/9, d1=0.913, d2=0.642 g=0.898\n",
      ">159, 7/9, d1=0.834, d2=0.648 g=0.912\n",
      ">159, 8/9, d1=0.841, d2=0.617 g=0.949\n",
      ">159, 9/9, d1=0.834, d2=0.574 g=1.067\n",
      ">160, 1/9, d1=0.777, d2=0.514 g=1.074\n",
      ">160, 2/9, d1=0.759, d2=0.497 g=1.130\n",
      ">160, 3/9, d1=0.761, d2=0.489 g=1.109\n",
      ">160, 4/9, d1=0.708, d2=0.461 g=1.177\n",
      ">160, 5/9, d1=0.757, d2=0.487 g=1.251\n",
      ">160, 6/9, d1=0.707, d2=0.509 g=1.184\n",
      ">160, 7/9, d1=0.673, d2=0.468 g=1.146\n",
      ">160, 8/9, d1=0.768, d2=0.460 g=1.172\n",
      ">160, 9/9, d1=0.723, d2=0.465 g=1.065\n",
      ">Accuracy real: 63%, fake: 87%\n",
      ">161, 1/9, d1=0.517, d2=0.523 g=1.063\n",
      ">161, 2/9, d1=0.675, d2=0.513 g=1.055\n",
      ">161, 3/9, d1=0.651, d2=0.536 g=0.973\n",
      ">161, 4/9, d1=0.635, d2=0.556 g=0.937\n",
      ">161, 5/9, d1=0.693, d2=0.621 g=0.893\n",
      ">161, 6/9, d1=0.628, d2=0.662 g=0.821\n",
      ">161, 7/9, d1=0.731, d2=0.670 g=0.764\n",
      ">161, 8/9, d1=0.634, d2=0.741 g=0.736\n",
      ">161, 9/9, d1=0.611, d2=0.778 g=0.737\n",
      ">162, 1/9, d1=0.655, d2=0.734 g=0.707\n",
      ">162, 2/9, d1=0.633, d2=0.738 g=0.745\n",
      ">162, 3/9, d1=0.640, d2=0.701 g=0.766\n",
      ">162, 4/9, d1=0.625, d2=0.717 g=0.801\n",
      ">162, 5/9, d1=0.686, d2=0.661 g=0.850\n",
      ">162, 6/9, d1=0.642, d2=0.631 g=0.888\n",
      ">162, 7/9, d1=0.661, d2=0.558 g=0.918\n",
      ">162, 8/9, d1=0.635, d2=0.578 g=0.911\n",
      ">162, 9/9, d1=0.635, d2=0.609 g=0.920\n",
      ">163, 1/9, d1=0.647, d2=0.609 g=0.918\n",
      ">163, 2/9, d1=0.624, d2=0.609 g=0.878\n",
      ">163, 3/9, d1=0.634, d2=0.620 g=0.892\n",
      ">163, 4/9, d1=0.614, d2=0.612 g=0.887\n",
      ">163, 5/9, d1=0.635, d2=0.666 g=0.873\n",
      ">163, 6/9, d1=0.581, d2=0.633 g=0.898\n",
      ">163, 7/9, d1=0.597, d2=0.626 g=0.903\n",
      ">163, 8/9, d1=0.657, d2=0.616 g=0.903\n",
      ">163, 9/9, d1=0.590, d2=0.629 g=0.931\n",
      ">164, 1/9, d1=0.662, d2=0.613 g=0.923\n",
      ">164, 2/9, d1=0.641, d2=0.647 g=0.942\n",
      ">164, 3/9, d1=0.626, d2=0.664 g=0.926\n",
      ">164, 4/9, d1=0.600, d2=0.619 g=0.971\n",
      ">164, 5/9, d1=0.642, d2=0.563 g=0.968\n",
      ">164, 6/9, d1=0.661, d2=0.575 g=0.984\n",
      ">164, 7/9, d1=0.617, d2=0.558 g=0.963\n",
      ">164, 8/9, d1=0.621, d2=0.526 g=0.989\n",
      ">164, 9/9, d1=0.640, d2=0.564 g=1.005\n",
      ">165, 1/9, d1=0.698, d2=0.561 g=0.979\n",
      ">165, 2/9, d1=0.671, d2=0.600 g=0.976\n",
      ">165, 3/9, d1=0.645, d2=0.643 g=0.930\n",
      ">165, 4/9, d1=0.627, d2=0.626 g=0.995\n",
      ">165, 5/9, d1=0.647, d2=0.609 g=0.953\n",
      ">165, 6/9, d1=0.672, d2=0.638 g=0.921\n",
      ">165, 7/9, d1=0.675, d2=0.616 g=0.973\n",
      ">165, 8/9, d1=0.715, d2=0.579 g=1.012\n",
      ">165, 9/9, d1=0.737, d2=0.620 g=0.986\n",
      ">166, 1/9, d1=0.625, d2=0.647 g=1.050\n",
      ">166, 2/9, d1=0.713, d2=0.524 g=1.095\n",
      ">166, 3/9, d1=0.709, d2=0.537 g=1.134\n",
      ">166, 4/9, d1=0.786, d2=0.503 g=1.104\n",
      ">166, 5/9, d1=0.703, d2=0.539 g=1.174\n",
      ">166, 6/9, d1=0.658, d2=0.594 g=1.031\n",
      ">166, 7/9, d1=0.727, d2=0.593 g=1.055\n",
      ">166, 8/9, d1=0.744, d2=0.547 g=1.014\n",
      ">166, 9/9, d1=0.805, d2=0.621 g=0.972\n",
      ">167, 1/9, d1=0.720, d2=0.578 g=0.960\n",
      ">167, 2/9, d1=0.685, d2=0.593 g=0.920\n",
      ">167, 3/9, d1=0.675, d2=0.620 g=0.885\n",
      ">167, 4/9, d1=0.737, d2=0.646 g=0.855\n",
      ">167, 5/9, d1=0.682, d2=0.692 g=0.838\n",
      ">167, 6/9, d1=0.670, d2=0.634 g=0.831\n",
      ">167, 7/9, d1=0.666, d2=0.654 g=0.816\n",
      ">167, 8/9, d1=0.606, d2=0.667 g=0.854\n",
      ">167, 9/9, d1=0.639, d2=0.626 g=0.842\n",
      ">168, 1/9, d1=0.576, d2=0.625 g=0.836\n",
      ">168, 2/9, d1=0.562, d2=0.648 g=0.821\n",
      ">168, 3/9, d1=0.578, d2=0.629 g=0.859\n",
      ">168, 4/9, d1=0.530, d2=0.610 g=0.855\n",
      ">168, 5/9, d1=0.536, d2=0.644 g=0.869\n",
      ">168, 6/9, d1=0.505, d2=0.626 g=0.829\n",
      ">168, 7/9, d1=0.512, d2=0.610 g=0.830\n",
      ">168, 8/9, d1=0.469, d2=0.603 g=0.895\n",
      ">168, 9/9, d1=0.476, d2=0.586 g=0.891\n",
      ">169, 1/9, d1=0.482, d2=0.592 g=0.859\n",
      ">169, 2/9, d1=0.467, d2=0.618 g=0.879\n",
      ">169, 3/9, d1=0.394, d2=0.643 g=0.866\n",
      ">169, 4/9, d1=0.450, d2=0.651 g=0.864\n",
      ">169, 5/9, d1=0.479, d2=0.676 g=0.855\n",
      ">169, 6/9, d1=0.469, d2=0.674 g=0.864\n",
      ">169, 7/9, d1=0.499, d2=0.648 g=0.929\n",
      ">169, 8/9, d1=0.522, d2=0.591 g=0.872\n",
      ">169, 9/9, d1=0.528, d2=0.602 g=0.950\n",
      ">170, 1/9, d1=0.610, d2=0.568 g=0.903\n",
      ">170, 2/9, d1=0.540, d2=0.625 g=0.928\n",
      ">170, 3/9, d1=0.552, d2=0.614 g=0.865\n",
      ">170, 4/9, d1=0.631, d2=0.618 g=0.932\n",
      ">170, 5/9, d1=0.657, d2=0.599 g=0.916\n",
      ">170, 6/9, d1=0.620, d2=0.707 g=0.827\n",
      ">170, 7/9, d1=0.598, d2=0.753 g=0.764\n",
      ">170, 8/9, d1=0.639, d2=0.871 g=0.719\n",
      ">170, 9/9, d1=0.680, d2=0.828 g=0.704\n",
      ">Accuracy real: 61%, fake: 21%\n",
      ">171, 1/9, d1=0.820, d2=0.923 g=0.713\n",
      ">171, 2/9, d1=0.684, d2=0.872 g=0.714\n",
      ">171, 3/9, d1=0.719, d2=0.825 g=0.771\n",
      ">171, 4/9, d1=0.700, d2=0.691 g=0.827\n",
      ">171, 5/9, d1=0.768, d2=0.676 g=0.884\n",
      ">171, 6/9, d1=0.717, d2=0.616 g=0.931\n",
      ">171, 7/9, d1=0.676, d2=0.576 g=0.975\n",
      ">171, 8/9, d1=0.608, d2=0.578 g=0.944\n",
      ">171, 9/9, d1=0.579, d2=0.566 g=1.011\n",
      ">172, 1/9, d1=0.581, d2=0.651 g=0.984\n",
      ">172, 2/9, d1=0.668, d2=0.689 g=0.967\n",
      ">172, 3/9, d1=0.725, d2=0.645 g=0.984\n",
      ">172, 4/9, d1=0.686, d2=0.598 g=1.000\n",
      ">172, 5/9, d1=0.641, d2=0.560 g=0.979\n",
      ">172, 6/9, d1=0.622, d2=0.558 g=1.001\n",
      ">172, 7/9, d1=0.676, d2=0.574 g=0.989\n",
      ">172, 8/9, d1=0.688, d2=0.575 g=1.024\n",
      ">172, 9/9, d1=0.730, d2=0.599 g=0.984\n",
      ">173, 1/9, d1=0.670, d2=0.627 g=0.897\n",
      ">173, 2/9, d1=0.657, d2=0.646 g=0.837\n",
      ">173, 3/9, d1=0.711, d2=0.691 g=0.861\n",
      ">173, 4/9, d1=0.774, d2=0.699 g=0.801\n",
      ">173, 5/9, d1=0.703, d2=0.725 g=0.809\n",
      ">173, 6/9, d1=0.711, d2=0.704 g=0.841\n",
      ">173, 7/9, d1=0.762, d2=0.683 g=0.855\n",
      ">173, 8/9, d1=0.798, d2=0.695 g=0.943\n",
      ">173, 9/9, d1=0.696, d2=0.624 g=0.930\n",
      ">174, 1/9, d1=0.751, d2=0.667 g=0.942\n",
      ">174, 2/9, d1=0.735, d2=0.605 g=0.923\n",
      ">174, 3/9, d1=0.730, d2=0.598 g=0.926\n",
      ">174, 4/9, d1=0.733, d2=0.592 g=0.891\n",
      ">174, 5/9, d1=0.704, d2=0.678 g=0.851\n",
      ">174, 6/9, d1=0.694, d2=0.725 g=0.890\n",
      ">174, 7/9, d1=0.702, d2=0.719 g=0.859\n",
      ">174, 8/9, d1=0.700, d2=0.652 g=0.935\n",
      ">174, 9/9, d1=0.661, d2=0.598 g=0.938\n",
      ">175, 1/9, d1=0.762, d2=0.576 g=0.999\n",
      ">175, 2/9, d1=0.744, d2=0.538 g=1.005\n",
      ">175, 3/9, d1=0.733, d2=0.566 g=0.977\n",
      ">175, 4/9, d1=0.626, d2=0.575 g=1.027\n",
      ">175, 5/9, d1=0.595, d2=0.538 g=1.028\n",
      ">175, 6/9, d1=0.649, d2=0.535 g=0.991\n",
      ">175, 7/9, d1=0.648, d2=0.579 g=0.974\n",
      ">175, 8/9, d1=0.581, d2=0.549 g=0.952\n",
      ">175, 9/9, d1=0.616, d2=0.673 g=0.917\n",
      ">176, 1/9, d1=0.612, d2=0.691 g=0.862\n",
      ">176, 2/9, d1=0.639, d2=0.666 g=0.896\n",
      ">176, 3/9, d1=0.590, d2=0.697 g=0.932\n",
      ">176, 4/9, d1=0.628, d2=0.577 g=0.980\n",
      ">176, 5/9, d1=0.665, d2=0.576 g=1.034\n",
      ">176, 6/9, d1=0.708, d2=0.530 g=1.037\n",
      ">176, 7/9, d1=0.697, d2=0.528 g=1.054\n",
      ">176, 8/9, d1=0.673, d2=0.505 g=1.073\n",
      ">176, 9/9, d1=0.639, d2=0.543 g=1.057\n",
      ">177, 1/9, d1=0.674, d2=0.565 g=1.022\n",
      ">177, 2/9, d1=0.568, d2=0.586 g=0.960\n",
      ">177, 3/9, d1=0.633, d2=0.633 g=0.882\n",
      ">177, 4/9, d1=0.593, d2=0.737 g=0.813\n",
      ">177, 5/9, d1=0.641, d2=0.766 g=0.771\n",
      ">177, 6/9, d1=0.655, d2=0.815 g=0.723\n",
      ">177, 7/9, d1=0.771, d2=0.874 g=0.750\n",
      ">177, 8/9, d1=0.790, d2=0.828 g=0.802\n",
      ">177, 9/9, d1=0.705, d2=0.737 g=0.886\n",
      ">178, 1/9, d1=0.824, d2=0.598 g=1.095\n",
      ">178, 2/9, d1=0.760, d2=0.423 g=1.284\n",
      ">178, 3/9, d1=0.770, d2=0.411 g=1.319\n",
      ">178, 4/9, d1=0.726, d2=0.422 g=1.250\n",
      ">178, 5/9, d1=0.740, d2=0.436 g=1.199\n",
      ">178, 6/9, d1=0.648, d2=0.504 g=1.110\n",
      ">178, 7/9, d1=0.667, d2=0.513 g=1.066\n",
      ">178, 8/9, d1=0.601, d2=0.565 g=1.019\n",
      ">178, 9/9, d1=0.567, d2=0.577 g=0.951\n",
      ">179, 1/9, d1=0.602, d2=0.605 g=0.904\n",
      ">179, 2/9, d1=0.578, d2=0.613 g=0.871\n",
      ">179, 3/9, d1=0.611, d2=0.799 g=0.803\n",
      ">179, 4/9, d1=0.645, d2=0.734 g=0.725\n",
      ">179, 5/9, d1=0.620, d2=0.798 g=0.727\n",
      ">179, 6/9, d1=0.592, d2=0.709 g=0.796\n",
      ">179, 7/9, d1=0.579, d2=0.700 g=0.825\n",
      ">179, 8/9, d1=0.678, d2=0.670 g=0.909\n",
      ">179, 9/9, d1=0.676, d2=0.648 g=0.931\n",
      ">180, 1/9, d1=0.695, d2=0.644 g=1.040\n",
      ">180, 2/9, d1=0.727, d2=0.589 g=1.049\n",
      ">180, 3/9, d1=0.701, d2=0.654 g=1.029\n",
      ">180, 4/9, d1=0.806, d2=0.758 g=0.983\n",
      ">180, 5/9, d1=0.732, d2=0.775 g=1.123\n",
      ">180, 6/9, d1=0.794, d2=0.588 g=1.380\n",
      ">180, 7/9, d1=0.745, d2=0.393 g=1.722\n",
      ">180, 8/9, d1=0.803, d2=0.347 g=1.926\n",
      ">180, 9/9, d1=0.749, d2=0.332 g=1.742\n",
      ">Accuracy real: 57%, fake: 88%\n",
      ">181, 1/9, d1=0.477, d2=0.324 g=1.866\n",
      ">181, 2/9, d1=0.681, d2=0.317 g=1.674\n",
      ">181, 3/9, d1=0.573, d2=0.587 g=1.209\n",
      ">181, 4/9, d1=0.695, d2=0.847 g=0.694\n",
      ">181, 5/9, d1=0.643, d2=0.944 g=0.622\n",
      ">181, 6/9, d1=0.639, d2=0.922 g=0.700\n",
      ">181, 7/9, d1=0.659, d2=0.779 g=0.759\n",
      ">181, 8/9, d1=0.701, d2=0.661 g=0.855\n",
      ">181, 9/9, d1=0.687, d2=0.645 g=0.926\n",
      ">182, 1/9, d1=0.617, d2=0.585 g=0.952\n",
      ">182, 2/9, d1=0.644, d2=0.569 g=0.974\n",
      ">182, 3/9, d1=0.673, d2=0.621 g=0.954\n",
      ">182, 4/9, d1=0.629, d2=0.583 g=0.948\n",
      ">182, 5/9, d1=0.596, d2=0.622 g=0.870\n",
      ">182, 6/9, d1=0.567, d2=0.613 g=0.896\n",
      ">182, 7/9, d1=0.710, d2=0.621 g=0.876\n",
      ">182, 8/9, d1=0.625, d2=0.773 g=0.867\n",
      ">182, 9/9, d1=0.580, d2=0.676 g=0.879\n",
      ">183, 1/9, d1=0.602, d2=0.563 g=0.971\n",
      ">183, 2/9, d1=0.666, d2=0.528 g=0.903\n",
      ">183, 3/9, d1=0.650, d2=0.622 g=0.938\n",
      ">183, 4/9, d1=0.624, d2=0.580 g=0.992\n",
      ">183, 5/9, d1=0.667, d2=0.573 g=1.021\n",
      ">183, 6/9, d1=0.725, d2=0.559 g=0.958\n",
      ">183, 7/9, d1=0.675, d2=0.545 g=0.953\n",
      ">183, 8/9, d1=0.731, d2=0.553 g=0.978\n",
      ">183, 9/9, d1=0.728, d2=0.648 g=0.933\n",
      ">184, 1/9, d1=0.754, d2=0.668 g=0.842\n",
      ">184, 2/9, d1=0.656, d2=0.673 g=0.814\n",
      ">184, 3/9, d1=0.668, d2=0.686 g=0.800\n",
      ">184, 4/9, d1=0.669, d2=0.679 g=0.799\n",
      ">184, 5/9, d1=0.713, d2=0.703 g=0.743\n",
      ">184, 6/9, d1=0.650, d2=0.738 g=0.722\n",
      ">184, 7/9, d1=0.648, d2=0.748 g=0.737\n",
      ">184, 8/9, d1=0.624, d2=0.683 g=0.773\n",
      ">184, 9/9, d1=0.672, d2=0.719 g=0.767\n",
      ">185, 1/9, d1=0.655, d2=0.672 g=0.768\n",
      ">185, 2/9, d1=0.660, d2=0.692 g=0.783\n",
      ">185, 3/9, d1=0.708, d2=0.703 g=0.787\n",
      ">185, 4/9, d1=0.640, d2=0.661 g=0.810\n",
      ">185, 5/9, d1=0.594, d2=0.655 g=0.796\n",
      ">185, 6/9, d1=0.586, d2=0.620 g=0.849\n",
      ">185, 7/9, d1=0.658, d2=0.651 g=0.860\n",
      ">185, 8/9, d1=0.670, d2=0.619 g=0.863\n",
      ">185, 9/9, d1=0.653, d2=0.658 g=0.866\n",
      ">186, 1/9, d1=0.625, d2=0.633 g=0.866\n",
      ">186, 2/9, d1=0.673, d2=0.588 g=0.856\n",
      ">186, 3/9, d1=0.628, d2=0.651 g=0.882\n",
      ">186, 4/9, d1=0.650, d2=0.631 g=0.874\n",
      ">186, 5/9, d1=0.657, d2=0.657 g=0.866\n",
      ">186, 6/9, d1=0.658, d2=0.641 g=0.854\n",
      ">186, 7/9, d1=0.662, d2=0.703 g=0.822\n",
      ">186, 8/9, d1=0.648, d2=0.676 g=0.871\n",
      ">186, 9/9, d1=0.670, d2=0.621 g=0.911\n",
      ">187, 1/9, d1=0.674, d2=0.595 g=0.956\n",
      ">187, 2/9, d1=0.634, d2=0.602 g=1.012\n",
      ">187, 3/9, d1=0.628, d2=0.477 g=1.154\n",
      ">187, 4/9, d1=0.688, d2=0.504 g=1.182\n",
      ">187, 5/9, d1=0.673, d2=0.512 g=1.018\n",
      ">187, 6/9, d1=0.640, d2=0.509 g=1.099\n",
      ">187, 7/9, d1=0.672, d2=0.561 g=1.076\n",
      ">187, 8/9, d1=0.634, d2=0.557 g=1.054\n",
      ">187, 9/9, d1=0.598, d2=0.527 g=1.044\n",
      ">188, 1/9, d1=0.517, d2=0.519 g=1.105\n",
      ">188, 2/9, d1=0.635, d2=0.531 g=1.074\n",
      ">188, 3/9, d1=0.636, d2=0.554 g=1.017\n",
      ">188, 4/9, d1=0.633, d2=0.612 g=0.984\n",
      ">188, 5/9, d1=0.609, d2=0.630 g=0.966\n",
      ">188, 6/9, d1=0.576, d2=0.674 g=0.885\n",
      ">188, 7/9, d1=0.632, d2=0.753 g=0.857\n",
      ">188, 8/9, d1=0.617, d2=0.677 g=0.811\n",
      ">188, 9/9, d1=0.613, d2=0.662 g=0.797\n",
      ">189, 1/9, d1=0.638, d2=0.712 g=0.806\n",
      ">189, 2/9, d1=0.675, d2=0.749 g=0.774\n",
      ">189, 3/9, d1=0.653, d2=0.703 g=0.758\n",
      ">189, 4/9, d1=0.631, d2=0.735 g=0.764\n",
      ">189, 5/9, d1=0.627, d2=0.713 g=0.779\n",
      ">189, 6/9, d1=0.612, d2=0.715 g=0.788\n",
      ">189, 7/9, d1=0.662, d2=0.694 g=0.803\n",
      ">189, 8/9, d1=0.581, d2=0.688 g=0.831\n",
      ">189, 9/9, d1=0.641, d2=0.736 g=0.780\n",
      ">190, 1/9, d1=0.618, d2=0.735 g=0.766\n",
      ">190, 2/9, d1=0.642, d2=0.761 g=0.780\n",
      ">190, 3/9, d1=0.665, d2=0.734 g=0.833\n",
      ">190, 4/9, d1=0.579, d2=0.707 g=0.892\n",
      ">190, 5/9, d1=0.592, d2=0.610 g=0.991\n",
      ">190, 6/9, d1=0.621, d2=0.543 g=1.117\n",
      ">190, 7/9, d1=0.578, d2=0.420 g=1.274\n",
      ">190, 8/9, d1=0.558, d2=0.401 g=1.315\n",
      ">190, 9/9, d1=0.605, d2=0.425 g=1.280\n",
      ">Accuracy real: 69%, fake: 98%\n",
      ">191, 1/9, d1=0.477, d2=0.464 g=1.150\n",
      ">191, 2/9, d1=0.616, d2=0.636 g=0.968\n",
      ">191, 3/9, d1=0.518, d2=0.731 g=0.865\n",
      ">191, 4/9, d1=0.524, d2=0.744 g=0.796\n",
      ">191, 5/9, d1=0.627, d2=0.770 g=0.940\n",
      ">191, 6/9, d1=0.636, d2=0.680 g=1.118\n",
      ">191, 7/9, d1=0.664, d2=0.526 g=1.349\n",
      ">191, 8/9, d1=0.764, d2=0.447 g=1.427\n",
      ">191, 9/9, d1=0.823, d2=0.459 g=1.344\n",
      ">192, 1/9, d1=0.807, d2=0.588 g=1.198\n",
      ">192, 2/9, d1=0.964, d2=0.594 g=0.890\n",
      ">192, 3/9, d1=0.781, d2=0.762 g=0.806\n",
      ">192, 4/9, d1=0.731, d2=0.856 g=0.825\n",
      ">192, 5/9, d1=0.783, d2=0.810 g=0.782\n",
      ">192, 6/9, d1=0.786, d2=0.781 g=0.722\n",
      ">192, 7/9, d1=0.690, d2=0.717 g=0.762\n",
      ">192, 8/9, d1=0.786, d2=0.726 g=0.780\n",
      ">192, 9/9, d1=0.811, d2=0.756 g=0.773\n",
      ">193, 1/9, d1=0.726, d2=0.716 g=0.798\n",
      ">193, 2/9, d1=0.728, d2=0.668 g=0.773\n",
      ">193, 3/9, d1=0.675, d2=0.631 g=0.849\n",
      ">193, 4/9, d1=0.638, d2=0.666 g=0.877\n",
      ">193, 5/9, d1=0.681, d2=0.631 g=0.915\n",
      ">193, 6/9, d1=0.673, d2=0.592 g=0.922\n",
      ">193, 7/9, d1=0.617, d2=0.579 g=0.923\n",
      ">193, 8/9, d1=0.610, d2=0.542 g=0.943\n",
      ">193, 9/9, d1=0.627, d2=0.580 g=0.949\n",
      ">194, 1/9, d1=0.568, d2=0.580 g=0.926\n",
      ">194, 2/9, d1=0.586, d2=0.576 g=0.919\n",
      ">194, 3/9, d1=0.566, d2=0.565 g=0.949\n",
      ">194, 4/9, d1=0.631, d2=0.566 g=0.914\n",
      ">194, 5/9, d1=0.632, d2=0.634 g=0.960\n",
      ">194, 6/9, d1=0.599, d2=0.605 g=0.931\n",
      ">194, 7/9, d1=0.617, d2=0.551 g=1.009\n",
      ">194, 8/9, d1=0.628, d2=0.578 g=1.044\n",
      ">194, 9/9, d1=0.671, d2=0.554 g=1.057\n",
      ">195, 1/9, d1=0.675, d2=0.540 g=1.018\n",
      ">195, 2/9, d1=0.693, d2=0.547 g=1.106\n",
      ">195, 3/9, d1=0.676, d2=0.511 g=1.066\n",
      ">195, 4/9, d1=0.704, d2=0.554 g=1.050\n",
      ">195, 5/9, d1=0.648, d2=0.508 g=1.078\n",
      ">195, 6/9, d1=0.709, d2=0.466 g=1.080\n",
      ">195, 7/9, d1=0.718, d2=0.456 g=1.085\n",
      ">195, 8/9, d1=0.707, d2=0.527 g=1.118\n",
      ">195, 9/9, d1=0.765, d2=0.554 g=1.069\n",
      ">196, 1/9, d1=0.707, d2=0.596 g=1.045\n",
      ">196, 2/9, d1=0.719, d2=0.494 g=1.076\n",
      ">196, 3/9, d1=0.683, d2=0.534 g=1.014\n",
      ">196, 4/9, d1=0.754, d2=0.582 g=0.985\n",
      ">196, 5/9, d1=0.691, d2=0.594 g=0.993\n",
      ">196, 6/9, d1=0.719, d2=0.511 g=1.019\n",
      ">196, 7/9, d1=0.742, d2=0.527 g=1.027\n",
      ">196, 8/9, d1=0.775, d2=0.516 g=1.055\n",
      ">196, 9/9, d1=0.712, d2=0.599 g=1.009\n",
      ">197, 1/9, d1=0.723, d2=0.566 g=1.034\n",
      ">197, 2/9, d1=0.708, d2=0.579 g=1.021\n",
      ">197, 3/9, d1=0.661, d2=0.527 g=1.067\n",
      ">197, 4/9, d1=0.737, d2=0.539 g=0.990\n",
      ">197, 5/9, d1=0.711, d2=0.531 g=1.075\n",
      ">197, 6/9, d1=0.697, d2=0.550 g=0.993\n",
      ">197, 7/9, d1=0.706, d2=0.557 g=0.930\n",
      ">197, 8/9, d1=0.671, d2=0.602 g=0.934\n",
      ">197, 9/9, d1=0.708, d2=0.633 g=0.888\n",
      ">198, 1/9, d1=0.731, d2=0.621 g=0.874\n",
      ">198, 2/9, d1=0.703, d2=0.611 g=0.912\n",
      ">198, 3/9, d1=0.649, d2=0.679 g=0.898\n",
      ">198, 4/9, d1=0.721, d2=0.604 g=0.909\n",
      ">198, 5/9, d1=0.605, d2=0.575 g=0.869\n",
      ">198, 6/9, d1=0.640, d2=0.686 g=0.907\n",
      ">198, 7/9, d1=0.713, d2=0.591 g=0.963\n",
      ">198, 8/9, d1=0.691, d2=0.571 g=0.945\n",
      ">198, 9/9, d1=0.671, d2=0.581 g=0.886\n",
      ">199, 1/9, d1=0.622, d2=0.623 g=0.876\n",
      ">199, 2/9, d1=0.550, d2=0.629 g=0.870\n",
      ">199, 3/9, d1=0.608, d2=0.618 g=0.861\n",
      ">199, 4/9, d1=0.551, d2=0.626 g=0.877\n",
      ">199, 5/9, d1=0.659, d2=0.646 g=0.948\n",
      ">199, 6/9, d1=0.570, d2=0.594 g=0.985\n",
      ">199, 7/9, d1=0.606, d2=0.554 g=0.981\n",
      ">199, 8/9, d1=0.588, d2=0.546 g=1.002\n",
      ">199, 9/9, d1=0.575, d2=0.578 g=1.032\n",
      ">200, 1/9, d1=0.561, d2=0.577 g=1.024\n",
      ">200, 2/9, d1=0.546, d2=0.593 g=1.019\n",
      ">200, 3/9, d1=0.581, d2=0.639 g=0.850\n",
      ">200, 4/9, d1=0.594, d2=0.662 g=0.845\n",
      ">200, 5/9, d1=0.606, d2=0.758 g=0.781\n",
      ">200, 6/9, d1=0.623, d2=0.813 g=0.723\n",
      ">200, 7/9, d1=0.622, d2=0.931 g=0.698\n",
      ">200, 8/9, d1=0.501, d2=0.856 g=0.732\n",
      ">200, 9/9, d1=0.572, d2=0.794 g=0.736\n",
      ">Accuracy real: 62%, fake: 39%\n",
      ">201, 1/9, d1=0.744, d2=0.859 g=0.717\n",
      ">201, 2/9, d1=0.608, d2=0.834 g=0.769\n",
      ">201, 3/9, d1=0.690, d2=0.658 g=0.812\n",
      ">201, 4/9, d1=0.604, d2=0.757 g=0.778\n",
      ">201, 5/9, d1=0.653, d2=0.734 g=0.896\n",
      ">201, 6/9, d1=0.695, d2=0.625 g=0.918\n",
      ">201, 7/9, d1=0.753, d2=0.578 g=0.985\n",
      ">201, 8/9, d1=0.636, d2=0.574 g=1.027\n",
      ">201, 9/9, d1=0.655, d2=0.612 g=1.005\n",
      ">202, 1/9, d1=0.658, d2=0.620 g=0.940\n",
      ">202, 2/9, d1=0.698, d2=0.717 g=0.858\n",
      ">202, 3/9, d1=0.688, d2=0.730 g=0.902\n",
      ">202, 4/9, d1=0.710, d2=0.742 g=0.882\n",
      ">202, 5/9, d1=0.710, d2=0.685 g=1.181\n",
      ">202, 6/9, d1=0.663, d2=0.568 g=1.427\n",
      ">202, 7/9, d1=0.710, d2=0.416 g=1.370\n",
      ">202, 8/9, d1=0.759, d2=0.447 g=1.297\n",
      ">202, 9/9, d1=0.714, d2=0.647 g=0.999\n",
      ">203, 1/9, d1=0.698, d2=0.809 g=0.817\n",
      ">203, 2/9, d1=0.702, d2=0.834 g=0.773\n",
      ">203, 3/9, d1=0.670, d2=0.787 g=0.792\n",
      ">203, 4/9, d1=0.700, d2=0.713 g=0.895\n",
      ">203, 5/9, d1=0.735, d2=0.655 g=0.926\n",
      ">203, 6/9, d1=0.783, d2=0.569 g=0.941\n",
      ">203, 7/9, d1=0.712, d2=0.610 g=0.980\n",
      ">203, 8/9, d1=0.668, d2=0.601 g=0.958\n",
      ">203, 9/9, d1=0.676, d2=0.593 g=0.932\n",
      ">204, 1/9, d1=0.655, d2=0.600 g=0.910\n",
      ">204, 2/9, d1=0.651, d2=0.602 g=0.879\n",
      ">204, 3/9, d1=0.667, d2=0.626 g=0.873\n",
      ">204, 4/9, d1=0.644, d2=0.712 g=0.870\n",
      ">204, 5/9, d1=0.676, d2=0.685 g=0.920\n",
      ">204, 6/9, d1=0.679, d2=0.664 g=0.984\n",
      ">204, 7/9, d1=0.655, d2=0.556 g=1.028\n",
      ">204, 8/9, d1=0.747, d2=0.590 g=1.230\n",
      ">204, 9/9, d1=0.724, d2=0.467 g=1.348\n",
      ">205, 1/9, d1=0.781, d2=0.458 g=1.192\n",
      ">205, 2/9, d1=0.662, d2=0.515 g=1.049\n",
      ">205, 3/9, d1=0.717, d2=0.586 g=0.880\n",
      ">205, 4/9, d1=0.656, d2=0.600 g=0.935\n",
      ">205, 5/9, d1=0.623, d2=0.698 g=0.864\n",
      ">205, 6/9, d1=0.555, d2=0.707 g=0.855\n",
      ">205, 7/9, d1=0.605, d2=0.678 g=0.900\n",
      ">205, 8/9, d1=0.667, d2=0.643 g=0.877\n",
      ">205, 9/9, d1=0.656, d2=0.682 g=0.872\n",
      ">206, 1/9, d1=0.659, d2=0.694 g=0.811\n",
      ">206, 2/9, d1=0.665, d2=0.662 g=0.829\n",
      ">206, 3/9, d1=0.625, d2=0.734 g=0.790\n",
      ">206, 4/9, d1=0.642, d2=0.741 g=0.738\n",
      ">206, 5/9, d1=0.590, d2=0.790 g=0.747\n",
      ">206, 6/9, d1=0.561, d2=0.733 g=0.788\n",
      ">206, 7/9, d1=0.592, d2=0.703 g=0.921\n",
      ">206, 8/9, d1=0.597, d2=0.587 g=1.179\n",
      ">206, 9/9, d1=0.662, d2=0.547 g=1.257\n",
      ">207, 1/9, d1=0.682, d2=0.537 g=1.078\n",
      ">207, 2/9, d1=0.624, d2=0.601 g=0.961\n",
      ">207, 3/9, d1=0.559, d2=0.666 g=0.846\n",
      ">207, 4/9, d1=0.568, d2=0.670 g=0.835\n",
      ">207, 5/9, d1=0.666, d2=0.670 g=0.784\n",
      ">207, 6/9, d1=0.622, d2=0.712 g=0.786\n",
      ">207, 7/9, d1=0.631, d2=0.711 g=0.814\n",
      ">207, 8/9, d1=0.612, d2=0.705 g=0.817\n",
      ">207, 9/9, d1=0.635, d2=0.644 g=0.840\n",
      ">208, 1/9, d1=0.656, d2=0.609 g=0.831\n",
      ">208, 2/9, d1=0.653, d2=0.630 g=0.849\n",
      ">208, 3/9, d1=0.626, d2=0.641 g=0.861\n",
      ">208, 4/9, d1=0.642, d2=0.679 g=0.896\n",
      ">208, 5/9, d1=0.635, d2=0.639 g=0.916\n",
      ">208, 6/9, d1=0.637, d2=0.648 g=0.914\n",
      ">208, 7/9, d1=0.676, d2=0.586 g=0.934\n",
      ">208, 8/9, d1=0.690, d2=0.581 g=0.976\n",
      ">208, 9/9, d1=0.693, d2=0.529 g=0.993\n",
      ">209, 1/9, d1=0.653, d2=0.598 g=0.968\n",
      ">209, 2/9, d1=0.749, d2=0.597 g=0.954\n",
      ">209, 3/9, d1=0.610, d2=0.665 g=0.976\n",
      ">209, 4/9, d1=0.725, d2=0.620 g=0.990\n",
      ">209, 5/9, d1=0.708, d2=0.638 g=0.972\n",
      ">209, 6/9, d1=0.722, d2=0.624 g=1.020\n",
      ">209, 7/9, d1=0.703, d2=0.587 g=0.946\n",
      ">209, 8/9, d1=0.739, d2=0.650 g=0.895\n",
      ">209, 9/9, d1=0.720, d2=0.655 g=0.860\n",
      ">210, 1/9, d1=0.718, d2=0.625 g=0.915\n",
      ">210, 2/9, d1=0.709, d2=0.668 g=0.894\n",
      ">210, 3/9, d1=0.775, d2=0.626 g=0.945\n",
      ">210, 4/9, d1=0.663, d2=0.577 g=0.977\n",
      ">210, 5/9, d1=0.736, d2=0.552 g=1.031\n",
      ">210, 6/9, d1=0.733, d2=0.540 g=1.029\n",
      ">210, 7/9, d1=0.800, d2=0.573 g=0.904\n",
      ">210, 8/9, d1=0.756, d2=0.627 g=0.897\n",
      ">210, 9/9, d1=0.723, d2=0.643 g=0.837\n",
      ">Accuracy real: 57%, fake: 75%\n",
      ">211, 1/9, d1=0.655, d2=0.631 g=0.771\n",
      ">211, 2/9, d1=0.674, d2=0.676 g=0.805\n",
      ">211, 3/9, d1=0.683, d2=0.614 g=0.830\n",
      ">211, 4/9, d1=0.614, d2=0.658 g=0.804\n",
      ">211, 5/9, d1=0.662, d2=0.648 g=0.835\n",
      ">211, 6/9, d1=0.638, d2=0.631 g=0.829\n",
      ">211, 7/9, d1=0.599, d2=0.647 g=0.865\n",
      ">211, 8/9, d1=0.659, d2=0.629 g=0.877\n",
      ">211, 9/9, d1=0.665, d2=0.614 g=0.880\n",
      ">212, 1/9, d1=0.667, d2=0.607 g=0.887\n",
      ">212, 2/9, d1=0.629, d2=0.605 g=0.863\n",
      ">212, 3/9, d1=0.607, d2=0.642 g=0.865\n",
      ">212, 4/9, d1=0.646, d2=0.647 g=0.849\n",
      ">212, 5/9, d1=0.587, d2=0.626 g=0.845\n",
      ">212, 6/9, d1=0.600, d2=0.644 g=0.881\n",
      ">212, 7/9, d1=0.578, d2=0.655 g=0.814\n",
      ">212, 8/9, d1=0.655, d2=0.663 g=0.820\n",
      ">212, 9/9, d1=0.657, d2=0.721 g=0.821\n",
      ">213, 1/9, d1=0.642, d2=0.681 g=0.817\n",
      ">213, 2/9, d1=0.639, d2=0.687 g=0.760\n",
      ">213, 3/9, d1=0.660, d2=0.746 g=0.788\n",
      ">213, 4/9, d1=0.657, d2=0.716 g=0.748\n",
      ">213, 5/9, d1=0.656, d2=0.759 g=0.754\n",
      ">213, 6/9, d1=0.665, d2=0.737 g=0.761\n",
      ">213, 7/9, d1=0.641, d2=0.699 g=0.787\n",
      ">213, 8/9, d1=0.656, d2=0.721 g=0.811\n",
      ">213, 9/9, d1=0.667, d2=0.649 g=0.852\n",
      ">214, 1/9, d1=0.717, d2=0.666 g=0.875\n",
      ">214, 2/9, d1=0.699, d2=0.663 g=0.822\n",
      ">214, 3/9, d1=0.688, d2=0.668 g=0.866\n",
      ">214, 4/9, d1=0.660, d2=0.678 g=0.872\n",
      ">214, 5/9, d1=0.679, d2=0.634 g=0.877\n",
      ">214, 6/9, d1=0.670, d2=0.619 g=0.864\n",
      ">214, 7/9, d1=0.677, d2=0.647 g=0.880\n",
      ">214, 8/9, d1=0.668, d2=0.606 g=0.933\n",
      ">214, 9/9, d1=0.663, d2=0.585 g=0.947\n",
      ">215, 1/9, d1=0.663, d2=0.562 g=0.921\n",
      ">215, 2/9, d1=0.630, d2=0.576 g=0.916\n",
      ">215, 3/9, d1=0.633, d2=0.602 g=0.895\n",
      ">215, 4/9, d1=0.630, d2=0.591 g=0.861\n",
      ">215, 5/9, d1=0.601, d2=0.632 g=0.849\n",
      ">215, 6/9, d1=0.610, d2=0.630 g=0.832\n",
      ">215, 7/9, d1=0.594, d2=0.643 g=0.812\n",
      ">215, 8/9, d1=0.579, d2=0.671 g=0.803\n",
      ">215, 9/9, d1=0.579, d2=0.674 g=0.829\n",
      ">216, 1/9, d1=0.559, d2=0.665 g=0.871\n",
      ">216, 2/9, d1=0.563, d2=0.648 g=0.871\n",
      ">216, 3/9, d1=0.583, d2=0.621 g=0.956\n",
      ">216, 4/9, d1=0.595, d2=0.599 g=0.885\n",
      ">216, 5/9, d1=0.547, d2=0.609 g=0.924\n",
      ">216, 6/9, d1=0.653, d2=0.590 g=0.897\n",
      ">216, 7/9, d1=0.569, d2=0.613 g=0.934\n",
      ">216, 8/9, d1=0.611, d2=0.736 g=0.913\n",
      ">216, 9/9, d1=0.577, d2=0.596 g=0.978\n",
      ">217, 1/9, d1=0.579, d2=0.702 g=0.912\n",
      ">217, 2/9, d1=0.587, d2=0.600 g=0.846\n",
      ">217, 3/9, d1=0.644, d2=0.835 g=0.929\n",
      ">217, 4/9, d1=0.705, d2=0.655 g=0.941\n",
      ">217, 5/9, d1=0.687, d2=0.683 g=0.934\n",
      ">217, 6/9, d1=0.696, d2=0.593 g=1.013\n",
      ">217, 7/9, d1=0.756, d2=0.546 g=1.045\n",
      ">217, 8/9, d1=0.681, d2=0.484 g=1.095\n",
      ">217, 9/9, d1=0.672, d2=0.482 g=1.167\n",
      ">218, 1/9, d1=0.721, d2=0.520 g=1.025\n",
      ">218, 2/9, d1=0.670, d2=0.592 g=1.048\n",
      ">218, 3/9, d1=0.567, d2=0.605 g=0.940\n",
      ">218, 4/9, d1=0.627, d2=0.667 g=0.898\n",
      ">218, 5/9, d1=0.582, d2=0.675 g=0.864\n",
      ">218, 6/9, d1=0.713, d2=0.621 g=0.828\n",
      ">218, 7/9, d1=0.573, d2=0.672 g=0.864\n",
      ">218, 8/9, d1=0.642, d2=0.656 g=0.868\n",
      ">218, 9/9, d1=0.601, d2=0.610 g=0.924\n",
      ">219, 1/9, d1=0.627, d2=0.582 g=0.928\n",
      ">219, 2/9, d1=0.576, d2=0.587 g=0.935\n",
      ">219, 3/9, d1=0.610, d2=0.636 g=0.914\n",
      ">219, 4/9, d1=0.562, d2=0.641 g=0.902\n",
      ">219, 5/9, d1=0.552, d2=0.678 g=0.870\n",
      ">219, 6/9, d1=0.580, d2=0.676 g=0.828\n",
      ">219, 7/9, d1=0.515, d2=0.684 g=0.807\n",
      ">219, 8/9, d1=0.568, d2=0.730 g=0.826\n",
      ">219, 9/9, d1=0.551, d2=0.721 g=0.743\n",
      ">220, 1/9, d1=0.544, d2=0.826 g=0.815\n",
      ">220, 2/9, d1=0.573, d2=0.738 g=0.815\n",
      ">220, 3/9, d1=0.624, d2=0.672 g=0.861\n",
      ">220, 4/9, d1=0.628, d2=0.703 g=0.860\n",
      ">220, 5/9, d1=0.633, d2=0.657 g=0.875\n",
      ">220, 6/9, d1=0.626, d2=0.679 g=0.884\n",
      ">220, 7/9, d1=0.619, d2=0.704 g=0.862\n",
      ">220, 8/9, d1=0.592, d2=0.760 g=0.809\n",
      ">220, 9/9, d1=0.649, d2=0.844 g=0.850\n",
      ">Accuracy real: 52%, fake: 48%\n",
      ">221, 1/9, d1=0.753, d2=0.744 g=0.895\n",
      ">221, 2/9, d1=0.687, d2=0.774 g=0.919\n",
      ">221, 3/9, d1=0.788, d2=0.618 g=1.021\n",
      ">221, 4/9, d1=0.803, d2=0.603 g=1.058\n",
      ">221, 5/9, d1=0.775, d2=0.517 g=1.102\n",
      ">221, 6/9, d1=0.725, d2=0.507 g=1.139\n",
      ">221, 7/9, d1=0.723, d2=0.510 g=1.163\n",
      ">221, 8/9, d1=0.720, d2=0.509 g=1.108\n",
      ">221, 9/9, d1=0.722, d2=0.543 g=1.040\n",
      ">222, 1/9, d1=0.670, d2=0.559 g=0.982\n",
      ">222, 2/9, d1=0.670, d2=0.577 g=0.947\n",
      ">222, 3/9, d1=0.666, d2=0.617 g=0.898\n",
      ">222, 4/9, d1=0.608, d2=0.664 g=0.845\n",
      ">222, 5/9, d1=0.553, d2=0.666 g=0.848\n",
      ">222, 6/9, d1=0.604, d2=0.712 g=0.799\n",
      ">222, 7/9, d1=0.537, d2=0.788 g=0.793\n",
      ">222, 8/9, d1=0.614, d2=0.775 g=0.800\n",
      ">222, 9/9, d1=0.613, d2=0.766 g=0.875\n",
      ">223, 1/9, d1=0.576, d2=0.675 g=0.938\n",
      ">223, 2/9, d1=0.673, d2=0.601 g=0.984\n",
      ">223, 3/9, d1=0.699, d2=0.603 g=1.008\n",
      ">223, 4/9, d1=0.707, d2=0.542 g=1.097\n",
      ">223, 5/9, d1=0.665, d2=0.595 g=1.071\n",
      ">223, 6/9, d1=0.641, d2=0.535 g=1.092\n",
      ">223, 7/9, d1=0.650, d2=0.543 g=1.024\n",
      ">223, 8/9, d1=0.615, d2=0.571 g=1.026\n",
      ">223, 9/9, d1=0.596, d2=0.654 g=1.014\n",
      ">224, 1/9, d1=0.646, d2=0.593 g=1.010\n",
      ">224, 2/9, d1=0.698, d2=0.591 g=1.072\n",
      ">224, 3/9, d1=0.726, d2=0.596 g=1.111\n",
      ">224, 4/9, d1=0.740, d2=0.573 g=1.076\n",
      ">224, 5/9, d1=0.795, d2=0.587 g=1.008\n",
      ">224, 6/9, d1=0.809, d2=0.672 g=0.954\n",
      ">224, 7/9, d1=0.682, d2=0.717 g=0.954\n",
      ">224, 8/9, d1=0.794, d2=0.691 g=0.872\n",
      ">224, 9/9, d1=0.797, d2=0.698 g=0.798\n",
      ">225, 1/9, d1=0.819, d2=0.724 g=0.797\n",
      ">225, 2/9, d1=0.693, d2=0.714 g=0.826\n",
      ">225, 3/9, d1=0.733, d2=0.669 g=0.863\n",
      ">225, 4/9, d1=0.719, d2=0.643 g=0.879\n",
      ">225, 5/9, d1=0.700, d2=0.566 g=0.922\n",
      ">225, 6/9, d1=0.660, d2=0.580 g=0.995\n",
      ">225, 7/9, d1=0.637, d2=0.578 g=1.029\n",
      ">225, 8/9, d1=0.682, d2=0.526 g=0.950\n",
      ">225, 9/9, d1=0.645, d2=0.661 g=0.946\n",
      ">226, 1/9, d1=0.648, d2=0.565 g=0.880\n",
      ">226, 2/9, d1=0.665, d2=0.707 g=0.819\n",
      ">226, 3/9, d1=0.652, d2=0.753 g=0.810\n",
      ">226, 4/9, d1=0.696, d2=0.720 g=0.771\n",
      ">226, 5/9, d1=0.649, d2=0.728 g=0.787\n",
      ">226, 6/9, d1=0.687, d2=0.665 g=0.775\n",
      ">226, 7/9, d1=0.624, d2=0.682 g=0.817\n",
      ">226, 8/9, d1=0.704, d2=0.735 g=0.823\n",
      ">226, 9/9, d1=0.665, d2=0.699 g=0.816\n",
      ">227, 1/9, d1=0.705, d2=0.748 g=0.808\n",
      ">227, 2/9, d1=0.629, d2=0.764 g=0.768\n",
      ">227, 3/9, d1=0.704, d2=0.754 g=0.812\n",
      ">227, 4/9, d1=0.755, d2=0.744 g=0.847\n",
      ">227, 5/9, d1=0.725, d2=0.658 g=0.892\n",
      ">227, 6/9, d1=0.683, d2=0.614 g=1.000\n",
      ">227, 7/9, d1=0.775, d2=0.512 g=1.141\n",
      ">227, 8/9, d1=0.783, d2=0.465 g=1.209\n",
      ">227, 9/9, d1=0.727, d2=0.506 g=1.099\n",
      ">228, 1/9, d1=0.694, d2=0.565 g=1.017\n",
      ">228, 2/9, d1=0.779, d2=0.674 g=0.830\n",
      ">228, 3/9, d1=0.733, d2=0.724 g=0.828\n",
      ">228, 4/9, d1=0.813, d2=0.693 g=0.825\n",
      ">228, 5/9, d1=0.824, d2=0.667 g=0.853\n",
      ">228, 6/9, d1=0.762, d2=0.634 g=0.917\n",
      ">228, 7/9, d1=0.708, d2=0.585 g=0.973\n",
      ">228, 8/9, d1=0.712, d2=0.591 g=0.997\n",
      ">228, 9/9, d1=0.702, d2=0.580 g=0.933\n",
      ">229, 1/9, d1=0.731, d2=0.634 g=0.946\n",
      ">229, 2/9, d1=0.812, d2=0.732 g=0.785\n",
      ">229, 3/9, d1=0.686, d2=0.746 g=0.695\n",
      ">229, 4/9, d1=0.704, d2=0.796 g=0.714\n",
      ">229, 5/9, d1=0.691, d2=0.758 g=0.750\n",
      ">229, 6/9, d1=0.671, d2=0.690 g=0.769\n",
      ">229, 7/9, d1=0.672, d2=0.661 g=0.841\n",
      ">229, 8/9, d1=0.655, d2=0.643 g=0.849\n",
      ">229, 9/9, d1=0.720, d2=0.643 g=0.820\n",
      ">230, 1/9, d1=0.654, d2=0.619 g=0.843\n",
      ">230, 2/9, d1=0.632, d2=0.628 g=0.835\n",
      ">230, 3/9, d1=0.652, d2=0.648 g=0.836\n",
      ">230, 4/9, d1=0.641, d2=0.633 g=0.833\n",
      ">230, 5/9, d1=0.711, d2=0.677 g=0.834\n",
      ">230, 6/9, d1=0.640, d2=0.666 g=0.817\n",
      ">230, 7/9, d1=0.632, d2=0.643 g=0.815\n",
      ">230, 8/9, d1=0.747, d2=0.655 g=0.814\n",
      ">230, 9/9, d1=0.632, d2=0.650 g=0.829\n",
      ">Accuracy real: 53%, fake: 76%\n",
      ">231, 1/9, d1=0.642, d2=0.635 g=0.811\n",
      ">231, 2/9, d1=0.695, d2=0.636 g=0.845\n",
      ">231, 3/9, d1=0.677, d2=0.627 g=0.798\n",
      ">231, 4/9, d1=0.656, d2=0.674 g=0.789\n",
      ">231, 5/9, d1=0.663, d2=0.667 g=0.788\n",
      ">231, 6/9, d1=0.675, d2=0.668 g=0.801\n",
      ">231, 7/9, d1=0.709, d2=0.657 g=0.787\n",
      ">231, 8/9, d1=0.670, d2=0.687 g=0.756\n",
      ">231, 9/9, d1=0.685, d2=0.695 g=0.786\n",
      ">232, 1/9, d1=0.634, d2=0.701 g=0.781\n",
      ">232, 2/9, d1=0.665, d2=0.688 g=0.758\n",
      ">232, 3/9, d1=0.619, d2=0.715 g=0.758\n",
      ">232, 4/9, d1=0.662, d2=0.697 g=0.787\n",
      ">232, 5/9, d1=0.658, d2=0.681 g=0.789\n",
      ">232, 6/9, d1=0.666, d2=0.634 g=0.821\n",
      ">232, 7/9, d1=0.639, d2=0.642 g=0.841\n",
      ">232, 8/9, d1=0.642, d2=0.621 g=0.835\n",
      ">232, 9/9, d1=0.610, d2=0.607 g=0.860\n",
      ">233, 1/9, d1=0.615, d2=0.619 g=0.855\n",
      ">233, 2/9, d1=0.583, d2=0.618 g=0.890\n",
      ">233, 3/9, d1=0.558, d2=0.626 g=0.874\n",
      ">233, 4/9, d1=0.570, d2=0.597 g=0.879\n",
      ">233, 5/9, d1=0.595, d2=0.572 g=0.846\n",
      ">233, 6/9, d1=0.566, d2=0.639 g=0.853\n",
      ">233, 7/9, d1=0.572, d2=0.653 g=0.820\n",
      ">233, 8/9, d1=0.548, d2=0.675 g=0.837\n",
      ">233, 9/9, d1=0.571, d2=0.645 g=0.793\n",
      ">234, 1/9, d1=0.552, d2=0.700 g=0.812\n",
      ">234, 2/9, d1=0.529, d2=0.664 g=0.839\n",
      ">234, 3/9, d1=0.597, d2=0.721 g=0.805\n",
      ">234, 4/9, d1=0.613, d2=0.684 g=0.779\n",
      ">234, 5/9, d1=0.615, d2=0.743 g=0.768\n",
      ">234, 6/9, d1=0.631, d2=0.717 g=0.760\n",
      ">234, 7/9, d1=0.677, d2=0.700 g=0.789\n",
      ">234, 8/9, d1=0.660, d2=0.743 g=0.811\n",
      ">234, 9/9, d1=0.662, d2=0.716 g=0.907\n",
      ">235, 1/9, d1=0.738, d2=0.635 g=0.891\n",
      ">235, 2/9, d1=0.744, d2=0.591 g=0.938\n",
      ">235, 3/9, d1=0.765, d2=0.562 g=0.955\n",
      ">235, 4/9, d1=0.768, d2=0.560 g=1.015\n",
      ">235, 5/9, d1=0.761, d2=0.549 g=0.945\n",
      ">235, 6/9, d1=0.753, d2=0.587 g=1.018\n",
      ">235, 7/9, d1=0.744, d2=0.572 g=0.977\n",
      ">235, 8/9, d1=0.768, d2=0.582 g=0.953\n",
      ">235, 9/9, d1=0.720, d2=0.575 g=0.943\n",
      ">236, 1/9, d1=0.767, d2=0.574 g=0.952\n",
      ">236, 2/9, d1=0.719, d2=0.586 g=0.880\n",
      ">236, 3/9, d1=0.755, d2=0.618 g=0.832\n",
      ">236, 4/9, d1=0.732, d2=0.644 g=0.823\n",
      ">236, 5/9, d1=0.763, d2=0.629 g=0.815\n",
      ">236, 6/9, d1=0.675, d2=0.654 g=0.802\n",
      ">236, 7/9, d1=0.684, d2=0.643 g=0.835\n",
      ">236, 8/9, d1=0.718, d2=0.661 g=0.864\n",
      ">236, 9/9, d1=0.671, d2=0.679 g=0.830\n",
      ">237, 1/9, d1=0.693, d2=0.659 g=0.804\n",
      ">237, 2/9, d1=0.639, d2=0.657 g=0.836\n",
      ">237, 3/9, d1=0.653, d2=0.664 g=0.847\n",
      ">237, 4/9, d1=0.674, d2=0.650 g=0.830\n",
      ">237, 5/9, d1=0.650, d2=0.702 g=0.785\n",
      ">237, 6/9, d1=0.589, d2=0.689 g=0.827\n",
      ">237, 7/9, d1=0.584, d2=0.648 g=0.833\n",
      ">237, 8/9, d1=0.582, d2=0.668 g=0.831\n",
      ">237, 9/9, d1=0.623, d2=0.660 g=0.831\n",
      ">238, 1/9, d1=0.549, d2=0.655 g=0.821\n",
      ">238, 2/9, d1=0.633, d2=0.634 g=0.817\n",
      ">238, 3/9, d1=0.610, d2=0.676 g=0.805\n",
      ">238, 4/9, d1=0.613, d2=0.663 g=0.796\n",
      ">238, 5/9, d1=0.645, d2=0.690 g=0.799\n",
      ">238, 6/9, d1=0.633, d2=0.662 g=0.811\n",
      ">238, 7/9, d1=0.575, d2=0.697 g=0.766\n",
      ">238, 8/9, d1=0.591, d2=0.764 g=0.765\n",
      ">238, 9/9, d1=0.564, d2=0.740 g=0.772\n",
      ">239, 1/9, d1=0.609, d2=0.789 g=0.758\n",
      ">239, 2/9, d1=0.675, d2=0.776 g=0.814\n",
      ">239, 3/9, d1=0.663, d2=0.633 g=0.834\n",
      ">239, 4/9, d1=0.637, d2=0.647 g=0.901\n",
      ">239, 5/9, d1=0.650, d2=0.614 g=0.871\n",
      ">239, 6/9, d1=0.657, d2=0.600 g=0.925\n",
      ">239, 7/9, d1=0.675, d2=0.665 g=0.911\n",
      ">239, 8/9, d1=0.671, d2=0.662 g=0.844\n",
      ">239, 9/9, d1=0.662, d2=0.671 g=0.846\n",
      ">240, 1/9, d1=0.683, d2=0.729 g=0.828\n",
      ">240, 2/9, d1=0.700, d2=0.712 g=0.824\n",
      ">240, 3/9, d1=0.703, d2=0.706 g=0.801\n",
      ">240, 4/9, d1=0.709, d2=0.655 g=0.827\n",
      ">240, 5/9, d1=0.709, d2=0.606 g=0.866\n",
      ">240, 6/9, d1=0.726, d2=0.627 g=0.859\n",
      ">240, 7/9, d1=0.663, d2=0.580 g=0.903\n",
      ">240, 8/9, d1=0.659, d2=0.605 g=0.961\n",
      ">240, 9/9, d1=0.655, d2=0.594 g=0.948\n",
      ">Accuracy real: 60%, fake: 89%\n",
      ">241, 1/9, d1=0.587, d2=0.582 g=0.917\n",
      ">241, 2/9, d1=0.644, d2=0.559 g=0.936\n",
      ">241, 3/9, d1=0.576, d2=0.608 g=0.884\n",
      ">241, 4/9, d1=0.585, d2=0.629 g=0.905\n",
      ">241, 5/9, d1=0.564, d2=0.729 g=0.869\n",
      ">241, 6/9, d1=0.643, d2=0.703 g=0.835\n",
      ">241, 7/9, d1=0.572, d2=0.729 g=0.824\n",
      ">241, 8/9, d1=0.589, d2=0.751 g=0.769\n",
      ">241, 9/9, d1=0.687, d2=0.763 g=0.791\n",
      ">242, 1/9, d1=0.677, d2=0.709 g=0.862\n",
      ">242, 2/9, d1=0.663, d2=0.597 g=1.006\n",
      ">242, 3/9, d1=0.685, d2=0.538 g=1.043\n",
      ">242, 4/9, d1=0.753, d2=0.486 g=1.146\n",
      ">242, 5/9, d1=0.699, d2=0.552 g=1.167\n",
      ">242, 6/9, d1=0.701, d2=0.614 g=1.028\n",
      ">242, 7/9, d1=0.662, d2=0.645 g=0.963\n",
      ">242, 8/9, d1=0.702, d2=0.676 g=0.843\n",
      ">242, 9/9, d1=0.643, d2=0.727 g=0.813\n",
      ">243, 1/9, d1=0.725, d2=0.830 g=0.786\n",
      ">243, 2/9, d1=0.641, d2=0.763 g=0.836\n",
      ">243, 3/9, d1=0.743, d2=0.610 g=0.926\n",
      ">243, 4/9, d1=0.724, d2=0.570 g=1.052\n",
      ">243, 5/9, d1=0.676, d2=0.521 g=1.152\n",
      ">243, 6/9, d1=0.767, d2=0.469 g=1.198\n",
      ">243, 7/9, d1=0.691, d2=0.493 g=1.130\n",
      ">243, 8/9, d1=0.701, d2=0.500 g=1.121\n",
      ">243, 9/9, d1=0.728, d2=0.519 g=1.103\n",
      ">244, 1/9, d1=0.674, d2=0.524 g=1.041\n",
      ">244, 2/9, d1=0.718, d2=0.591 g=0.893\n",
      ">244, 3/9, d1=0.704, d2=0.648 g=0.906\n",
      ">244, 4/9, d1=0.688, d2=0.668 g=0.868\n",
      ">244, 5/9, d1=0.706, d2=0.650 g=0.929\n",
      ">244, 6/9, d1=0.709, d2=0.610 g=0.939\n",
      ">244, 7/9, d1=0.624, d2=0.547 g=1.025\n",
      ">244, 8/9, d1=0.668, d2=0.530 g=1.054\n",
      ">244, 9/9, d1=0.668, d2=0.537 g=1.021\n",
      ">245, 1/9, d1=0.683, d2=0.531 g=1.023\n",
      ">245, 2/9, d1=0.684, d2=0.582 g=0.965\n",
      ">245, 3/9, d1=0.638, d2=0.678 g=0.914\n",
      ">245, 4/9, d1=0.630, d2=0.723 g=0.885\n",
      ">245, 5/9, d1=0.668, d2=0.661 g=0.851\n",
      ">245, 6/9, d1=0.637, d2=0.669 g=0.845\n",
      ">245, 7/9, d1=0.677, d2=0.665 g=0.848\n",
      ">245, 8/9, d1=0.733, d2=0.641 g=0.873\n",
      ">245, 9/9, d1=0.724, d2=0.593 g=0.917\n",
      ">246, 1/9, d1=0.700, d2=0.619 g=0.894\n",
      ">246, 2/9, d1=0.650, d2=0.653 g=0.851\n",
      ">246, 3/9, d1=0.691, d2=0.658 g=0.812\n",
      ">246, 4/9, d1=0.621, d2=0.677 g=0.768\n",
      ">246, 5/9, d1=0.579, d2=0.684 g=0.786\n",
      ">246, 6/9, d1=0.593, d2=0.707 g=0.759\n",
      ">246, 7/9, d1=0.616, d2=0.682 g=0.773\n",
      ">246, 8/9, d1=0.555, d2=0.671 g=0.787\n",
      ">246, 9/9, d1=0.545, d2=0.682 g=0.802\n",
      ">247, 1/9, d1=0.522, d2=0.648 g=0.817\n",
      ">247, 2/9, d1=0.447, d2=0.648 g=0.827\n",
      ">247, 3/9, d1=0.516, d2=0.654 g=0.821\n",
      ">247, 4/9, d1=0.513, d2=0.709 g=0.802\n",
      ">247, 5/9, d1=0.536, d2=0.725 g=0.814\n",
      ">247, 6/9, d1=0.583, d2=0.735 g=0.788\n",
      ">247, 7/9, d1=0.583, d2=0.787 g=0.797\n",
      ">247, 8/9, d1=0.585, d2=0.708 g=0.815\n",
      ">247, 9/9, d1=0.602, d2=0.720 g=0.847\n",
      ">248, 1/9, d1=0.544, d2=0.669 g=0.859\n",
      ">248, 2/9, d1=0.637, d2=0.669 g=0.825\n",
      ">248, 3/9, d1=0.651, d2=0.751 g=0.843\n",
      ">248, 4/9, d1=0.665, d2=0.695 g=0.812\n",
      ">248, 5/9, d1=0.603, d2=0.800 g=0.803\n",
      ">248, 6/9, d1=0.684, d2=0.743 g=0.820\n",
      ">248, 7/9, d1=0.666, d2=0.708 g=0.892\n",
      ">248, 8/9, d1=0.697, d2=0.642 g=0.902\n",
      ">248, 9/9, d1=0.714, d2=0.601 g=0.951\n",
      ">249, 1/9, d1=0.747, d2=0.571 g=0.986\n",
      ">249, 2/9, d1=0.719, d2=0.572 g=0.941\n",
      ">249, 3/9, d1=0.722, d2=0.632 g=0.927\n",
      ">249, 4/9, d1=0.724, d2=0.604 g=0.918\n",
      ">249, 5/9, d1=0.797, d2=0.639 g=0.829\n",
      ">249, 6/9, d1=0.677, d2=0.702 g=0.793\n",
      ">249, 7/9, d1=0.706, d2=0.761 g=0.753\n",
      ">249, 8/9, d1=0.627, d2=0.757 g=0.756\n",
      ">249, 9/9, d1=0.687, d2=0.710 g=0.718\n",
      ">250, 1/9, d1=0.747, d2=0.790 g=0.730\n",
      ">250, 2/9, d1=0.703, d2=0.742 g=0.760\n",
      ">250, 3/9, d1=0.667, d2=0.729 g=0.814\n",
      ">250, 4/9, d1=0.695, d2=0.655 g=0.864\n",
      ">250, 5/9, d1=0.734, d2=0.618 g=0.921\n",
      ">250, 6/9, d1=0.664, d2=0.573 g=0.991\n",
      ">250, 7/9, d1=0.678, d2=0.559 g=0.987\n",
      ">250, 8/9, d1=0.684, d2=0.590 g=0.903\n",
      ">250, 9/9, d1=0.688, d2=0.607 g=0.898\n",
      ">Accuracy real: 64%, fake: 65%\n",
      ">251, 1/9, d1=0.656, d2=0.684 g=0.859\n",
      ">251, 2/9, d1=0.739, d2=0.734 g=0.882\n",
      ">251, 3/9, d1=0.670, d2=0.674 g=0.837\n",
      ">251, 4/9, d1=0.740, d2=0.630 g=0.887\n",
      ">251, 5/9, d1=0.775, d2=0.658 g=0.948\n",
      ">251, 6/9, d1=0.727, d2=0.594 g=0.962\n",
      ">251, 7/9, d1=0.747, d2=0.564 g=1.006\n",
      ">251, 8/9, d1=0.713, d2=0.568 g=0.968\n",
      ">251, 9/9, d1=0.704, d2=0.564 g=0.929\n",
      ">252, 1/9, d1=0.691, d2=0.587 g=0.932\n",
      ">252, 2/9, d1=0.745, d2=0.552 g=0.947\n",
      ">252, 3/9, d1=0.688, d2=0.573 g=0.920\n",
      ">252, 4/9, d1=0.663, d2=0.587 g=0.913\n",
      ">252, 5/9, d1=0.668, d2=0.564 g=0.960\n",
      ">252, 6/9, d1=0.646, d2=0.540 g=0.961\n",
      ">252, 7/9, d1=0.615, d2=0.529 g=1.006\n",
      ">252, 8/9, d1=0.635, d2=0.519 g=0.998\n",
      ">252, 9/9, d1=0.606, d2=0.517 g=1.000\n",
      ">253, 1/9, d1=0.592, d2=0.579 g=0.998\n",
      ">253, 2/9, d1=0.614, d2=0.561 g=0.937\n",
      ">253, 3/9, d1=0.588, d2=0.564 g=0.917\n",
      ">253, 4/9, d1=0.627, d2=0.645 g=0.908\n",
      ">253, 5/9, d1=0.609, d2=0.659 g=0.947\n",
      ">253, 6/9, d1=0.647, d2=0.593 g=0.933\n",
      ">253, 7/9, d1=0.629, d2=0.567 g=0.954\n",
      ">253, 8/9, d1=0.633, d2=0.584 g=0.956\n",
      ">253, 9/9, d1=0.649, d2=0.628 g=0.984\n",
      ">254, 1/9, d1=0.661, d2=0.617 g=0.928\n",
      ">254, 2/9, d1=0.664, d2=0.705 g=0.874\n",
      ">254, 3/9, d1=0.629, d2=0.763 g=0.840\n",
      ">254, 4/9, d1=0.685, d2=0.771 g=0.829\n",
      ">254, 5/9, d1=0.633, d2=0.703 g=0.806\n",
      ">254, 6/9, d1=0.692, d2=0.723 g=0.775\n",
      ">254, 7/9, d1=0.707, d2=0.796 g=0.741\n",
      ">254, 8/9, d1=0.681, d2=0.723 g=0.800\n",
      ">254, 9/9, d1=0.715, d2=0.738 g=0.797\n",
      ">255, 1/9, d1=0.681, d2=0.718 g=0.889\n",
      ">255, 2/9, d1=0.707, d2=0.606 g=0.929\n",
      ">255, 3/9, d1=0.682, d2=0.588 g=0.915\n",
      ">255, 4/9, d1=0.663, d2=0.635 g=0.916\n",
      ">255, 5/9, d1=0.674, d2=0.556 g=0.909\n",
      ">255, 6/9, d1=0.657, d2=0.598 g=0.911\n",
      ">255, 7/9, d1=0.647, d2=0.575 g=0.887\n",
      ">255, 8/9, d1=0.616, d2=0.610 g=0.846\n",
      ">255, 9/9, d1=0.609, d2=0.639 g=0.847\n",
      ">256, 1/9, d1=0.609, d2=0.636 g=0.849\n",
      ">256, 2/9, d1=0.612, d2=0.605 g=0.835\n",
      ">256, 3/9, d1=0.617, d2=0.653 g=0.838\n",
      ">256, 4/9, d1=0.622, d2=0.654 g=0.823\n",
      ">256, 5/9, d1=0.578, d2=0.643 g=0.850\n",
      ">256, 6/9, d1=0.584, d2=0.618 g=0.861\n",
      ">256, 7/9, d1=0.565, d2=0.633 g=0.909\n",
      ">256, 8/9, d1=0.628, d2=0.585 g=0.923\n",
      ">256, 9/9, d1=0.578, d2=0.607 g=0.903\n",
      ">257, 1/9, d1=0.596, d2=0.584 g=0.901\n",
      ">257, 2/9, d1=0.561, d2=0.589 g=0.868\n",
      ">257, 3/9, d1=0.515, d2=0.637 g=0.859\n",
      ">257, 4/9, d1=0.574, d2=0.666 g=0.820\n",
      ">257, 5/9, d1=0.593, d2=0.681 g=0.819\n",
      ">257, 6/9, d1=0.602, d2=0.656 g=0.774\n",
      ">257, 7/9, d1=0.563, d2=0.753 g=0.779\n",
      ">257, 8/9, d1=0.541, d2=0.703 g=0.753\n",
      ">257, 9/9, d1=0.548, d2=0.673 g=0.794\n",
      ">258, 1/9, d1=0.508, d2=0.629 g=0.830\n",
      ">258, 2/9, d1=0.579, d2=0.670 g=0.876\n",
      ">258, 3/9, d1=0.561, d2=0.629 g=0.880\n",
      ">258, 4/9, d1=0.593, d2=0.591 g=0.917\n",
      ">258, 5/9, d1=0.477, d2=0.598 g=0.915\n",
      ">258, 6/9, d1=0.585, d2=0.646 g=0.889\n",
      ">258, 7/9, d1=0.595, d2=0.664 g=0.840\n",
      ">258, 8/9, d1=0.545, d2=0.664 g=0.848\n",
      ">258, 9/9, d1=0.605, d2=0.644 g=0.881\n",
      ">259, 1/9, d1=0.599, d2=0.684 g=0.931\n",
      ">259, 2/9, d1=0.618, d2=0.595 g=0.948\n",
      ">259, 3/9, d1=0.580, d2=0.610 g=0.985\n",
      ">259, 4/9, d1=0.646, d2=0.682 g=1.030\n",
      ">259, 5/9, d1=0.661, d2=0.585 g=1.178\n",
      ">259, 6/9, d1=0.656, d2=0.486 g=1.289\n",
      ">259, 7/9, d1=0.720, d2=0.420 g=1.353\n",
      ">259, 8/9, d1=0.593, d2=0.461 g=1.243\n",
      ">259, 9/9, d1=0.683, d2=0.539 g=1.144\n",
      ">260, 1/9, d1=0.779, d2=0.659 g=0.956\n",
      ">260, 2/9, d1=0.605, d2=0.850 g=0.857\n",
      ">260, 3/9, d1=0.636, d2=0.857 g=0.937\n",
      ">260, 4/9, d1=0.746, d2=0.745 g=1.018\n",
      ">260, 5/9, d1=0.792, d2=0.584 g=1.038\n",
      ">260, 6/9, d1=0.725, d2=0.562 g=1.050\n",
      ">260, 7/9, d1=0.770, d2=0.611 g=0.987\n",
      ">260, 8/9, d1=0.750, d2=0.577 g=0.935\n",
      ">260, 9/9, d1=0.630, d2=0.582 g=0.936\n",
      ">Accuracy real: 71%, fake: 83%\n",
      ">261, 1/9, d1=0.604, d2=0.599 g=0.882\n",
      ">261, 2/9, d1=0.578, d2=0.585 g=0.881\n",
      ">261, 3/9, d1=0.646, d2=0.688 g=0.855\n",
      ">261, 4/9, d1=0.534, d2=0.788 g=0.847\n",
      ">261, 5/9, d1=0.612, d2=0.768 g=0.823\n",
      ">261, 6/9, d1=0.627, d2=0.716 g=0.888\n",
      ">261, 7/9, d1=0.704, d2=0.780 g=0.827\n",
      ">261, 8/9, d1=0.637, d2=0.737 g=0.771\n",
      ">261, 9/9, d1=0.655, d2=0.753 g=0.814\n",
      ">262, 1/9, d1=0.657, d2=0.725 g=0.916\n",
      ">262, 2/9, d1=0.705, d2=0.644 g=0.966\n",
      ">262, 3/9, d1=0.693, d2=0.512 g=1.029\n",
      ">262, 4/9, d1=0.707, d2=0.502 g=1.123\n",
      ">262, 5/9, d1=0.681, d2=0.489 g=1.176\n",
      ">262, 6/9, d1=0.668, d2=0.446 g=1.200\n",
      ">262, 7/9, d1=0.631, d2=0.461 g=1.248\n",
      ">262, 8/9, d1=0.703, d2=0.503 g=1.183\n",
      ">262, 9/9, d1=0.703, d2=0.411 g=1.222\n",
      ">263, 1/9, d1=0.716, d2=0.545 g=1.064\n",
      ">263, 2/9, d1=0.736, d2=0.661 g=1.016\n",
      ">263, 3/9, d1=0.714, d2=0.604 g=0.999\n",
      ">263, 4/9, d1=0.739, d2=0.636 g=0.960\n",
      ">263, 5/9, d1=0.722, d2=0.610 g=0.952\n",
      ">263, 6/9, d1=0.707, d2=0.642 g=0.905\n",
      ">263, 7/9, d1=0.795, d2=0.685 g=0.879\n",
      ">263, 8/9, d1=0.723, d2=0.676 g=0.822\n",
      ">263, 9/9, d1=0.697, d2=0.656 g=0.771\n",
      ">264, 1/9, d1=0.742, d2=0.679 g=0.760\n",
      ">264, 2/9, d1=0.669, d2=0.694 g=0.792\n",
      ">264, 3/9, d1=0.685, d2=0.675 g=0.789\n",
      ">264, 4/9, d1=0.589, d2=0.692 g=0.812\n",
      ">264, 5/9, d1=0.726, d2=0.658 g=0.810\n",
      ">264, 6/9, d1=0.569, d2=0.647 g=0.813\n",
      ">264, 7/9, d1=0.569, d2=0.648 g=0.820\n",
      ">264, 8/9, d1=0.591, d2=0.601 g=0.854\n",
      ">264, 9/9, d1=0.582, d2=0.633 g=0.869\n",
      ">265, 1/9, d1=0.553, d2=0.592 g=0.865\n",
      ">265, 2/9, d1=0.541, d2=0.618 g=0.882\n",
      ">265, 3/9, d1=0.568, d2=0.634 g=0.846\n",
      ">265, 4/9, d1=0.548, d2=0.626 g=0.854\n",
      ">265, 5/9, d1=0.603, d2=0.646 g=0.820\n",
      ">265, 6/9, d1=0.549, d2=0.719 g=0.797\n",
      ">265, 7/9, d1=0.562, d2=0.736 g=0.770\n",
      ">265, 8/9, d1=0.580, d2=0.743 g=0.778\n",
      ">265, 9/9, d1=0.622, d2=0.716 g=0.835\n",
      ">266, 1/9, d1=0.596, d2=0.693 g=0.838\n",
      ">266, 2/9, d1=0.638, d2=0.667 g=0.879\n",
      ">266, 3/9, d1=0.671, d2=0.634 g=0.960\n",
      ">266, 4/9, d1=0.653, d2=0.614 g=1.002\n",
      ">266, 5/9, d1=0.704, d2=0.557 g=1.004\n",
      ">266, 6/9, d1=0.646, d2=0.559 g=0.937\n",
      ">266, 7/9, d1=0.704, d2=0.597 g=0.918\n",
      ">266, 8/9, d1=0.655, d2=0.625 g=0.932\n",
      ">266, 9/9, d1=0.624, d2=0.624 g=0.909\n",
      ">267, 1/9, d1=0.702, d2=0.595 g=0.926\n",
      ">267, 2/9, d1=0.653, d2=0.630 g=0.890\n",
      ">267, 3/9, d1=0.675, d2=0.595 g=0.913\n",
      ">267, 4/9, d1=0.641, d2=0.568 g=0.984\n",
      ">267, 5/9, d1=0.664, d2=0.595 g=0.934\n",
      ">267, 6/9, d1=0.629, d2=0.561 g=0.949\n",
      ">267, 7/9, d1=0.674, d2=0.565 g=0.956\n",
      ">267, 8/9, d1=0.735, d2=0.580 g=0.969\n",
      ">267, 9/9, d1=0.666, d2=0.587 g=0.896\n",
      ">268, 1/9, d1=0.701, d2=0.596 g=0.889\n",
      ">268, 2/9, d1=0.651, d2=0.608 g=0.955\n",
      ">268, 3/9, d1=0.660, d2=0.544 g=0.989\n",
      ">268, 4/9, d1=0.665, d2=0.574 g=1.078\n",
      ">268, 5/9, d1=0.691, d2=0.513 g=1.026\n",
      ">268, 6/9, d1=0.674, d2=0.516 g=1.049\n",
      ">268, 7/9, d1=0.750, d2=0.552 g=1.008\n",
      ">268, 8/9, d1=0.606, d2=0.607 g=0.908\n",
      ">268, 9/9, d1=0.706, d2=0.629 g=0.906\n",
      ">269, 1/9, d1=0.643, d2=0.709 g=0.843\n",
      ">269, 2/9, d1=0.677, d2=0.736 g=0.843\n",
      ">269, 3/9, d1=0.622, d2=0.687 g=0.784\n",
      ">269, 4/9, d1=0.669, d2=0.705 g=0.832\n",
      ">269, 5/9, d1=0.741, d2=0.678 g=0.889\n",
      ">269, 6/9, d1=0.664, d2=0.622 g=0.934\n",
      ">269, 7/9, d1=0.649, d2=0.587 g=1.021\n",
      ">269, 8/9, d1=0.643, d2=0.520 g=1.077\n",
      ">269, 9/9, d1=0.636, d2=0.503 g=1.085\n",
      ">270, 1/9, d1=0.556, d2=0.508 g=1.121\n",
      ">270, 2/9, d1=0.600, d2=0.505 g=1.150\n",
      ">270, 3/9, d1=0.654, d2=0.466 g=1.160\n",
      ">270, 4/9, d1=0.482, d2=0.485 g=1.123\n",
      ">270, 5/9, d1=0.597, d2=0.471 g=1.038\n",
      ">270, 6/9, d1=0.549, d2=0.552 g=0.972\n",
      ">270, 7/9, d1=0.484, d2=0.617 g=0.909\n",
      ">270, 8/9, d1=0.459, d2=0.654 g=0.840\n",
      ">270, 9/9, d1=0.478, d2=0.665 g=0.881\n",
      ">Accuracy real: 66%, fake: 49%\n",
      ">271, 1/9, d1=0.639, d2=0.655 g=0.853\n",
      ">271, 2/9, d1=0.472, d2=0.690 g=0.890\n",
      ">271, 3/9, d1=0.543, d2=0.685 g=0.970\n",
      ">271, 4/9, d1=0.510, d2=0.649 g=1.100\n",
      ">271, 5/9, d1=0.553, d2=0.555 g=1.342\n",
      ">271, 6/9, d1=0.603, d2=0.419 g=1.508\n",
      ">271, 7/9, d1=0.557, d2=0.392 g=1.455\n",
      ">271, 8/9, d1=0.555, d2=0.424 g=1.409\n",
      ">271, 9/9, d1=0.559, d2=0.567 g=1.107\n",
      ">272, 1/9, d1=0.553, d2=0.799 g=0.806\n",
      ">272, 2/9, d1=0.572, d2=1.008 g=0.692\n",
      ">272, 3/9, d1=0.538, d2=1.108 g=0.780\n",
      ">272, 4/9, d1=0.671, d2=0.770 g=0.977\n",
      ">272, 5/9, d1=0.608, d2=0.612 g=1.185\n",
      ">272, 6/9, d1=0.624, d2=0.478 g=1.409\n",
      ">272, 7/9, d1=0.655, d2=0.432 g=1.399\n",
      ">272, 8/9, d1=0.599, d2=0.514 g=1.230\n",
      ">272, 9/9, d1=0.649, d2=0.666 g=1.140\n",
      ">273, 1/9, d1=0.687, d2=0.655 g=0.985\n",
      ">273, 2/9, d1=0.617, d2=0.804 g=0.825\n",
      ">273, 3/9, d1=0.584, d2=0.797 g=0.844\n",
      ">273, 4/9, d1=0.586, d2=0.798 g=1.059\n",
      ">273, 5/9, d1=0.667, d2=0.498 g=1.298\n",
      ">273, 6/9, d1=0.780, d2=0.443 g=1.506\n",
      ">273, 7/9, d1=0.737, d2=0.436 g=1.342\n",
      ">273, 8/9, d1=0.816, d2=0.590 g=1.260\n",
      ">273, 9/9, d1=0.773, d2=0.674 g=1.071\n",
      ">274, 1/9, d1=0.834, d2=0.625 g=1.061\n",
      ">274, 2/9, d1=0.791, d2=0.678 g=0.987\n",
      ">274, 3/9, d1=0.768, d2=0.576 g=0.999\n",
      ">274, 4/9, d1=0.773, d2=0.543 g=0.951\n",
      ">274, 5/9, d1=0.713, d2=0.631 g=0.971\n",
      ">274, 6/9, d1=0.733, d2=0.595 g=1.022\n",
      ">274, 7/9, d1=0.698, d2=0.555 g=1.051\n",
      ">274, 8/9, d1=0.674, d2=0.592 g=1.048\n",
      ">274, 9/9, d1=0.800, d2=0.571 g=1.026\n",
      ">275, 1/9, d1=0.762, d2=0.593 g=0.984\n",
      ">275, 2/9, d1=0.709, d2=0.585 g=0.959\n",
      ">275, 3/9, d1=0.781, d2=0.628 g=0.952\n",
      ">275, 4/9, d1=0.789, d2=0.611 g=0.966\n",
      ">275, 5/9, d1=0.743, d2=0.580 g=0.940\n",
      ">275, 6/9, d1=0.764, d2=0.577 g=1.028\n",
      ">275, 7/9, d1=0.753, d2=0.506 g=1.054\n",
      ">275, 8/9, d1=0.719, d2=0.445 g=1.076\n",
      ">275, 9/9, d1=0.730, d2=0.536 g=1.075\n",
      ">276, 1/9, d1=0.751, d2=0.576 g=1.051\n",
      ">276, 2/9, d1=0.711, d2=0.586 g=0.909\n",
      ">276, 3/9, d1=0.720, d2=0.653 g=0.866\n",
      ">276, 4/9, d1=0.681, d2=0.665 g=0.868\n",
      ">276, 5/9, d1=0.709, d2=0.632 g=0.850\n",
      ">276, 6/9, d1=0.689, d2=0.602 g=0.851\n",
      ">276, 7/9, d1=0.737, d2=0.606 g=0.866\n",
      ">276, 8/9, d1=0.703, d2=0.621 g=0.861\n",
      ">276, 9/9, d1=0.698, d2=0.633 g=0.876\n",
      ">277, 1/9, d1=0.709, d2=0.652 g=0.865\n",
      ">277, 2/9, d1=0.641, d2=0.600 g=0.861\n",
      ">277, 3/9, d1=0.662, d2=0.623 g=0.880\n",
      ">277, 4/9, d1=0.654, d2=0.581 g=0.918\n",
      ">277, 5/9, d1=0.676, d2=0.580 g=0.962\n",
      ">277, 6/9, d1=0.712, d2=0.575 g=0.908\n",
      ">277, 7/9, d1=0.654, d2=0.609 g=0.913\n",
      ">277, 8/9, d1=0.655, d2=0.580 g=0.934\n",
      ">277, 9/9, d1=0.611, d2=0.591 g=0.944\n",
      ">278, 1/9, d1=0.603, d2=0.568 g=0.930\n",
      ">278, 2/9, d1=0.652, d2=0.557 g=0.911\n",
      ">278, 3/9, d1=0.597, d2=0.614 g=0.926\n",
      ">278, 4/9, d1=0.665, d2=0.597 g=0.916\n",
      ">278, 5/9, d1=0.676, d2=0.579 g=0.919\n",
      ">278, 6/9, d1=0.633, d2=0.607 g=0.899\n",
      ">278, 7/9, d1=0.627, d2=0.663 g=0.905\n",
      ">278, 8/9, d1=0.637, d2=0.635 g=0.893\n",
      ">278, 9/9, d1=0.613, d2=0.626 g=0.870\n",
      ">279, 1/9, d1=0.616, d2=0.626 g=0.842\n",
      ">279, 2/9, d1=0.641, d2=0.631 g=0.863\n",
      ">279, 3/9, d1=0.598, d2=0.597 g=0.871\n",
      ">279, 4/9, d1=0.638, d2=0.638 g=0.885\n",
      ">279, 5/9, d1=0.654, d2=0.619 g=0.905\n",
      ">279, 6/9, d1=0.628, d2=0.602 g=0.911\n",
      ">279, 7/9, d1=0.593, d2=0.583 g=0.893\n",
      ">279, 8/9, d1=0.628, d2=0.609 g=0.916\n",
      ">279, 9/9, d1=0.610, d2=0.586 g=0.923\n",
      ">280, 1/9, d1=0.611, d2=0.594 g=0.953\n",
      ">280, 2/9, d1=0.627, d2=0.630 g=0.933\n",
      ">280, 3/9, d1=0.590, d2=0.611 g=0.915\n",
      ">280, 4/9, d1=0.574, d2=0.626 g=0.940\n",
      ">280, 5/9, d1=0.624, d2=0.625 g=0.916\n",
      ">280, 6/9, d1=0.613, d2=0.652 g=0.925\n",
      ">280, 7/9, d1=0.669, d2=0.631 g=0.907\n",
      ">280, 8/9, d1=0.593, d2=0.639 g=0.918\n",
      ">280, 9/9, d1=0.618, d2=0.613 g=0.905\n",
      ">Accuracy real: 61%, fake: 85%\n",
      ">281, 1/9, d1=0.586, d2=0.606 g=0.916\n",
      ">281, 2/9, d1=0.637, d2=0.566 g=0.982\n",
      ">281, 3/9, d1=0.651, d2=0.579 g=1.049\n",
      ">281, 4/9, d1=0.619, d2=0.496 g=1.017\n",
      ">281, 5/9, d1=0.719, d2=0.603 g=1.033\n",
      ">281, 6/9, d1=0.645, d2=0.519 g=1.037\n",
      ">281, 7/9, d1=0.667, d2=0.525 g=1.048\n",
      ">281, 8/9, d1=0.652, d2=0.539 g=1.037\n",
      ">281, 9/9, d1=0.668, d2=0.640 g=0.992\n",
      ">282, 1/9, d1=0.683, d2=0.612 g=0.896\n",
      ">282, 2/9, d1=0.623, d2=0.683 g=0.860\n",
      ">282, 3/9, d1=0.643, d2=0.683 g=0.901\n",
      ">282, 4/9, d1=0.664, d2=0.627 g=0.878\n",
      ">282, 5/9, d1=0.642, d2=0.679 g=0.902\n",
      ">282, 6/9, d1=0.690, d2=0.653 g=0.871\n",
      ">282, 7/9, d1=0.677, d2=0.630 g=0.944\n",
      ">282, 8/9, d1=0.694, d2=0.651 g=0.886\n",
      ">282, 9/9, d1=0.676, d2=0.625 g=0.891\n",
      ">283, 1/9, d1=0.631, d2=0.680 g=0.877\n",
      ">283, 2/9, d1=0.636, d2=0.684 g=0.906\n",
      ">283, 3/9, d1=0.617, d2=0.623 g=0.949\n",
      ">283, 4/9, d1=0.619, d2=0.537 g=1.006\n",
      ">283, 5/9, d1=0.654, d2=0.513 g=1.029\n",
      ">283, 6/9, d1=0.667, d2=0.537 g=0.971\n",
      ">283, 7/9, d1=0.579, d2=0.564 g=0.935\n",
      ">283, 8/9, d1=0.597, d2=0.640 g=0.888\n",
      ">283, 9/9, d1=0.559, d2=0.687 g=0.852\n",
      ">284, 1/9, d1=0.566, d2=0.704 g=0.801\n",
      ">284, 2/9, d1=0.623, d2=0.699 g=0.787\n",
      ">284, 3/9, d1=0.578, d2=0.772 g=0.773\n",
      ">284, 4/9, d1=0.583, d2=0.713 g=0.790\n",
      ">284, 5/9, d1=0.550, d2=0.695 g=0.825\n",
      ">284, 6/9, d1=0.593, d2=0.728 g=0.816\n",
      ">284, 7/9, d1=0.563, d2=0.690 g=0.861\n",
      ">284, 8/9, d1=0.572, d2=0.672 g=0.862\n",
      ">284, 9/9, d1=0.603, d2=0.672 g=0.877\n",
      ">285, 1/9, d1=0.593, d2=0.648 g=0.891\n",
      ">285, 2/9, d1=0.628, d2=0.621 g=0.865\n",
      ">285, 3/9, d1=0.602, d2=0.681 g=0.943\n",
      ">285, 4/9, d1=0.635, d2=0.600 g=0.916\n",
      ">285, 5/9, d1=0.647, d2=0.634 g=0.957\n",
      ">285, 6/9, d1=0.615, d2=0.619 g=1.015\n",
      ">285, 7/9, d1=0.633, d2=0.591 g=0.964\n",
      ">285, 8/9, d1=0.677, d2=0.602 g=0.929\n",
      ">285, 9/9, d1=0.705, d2=0.624 g=0.815\n",
      ">286, 1/9, d1=0.644, d2=0.712 g=0.747\n",
      ">286, 2/9, d1=0.699, d2=0.737 g=0.805\n",
      ">286, 3/9, d1=0.602, d2=0.733 g=0.807\n",
      ">286, 4/9, d1=0.586, d2=0.738 g=0.842\n",
      ">286, 5/9, d1=0.674, d2=0.664 g=0.876\n",
      ">286, 6/9, d1=0.647, d2=0.641 g=0.877\n",
      ">286, 7/9, d1=0.756, d2=0.705 g=0.861\n",
      ">286, 8/9, d1=0.654, d2=0.684 g=0.899\n",
      ">286, 9/9, d1=0.730, d2=0.664 g=0.889\n",
      ">287, 1/9, d1=0.687, d2=0.684 g=0.914\n",
      ">287, 2/9, d1=0.735, d2=0.601 g=0.900\n",
      ">287, 3/9, d1=0.766, d2=0.664 g=0.902\n",
      ">287, 4/9, d1=0.717, d2=0.660 g=0.878\n",
      ">287, 5/9, d1=0.706, d2=0.642 g=0.913\n",
      ">287, 6/9, d1=0.740, d2=0.622 g=0.889\n",
      ">287, 7/9, d1=0.736, d2=0.654 g=0.909\n",
      ">287, 8/9, d1=0.714, d2=0.595 g=0.930\n",
      ">287, 9/9, d1=0.711, d2=0.608 g=0.968\n",
      ">288, 1/9, d1=0.756, d2=0.531 g=0.999\n",
      ">288, 2/9, d1=0.699, d2=0.539 g=1.007\n",
      ">288, 3/9, d1=0.711, d2=0.578 g=0.968\n",
      ">288, 4/9, d1=0.670, d2=0.607 g=0.915\n",
      ">288, 5/9, d1=0.658, d2=0.620 g=0.887\n",
      ">288, 6/9, d1=0.692, d2=0.646 g=0.870\n",
      ">288, 7/9, d1=0.661, d2=0.662 g=0.847\n",
      ">288, 8/9, d1=0.729, d2=0.626 g=0.862\n",
      ">288, 9/9, d1=0.690, d2=0.692 g=0.866\n",
      ">289, 1/9, d1=0.698, d2=0.609 g=0.891\n",
      ">289, 2/9, d1=0.711, d2=0.644 g=0.878\n",
      ">289, 3/9, d1=0.678, d2=0.644 g=0.887\n",
      ">289, 4/9, d1=0.657, d2=0.609 g=0.854\n",
      ">289, 5/9, d1=0.678, d2=0.643 g=0.828\n",
      ">289, 6/9, d1=0.659, d2=0.634 g=0.856\n",
      ">289, 7/9, d1=0.673, d2=0.638 g=0.864\n",
      ">289, 8/9, d1=0.691, d2=0.678 g=0.896\n",
      ">289, 9/9, d1=0.692, d2=0.629 g=0.892\n",
      ">290, 1/9, d1=0.651, d2=0.601 g=0.877\n",
      ">290, 2/9, d1=0.651, d2=0.633 g=0.850\n",
      ">290, 3/9, d1=0.663, d2=0.645 g=0.848\n",
      ">290, 4/9, d1=0.603, d2=0.653 g=0.802\n",
      ">290, 5/9, d1=0.593, d2=0.671 g=0.813\n",
      ">290, 6/9, d1=0.567, d2=0.664 g=0.826\n",
      ">290, 7/9, d1=0.591, d2=0.641 g=0.830\n",
      ">290, 8/9, d1=0.640, d2=0.640 g=0.858\n",
      ">290, 9/9, d1=0.584, d2=0.637 g=0.861\n",
      ">Accuracy real: 65%, fake: 65%\n",
      ">291, 1/9, d1=0.631, d2=0.673 g=0.860\n",
      ">291, 2/9, d1=0.535, d2=0.681 g=0.857\n",
      ">291, 3/9, d1=0.652, d2=0.658 g=0.842\n",
      ">291, 4/9, d1=0.610, d2=0.666 g=0.824\n",
      ">291, 5/9, d1=0.686, d2=0.695 g=0.888\n",
      ">291, 6/9, d1=0.653, d2=0.661 g=0.880\n",
      ">291, 7/9, d1=0.735, d2=0.655 g=0.867\n",
      ">291, 8/9, d1=0.666, d2=0.651 g=0.907\n",
      ">291, 9/9, d1=0.670, d2=0.650 g=0.879\n",
      ">292, 1/9, d1=0.743, d2=0.615 g=0.891\n",
      ">292, 2/9, d1=0.772, d2=0.661 g=0.949\n",
      ">292, 3/9, d1=0.768, d2=0.593 g=0.971\n",
      ">292, 4/9, d1=0.800, d2=0.570 g=0.985\n",
      ">292, 5/9, d1=0.859, d2=0.584 g=0.996\n",
      ">292, 6/9, d1=0.804, d2=0.593 g=0.938\n",
      ">292, 7/9, d1=0.741, d2=0.579 g=0.918\n",
      ">292, 8/9, d1=0.806, d2=0.596 g=0.921\n",
      ">292, 9/9, d1=0.650, d2=0.594 g=0.908\n",
      ">293, 1/9, d1=0.683, d2=0.592 g=0.953\n",
      ">293, 2/9, d1=0.688, d2=0.573 g=0.924\n",
      ">293, 3/9, d1=0.684, d2=0.665 g=0.878\n",
      ">293, 4/9, d1=0.657, d2=0.634 g=0.923\n",
      ">293, 5/9, d1=0.730, d2=0.705 g=0.885\n",
      ">293, 6/9, d1=0.729, d2=0.680 g=0.924\n",
      ">293, 7/9, d1=0.638, d2=0.651 g=0.881\n",
      ">293, 8/9, d1=0.735, d2=0.689 g=0.879\n",
      ">293, 9/9, d1=0.675, d2=0.602 g=0.935\n",
      ">294, 1/9, d1=0.688, d2=0.552 g=0.950\n",
      ">294, 2/9, d1=0.635, d2=0.538 g=0.981\n",
      ">294, 3/9, d1=0.620, d2=0.548 g=0.971\n",
      ">294, 4/9, d1=0.585, d2=0.521 g=1.038\n",
      ">294, 5/9, d1=0.605, d2=0.580 g=0.970\n",
      ">294, 6/9, d1=0.569, d2=0.654 g=0.955\n",
      ">294, 7/9, d1=0.626, d2=0.645 g=0.887\n",
      ">294, 8/9, d1=0.598, d2=0.741 g=0.847\n",
      ">294, 9/9, d1=0.583, d2=0.734 g=0.832\n",
      ">295, 1/9, d1=0.611, d2=0.780 g=0.789\n",
      ">295, 2/9, d1=0.596, d2=0.783 g=0.877\n",
      ">295, 3/9, d1=0.602, d2=0.658 g=0.920\n",
      ">295, 4/9, d1=0.611, d2=0.631 g=1.108\n",
      ">295, 5/9, d1=0.648, d2=0.577 g=1.117\n",
      ">295, 6/9, d1=0.661, d2=0.590 g=1.106\n",
      ">295, 7/9, d1=0.706, d2=0.628 g=0.971\n",
      ">295, 8/9, d1=0.665, d2=0.721 g=0.842\n",
      ">295, 9/9, d1=0.743, d2=0.742 g=0.776\n",
      ">296, 1/9, d1=0.706, d2=0.753 g=0.844\n",
      ">296, 2/9, d1=0.709, d2=0.671 g=0.893\n",
      ">296, 3/9, d1=0.695, d2=0.587 g=0.954\n",
      ">296, 4/9, d1=0.711, d2=0.576 g=0.983\n",
      ">296, 5/9, d1=0.661, d2=0.566 g=0.971\n",
      ">296, 6/9, d1=0.675, d2=0.599 g=0.999\n",
      ">296, 7/9, d1=0.650, d2=0.551 g=0.975\n",
      ">296, 8/9, d1=0.658, d2=0.593 g=0.906\n",
      ">296, 9/9, d1=0.607, d2=0.665 g=0.889\n",
      ">297, 1/9, d1=0.571, d2=0.630 g=0.850\n",
      ">297, 2/9, d1=0.638, d2=0.639 g=0.850\n",
      ">297, 3/9, d1=0.605, d2=0.661 g=0.844\n",
      ">297, 4/9, d1=0.625, d2=0.637 g=0.839\n",
      ">297, 5/9, d1=0.569, d2=0.640 g=0.827\n",
      ">297, 6/9, d1=0.574, d2=0.654 g=0.843\n",
      ">297, 7/9, d1=0.534, d2=0.650 g=0.810\n",
      ">297, 8/9, d1=0.599, d2=0.686 g=0.819\n",
      ">297, 9/9, d1=0.618, d2=0.715 g=0.806\n",
      ">298, 1/9, d1=0.665, d2=0.722 g=0.862\n",
      ">298, 2/9, d1=0.604, d2=0.683 g=0.886\n",
      ">298, 3/9, d1=0.685, d2=0.629 g=0.890\n",
      ">298, 4/9, d1=0.692, d2=0.636 g=0.925\n",
      ">298, 5/9, d1=0.694, d2=0.636 g=1.040\n",
      ">298, 6/9, d1=0.742, d2=0.543 g=1.024\n",
      ">298, 7/9, d1=0.697, d2=0.574 g=1.007\n",
      ">298, 8/9, d1=0.692, d2=0.579 g=0.977\n",
      ">298, 9/9, d1=0.727, d2=0.669 g=0.906\n",
      ">299, 1/9, d1=0.699, d2=0.654 g=0.891\n",
      ">299, 2/9, d1=0.734, d2=0.703 g=0.850\n",
      ">299, 3/9, d1=0.741, d2=0.654 g=0.897\n",
      ">299, 4/9, d1=0.722, d2=0.589 g=0.956\n",
      ">299, 5/9, d1=0.739, d2=0.595 g=0.950\n",
      ">299, 6/9, d1=0.638, d2=0.534 g=0.991\n",
      ">299, 7/9, d1=0.701, d2=0.583 g=0.963\n",
      ">299, 8/9, d1=0.707, d2=0.572 g=0.965\n",
      ">299, 9/9, d1=0.682, d2=0.594 g=0.914\n",
      ">300, 1/9, d1=0.586, d2=0.577 g=0.917\n",
      ">300, 2/9, d1=0.632, d2=0.584 g=0.884\n",
      ">300, 3/9, d1=0.713, d2=0.621 g=0.854\n",
      ">300, 4/9, d1=0.628, d2=0.634 g=0.857\n",
      ">300, 5/9, d1=0.663, d2=0.724 g=0.794\n",
      ">300, 6/9, d1=0.644, d2=0.681 g=0.827\n",
      ">300, 7/9, d1=0.669, d2=0.684 g=0.835\n",
      ">300, 8/9, d1=0.639, d2=0.654 g=0.866\n",
      ">300, 9/9, d1=0.632, d2=0.648 g=0.909\n",
      ">Accuracy real: 59%, fake: 80%\n",
      ">301, 1/9, d1=0.608, d2=0.562 g=0.928\n",
      ">301, 2/9, d1=0.649, d2=0.589 g=0.911\n",
      ">301, 3/9, d1=0.710, d2=0.598 g=0.886\n",
      ">301, 4/9, d1=0.679, d2=0.646 g=0.876\n",
      ">301, 5/9, d1=0.700, d2=0.657 g=0.821\n",
      ">301, 6/9, d1=0.668, d2=0.688 g=0.808\n",
      ">301, 7/9, d1=0.691, d2=0.682 g=0.781\n",
      ">301, 8/9, d1=0.643, d2=0.692 g=0.842\n",
      ">301, 9/9, d1=0.702, d2=0.654 g=0.855\n",
      ">302, 1/9, d1=0.704, d2=0.628 g=0.882\n",
      ">302, 2/9, d1=0.677, d2=0.603 g=0.911\n",
      ">302, 3/9, d1=0.637, d2=0.603 g=0.904\n",
      ">302, 4/9, d1=0.704, d2=0.626 g=0.897\n",
      ">302, 5/9, d1=0.644, d2=0.596 g=0.878\n",
      ">302, 6/9, d1=0.627, d2=0.610 g=0.857\n",
      ">302, 7/9, d1=0.570, d2=0.620 g=0.868\n",
      ">302, 8/9, d1=0.646, d2=0.652 g=0.848\n",
      ">302, 9/9, d1=0.608, d2=0.665 g=0.834\n",
      ">303, 1/9, d1=0.636, d2=0.698 g=0.789\n",
      ">303, 2/9, d1=0.554, d2=0.718 g=0.778\n",
      ">303, 3/9, d1=0.626, d2=0.734 g=0.807\n",
      ">303, 4/9, d1=0.664, d2=0.726 g=0.851\n",
      ">303, 5/9, d1=0.621, d2=0.650 g=0.961\n",
      ">303, 6/9, d1=0.680, d2=0.538 g=0.972\n",
      ">303, 7/9, d1=0.693, d2=0.579 g=1.020\n",
      ">303, 8/9, d1=0.673, d2=0.547 g=1.002\n",
      ">303, 9/9, d1=0.586, d2=0.595 g=0.980\n",
      ">304, 1/9, d1=0.586, d2=0.614 g=0.936\n",
      ">304, 2/9, d1=0.614, d2=0.628 g=0.879\n",
      ">304, 3/9, d1=0.614, d2=0.699 g=0.837\n",
      ">304, 4/9, d1=0.572, d2=0.703 g=0.808\n",
      ">304, 5/9, d1=0.634, d2=0.677 g=0.848\n",
      ">304, 6/9, d1=0.557, d2=0.633 g=0.864\n",
      ">304, 7/9, d1=0.580, d2=0.641 g=0.870\n",
      ">304, 8/9, d1=0.606, d2=0.631 g=0.887\n",
      ">304, 9/9, d1=0.560, d2=0.636 g=0.915\n",
      ">305, 1/9, d1=0.573, d2=0.627 g=0.895\n",
      ">305, 2/9, d1=0.582, d2=0.594 g=0.882\n",
      ">305, 3/9, d1=0.549, d2=0.659 g=0.831\n",
      ">305, 4/9, d1=0.599, d2=0.694 g=0.835\n",
      ">305, 5/9, d1=0.612, d2=0.663 g=0.829\n",
      ">305, 6/9, d1=0.560, d2=0.709 g=0.805\n",
      ">305, 7/9, d1=0.581, d2=0.705 g=0.803\n",
      ">305, 8/9, d1=0.598, d2=0.768 g=0.829\n",
      ">305, 9/9, d1=0.610, d2=0.683 g=0.833\n",
      ">306, 1/9, d1=0.660, d2=0.626 g=0.954\n",
      ">306, 2/9, d1=0.590, d2=0.552 g=1.081\n",
      ">306, 3/9, d1=0.603, d2=0.604 g=1.139\n",
      ">306, 4/9, d1=0.652, d2=0.578 g=1.063\n",
      ">306, 5/9, d1=0.712, d2=0.591 g=0.935\n",
      ">306, 6/9, d1=0.647, d2=0.750 g=0.862\n",
      ">306, 7/9, d1=0.639, d2=0.786 g=0.766\n",
      ">306, 8/9, d1=0.722, d2=0.773 g=0.838\n",
      ">306, 9/9, d1=0.726, d2=0.693 g=0.940\n",
      ">307, 1/9, d1=0.705, d2=0.579 g=1.068\n",
      ">307, 2/9, d1=0.787, d2=0.467 g=1.183\n",
      ">307, 3/9, d1=0.675, d2=0.492 g=1.239\n",
      ">307, 4/9, d1=0.707, d2=0.416 g=1.213\n",
      ">307, 5/9, d1=0.685, d2=0.479 g=1.091\n",
      ">307, 6/9, d1=0.728, d2=0.558 g=0.994\n",
      ">307, 7/9, d1=0.709, d2=0.716 g=0.812\n",
      ">307, 8/9, d1=0.647, d2=0.776 g=0.733\n",
      ">307, 9/9, d1=0.691, d2=0.764 g=0.744\n",
      ">308, 1/9, d1=0.639, d2=0.732 g=0.812\n",
      ">308, 2/9, d1=0.642, d2=0.707 g=0.838\n",
      ">308, 3/9, d1=0.640, d2=0.633 g=0.881\n",
      ">308, 4/9, d1=0.693, d2=0.643 g=0.870\n",
      ">308, 5/9, d1=0.645, d2=0.648 g=0.896\n",
      ">308, 6/9, d1=0.638, d2=0.663 g=0.933\n",
      ">308, 7/9, d1=0.737, d2=0.644 g=0.953\n",
      ">308, 8/9, d1=0.671, d2=0.599 g=0.968\n",
      ">308, 9/9, d1=0.690, d2=0.600 g=0.964\n",
      ">309, 1/9, d1=0.655, d2=0.636 g=0.919\n",
      ">309, 2/9, d1=0.698, d2=0.665 g=0.914\n",
      ">309, 3/9, d1=0.726, d2=0.801 g=0.807\n",
      ">309, 4/9, d1=0.691, d2=0.860 g=0.799\n",
      ">309, 5/9, d1=0.762, d2=0.725 g=0.838\n",
      ">309, 6/9, d1=0.771, d2=0.668 g=0.895\n",
      ">309, 7/9, d1=0.751, d2=0.609 g=0.950\n",
      ">309, 8/9, d1=0.744, d2=0.583 g=1.042\n",
      ">309, 9/9, d1=0.728, d2=0.497 g=1.130\n",
      ">310, 1/9, d1=0.730, d2=0.521 g=1.096\n",
      ">310, 2/9, d1=0.687, d2=0.535 g=1.006\n",
      ">310, 3/9, d1=0.698, d2=0.578 g=0.940\n",
      ">310, 4/9, d1=0.602, d2=0.561 g=0.935\n",
      ">310, 5/9, d1=0.678, d2=0.637 g=0.935\n",
      ">310, 6/9, d1=0.654, d2=0.626 g=0.882\n",
      ">310, 7/9, d1=0.637, d2=0.633 g=0.828\n",
      ">310, 8/9, d1=0.742, d2=0.710 g=0.810\n",
      ">310, 9/9, d1=0.649, d2=0.724 g=0.812\n",
      ">Accuracy real: 55%, fake: 50%\n",
      ">311, 1/9, d1=0.675, d2=0.721 g=0.795\n",
      ">311, 2/9, d1=0.688, d2=0.682 g=0.875\n",
      ">311, 3/9, d1=0.707, d2=0.620 g=0.895\n",
      ">311, 4/9, d1=0.694, d2=0.601 g=0.909\n",
      ">311, 5/9, d1=0.646, d2=0.609 g=0.924\n",
      ">311, 6/9, d1=0.634, d2=0.568 g=0.948\n",
      ">311, 7/9, d1=0.644, d2=0.583 g=0.970\n",
      ">311, 8/9, d1=0.603, d2=0.557 g=0.957\n",
      ">311, 9/9, d1=0.576, d2=0.581 g=0.920\n",
      ">312, 1/9, d1=0.597, d2=0.587 g=0.907\n",
      ">312, 2/9, d1=0.596, d2=0.616 g=0.915\n",
      ">312, 3/9, d1=0.602, d2=0.596 g=0.948\n",
      ">312, 4/9, d1=0.508, d2=0.553 g=1.024\n",
      ">312, 5/9, d1=0.615, d2=0.525 g=1.056\n",
      ">312, 6/9, d1=0.609, d2=0.513 g=1.072\n",
      ">312, 7/9, d1=0.539, d2=0.623 g=1.047\n",
      ">312, 8/9, d1=0.588, d2=0.638 g=0.962\n",
      ">312, 9/9, d1=0.562, d2=0.641 g=0.939\n",
      ">313, 1/9, d1=0.587, d2=0.734 g=0.852\n",
      ">313, 2/9, d1=0.572, d2=0.761 g=0.791\n",
      ">313, 3/9, d1=0.621, d2=0.736 g=0.789\n",
      ">313, 4/9, d1=0.672, d2=0.726 g=0.906\n",
      ">313, 5/9, d1=0.670, d2=0.609 g=1.122\n",
      ">313, 6/9, d1=0.673, d2=0.494 g=1.197\n",
      ">313, 7/9, d1=0.713, d2=0.475 g=1.209\n",
      ">313, 8/9, d1=0.690, d2=0.531 g=1.047\n",
      ">313, 9/9, d1=0.626, d2=0.641 g=0.927\n",
      ">314, 1/9, d1=0.701, d2=0.711 g=0.814\n",
      ">314, 2/9, d1=0.753, d2=0.720 g=0.751\n",
      ">314, 3/9, d1=0.642, d2=0.740 g=0.767\n",
      ">314, 4/9, d1=0.614, d2=0.690 g=0.821\n",
      ">314, 5/9, d1=0.707, d2=0.675 g=0.837\n",
      ">314, 6/9, d1=0.623, d2=0.640 g=0.848\n",
      ">314, 7/9, d1=0.626, d2=0.634 g=0.867\n",
      ">314, 8/9, d1=0.644, d2=0.622 g=0.916\n",
      ">314, 9/9, d1=0.686, d2=0.609 g=0.856\n",
      ">315, 1/9, d1=0.609, d2=0.658 g=0.859\n",
      ">315, 2/9, d1=0.625, d2=0.651 g=0.831\n",
      ">315, 3/9, d1=0.610, d2=0.692 g=0.807\n",
      ">315, 4/9, d1=0.550, d2=0.681 g=0.826\n",
      ">315, 5/9, d1=0.650, d2=0.654 g=0.832\n",
      ">315, 6/9, d1=0.666, d2=0.697 g=0.806\n",
      ">315, 7/9, d1=0.634, d2=0.666 g=0.836\n",
      ">315, 8/9, d1=0.633, d2=0.627 g=0.877\n",
      ">315, 9/9, d1=0.647, d2=0.601 g=0.896\n",
      ">316, 1/9, d1=0.629, d2=0.625 g=0.934\n",
      ">316, 2/9, d1=0.584, d2=0.612 g=0.960\n",
      ">316, 3/9, d1=0.638, d2=0.598 g=0.937\n",
      ">316, 4/9, d1=0.709, d2=0.609 g=0.895\n",
      ">316, 5/9, d1=0.590, d2=0.662 g=0.874\n",
      ">316, 6/9, d1=0.670, d2=0.702 g=0.833\n",
      ">316, 7/9, d1=0.617, d2=0.639 g=0.848\n",
      ">316, 8/9, d1=0.641, d2=0.691 g=0.849\n",
      ">316, 9/9, d1=0.638, d2=0.640 g=0.911\n",
      ">317, 1/9, d1=0.605, d2=0.668 g=1.033\n",
      ">317, 2/9, d1=0.691, d2=0.513 g=1.119\n",
      ">317, 3/9, d1=0.660, d2=0.451 g=1.161\n",
      ">317, 4/9, d1=0.613, d2=0.452 g=1.185\n",
      ">317, 5/9, d1=0.649, d2=0.473 g=1.100\n",
      ">317, 6/9, d1=0.641, d2=0.549 g=1.016\n",
      ">317, 7/9, d1=0.604, d2=0.561 g=1.069\n",
      ">317, 8/9, d1=0.698, d2=0.596 g=0.993\n",
      ">317, 9/9, d1=0.681, d2=0.543 g=0.952\n",
      ">318, 1/9, d1=0.700, d2=0.614 g=0.982\n",
      ">318, 2/9, d1=0.675, d2=0.555 g=1.024\n",
      ">318, 3/9, d1=0.658, d2=0.520 g=1.066\n",
      ">318, 4/9, d1=0.665, d2=0.534 g=1.024\n",
      ">318, 5/9, d1=0.714, d2=0.548 g=0.910\n",
      ">318, 6/9, d1=0.634, d2=0.649 g=0.894\n",
      ">318, 7/9, d1=0.671, d2=0.660 g=0.872\n",
      ">318, 8/9, d1=0.684, d2=0.649 g=0.805\n",
      ">318, 9/9, d1=0.642, d2=0.717 g=0.866\n",
      ">319, 1/9, d1=0.655, d2=0.637 g=0.838\n",
      ">319, 2/9, d1=0.636, d2=0.653 g=0.843\n",
      ">319, 3/9, d1=0.653, d2=0.646 g=0.888\n",
      ">319, 4/9, d1=0.651, d2=0.578 g=0.923\n",
      ">319, 5/9, d1=0.617, d2=0.629 g=0.920\n",
      ">319, 6/9, d1=0.647, d2=0.642 g=0.943\n",
      ">319, 7/9, d1=0.615, d2=0.616 g=0.928\n",
      ">319, 8/9, d1=0.606, d2=0.604 g=0.982\n",
      ">319, 9/9, d1=0.538, d2=0.595 g=0.985\n",
      ">320, 1/9, d1=0.597, d2=0.566 g=0.943\n",
      ">320, 2/9, d1=0.587, d2=0.578 g=1.006\n",
      ">320, 3/9, d1=0.581, d2=0.586 g=0.952\n",
      ">320, 4/9, d1=0.550, d2=0.640 g=0.952\n",
      ">320, 5/9, d1=0.576, d2=0.653 g=0.967\n",
      ">320, 6/9, d1=0.593, d2=0.697 g=0.980\n",
      ">320, 7/9, d1=0.610, d2=0.705 g=0.989\n",
      ">320, 8/9, d1=0.618, d2=0.676 g=0.955\n",
      ">320, 9/9, d1=0.647, d2=0.607 g=0.981\n",
      ">Accuracy real: 45%, fake: 75%\n",
      ">321, 1/9, d1=0.621, d2=0.612 g=0.996\n",
      ">321, 2/9, d1=0.740, d2=0.623 g=1.049\n",
      ">321, 3/9, d1=0.722, d2=0.585 g=1.062\n",
      ">321, 4/9, d1=0.694, d2=0.552 g=1.107\n",
      ">321, 5/9, d1=0.635, d2=0.527 g=1.073\n",
      ">321, 6/9, d1=0.618, d2=0.528 g=1.071\n",
      ">321, 7/9, d1=0.626, d2=0.523 g=1.107\n",
      ">321, 8/9, d1=0.601, d2=0.524 g=1.067\n",
      ">321, 9/9, d1=0.599, d2=0.567 g=0.971\n",
      ">322, 1/9, d1=0.524, d2=0.630 g=0.894\n",
      ">322, 2/9, d1=0.537, d2=0.709 g=0.831\n",
      ">322, 3/9, d1=0.514, d2=0.767 g=0.809\n",
      ">322, 4/9, d1=0.499, d2=0.762 g=0.762\n",
      ">322, 5/9, d1=0.456, d2=0.768 g=0.792\n",
      ">322, 6/9, d1=0.515, d2=0.665 g=0.812\n",
      ">322, 7/9, d1=0.515, d2=0.664 g=0.808\n",
      ">322, 8/9, d1=0.546, d2=0.726 g=0.844\n",
      ">322, 9/9, d1=0.565, d2=0.684 g=0.887\n",
      ">323, 1/9, d1=0.555, d2=0.664 g=0.899\n",
      ">323, 2/9, d1=0.566, d2=0.665 g=0.884\n",
      ">323, 3/9, d1=0.668, d2=0.666 g=0.886\n",
      ">323, 4/9, d1=0.719, d2=0.731 g=0.916\n",
      ">323, 5/9, d1=0.741, d2=0.642 g=0.896\n",
      ">323, 6/9, d1=0.738, d2=0.622 g=0.961\n",
      ">323, 7/9, d1=0.763, d2=0.663 g=1.000\n",
      ">323, 8/9, d1=0.752, d2=0.564 g=1.052\n",
      ">323, 9/9, d1=0.766, d2=0.507 g=1.140\n",
      ">324, 1/9, d1=0.724, d2=0.519 g=1.055\n",
      ">324, 2/9, d1=0.706, d2=0.580 g=1.120\n",
      ">324, 3/9, d1=0.698, d2=0.617 g=1.065\n",
      ">324, 4/9, d1=0.729, d2=0.567 g=1.068\n",
      ">324, 5/9, d1=0.760, d2=0.527 g=1.001\n",
      ">324, 6/9, d1=0.715, d2=0.554 g=0.990\n",
      ">324, 7/9, d1=0.680, d2=0.578 g=0.964\n",
      ">324, 8/9, d1=0.707, d2=0.647 g=0.932\n",
      ">324, 9/9, d1=0.640, d2=0.627 g=0.883\n",
      ">325, 1/9, d1=0.596, d2=0.639 g=0.923\n",
      ">325, 2/9, d1=0.665, d2=0.626 g=0.876\n",
      ">325, 3/9, d1=0.605, d2=0.671 g=0.862\n",
      ">325, 4/9, d1=0.691, d2=0.681 g=0.767\n",
      ">325, 5/9, d1=0.660, d2=0.668 g=0.840\n",
      ">325, 6/9, d1=0.633, d2=0.658 g=0.836\n",
      ">325, 7/9, d1=0.707, d2=0.625 g=0.873\n",
      ">325, 8/9, d1=0.694, d2=0.618 g=0.921\n",
      ">325, 9/9, d1=0.674, d2=0.583 g=0.941\n",
      ">326, 1/9, d1=0.616, d2=0.581 g=0.979\n",
      ">326, 2/9, d1=0.665, d2=0.582 g=0.952\n",
      ">326, 3/9, d1=0.638, d2=0.634 g=0.866\n",
      ">326, 4/9, d1=0.636, d2=0.689 g=0.799\n",
      ">326, 5/9, d1=0.649, d2=0.792 g=0.749\n",
      ">326, 6/9, d1=0.573, d2=0.865 g=0.817\n",
      ">326, 7/9, d1=0.635, d2=0.726 g=0.883\n",
      ">326, 8/9, d1=0.641, d2=0.626 g=0.999\n",
      ">326, 9/9, d1=0.662, d2=0.570 g=1.182\n",
      ">327, 1/9, d1=0.681, d2=0.536 g=1.187\n",
      ">327, 2/9, d1=0.600, d2=0.550 g=1.101\n",
      ">327, 3/9, d1=0.647, d2=0.594 g=1.087\n",
      ">327, 4/9, d1=0.675, d2=0.630 g=1.063\n",
      ">327, 5/9, d1=0.643, d2=0.601 g=1.086\n",
      ">327, 6/9, d1=0.686, d2=0.541 g=1.223\n",
      ">327, 7/9, d1=0.704, d2=0.436 g=1.288\n",
      ">327, 8/9, d1=0.674, d2=0.452 g=1.243\n",
      ">327, 9/9, d1=0.642, d2=0.537 g=1.090\n",
      ">328, 1/9, d1=0.628, d2=0.597 g=0.966\n",
      ">328, 2/9, d1=0.628, d2=0.692 g=0.884\n",
      ">328, 3/9, d1=0.675, d2=0.727 g=0.813\n",
      ">328, 4/9, d1=0.671, d2=0.757 g=0.777\n",
      ">328, 5/9, d1=0.668, d2=0.741 g=0.780\n",
      ">328, 6/9, d1=0.635, d2=0.694 g=0.893\n",
      ">328, 7/9, d1=0.625, d2=0.741 g=0.883\n",
      ">328, 8/9, d1=0.774, d2=0.676 g=0.856\n",
      ">328, 9/9, d1=0.670, d2=0.619 g=0.913\n",
      ">329, 1/9, d1=0.745, d2=0.591 g=0.924\n",
      ">329, 2/9, d1=0.667, d2=0.599 g=0.906\n",
      ">329, 3/9, d1=0.652, d2=0.624 g=0.940\n",
      ">329, 4/9, d1=0.733, d2=0.623 g=0.892\n",
      ">329, 5/9, d1=0.634, d2=0.625 g=0.894\n",
      ">329, 6/9, d1=0.627, d2=0.614 g=0.917\n",
      ">329, 7/9, d1=0.602, d2=0.621 g=0.893\n",
      ">329, 8/9, d1=0.696, d2=0.608 g=0.956\n",
      ">329, 9/9, d1=0.680, d2=0.592 g=0.927\n",
      ">330, 1/9, d1=0.679, d2=0.590 g=0.926\n",
      ">330, 2/9, d1=0.672, d2=0.605 g=0.947\n",
      ">330, 3/9, d1=0.631, d2=0.620 g=0.973\n",
      ">330, 4/9, d1=0.645, d2=0.603 g=0.967\n",
      ">330, 5/9, d1=0.645, d2=0.588 g=0.935\n",
      ">330, 6/9, d1=0.609, d2=0.614 g=0.923\n",
      ">330, 7/9, d1=0.645, d2=0.583 g=0.914\n",
      ">330, 8/9, d1=0.669, d2=0.642 g=0.884\n",
      ">330, 9/9, d1=0.643, d2=0.616 g=0.877\n",
      ">Accuracy real: 65%, fake: 89%\n",
      ">331, 1/9, d1=0.588, d2=0.604 g=0.877\n",
      ">331, 2/9, d1=0.616, d2=0.642 g=0.851\n",
      ">331, 3/9, d1=0.679, d2=0.605 g=0.858\n",
      ">331, 4/9, d1=0.642, d2=0.658 g=0.829\n",
      ">331, 5/9, d1=0.655, d2=0.690 g=0.828\n",
      ">331, 6/9, d1=0.597, d2=0.677 g=0.847\n",
      ">331, 7/9, d1=0.617, d2=0.631 g=0.883\n",
      ">331, 8/9, d1=0.606, d2=0.595 g=0.878\n",
      ">331, 9/9, d1=0.628, d2=0.601 g=0.865\n",
      ">332, 1/9, d1=0.672, d2=0.655 g=0.860\n",
      ">332, 2/9, d1=0.645, d2=0.655 g=0.885\n",
      ">332, 3/9, d1=0.663, d2=0.597 g=0.907\n",
      ">332, 4/9, d1=0.649, d2=0.653 g=0.937\n",
      ">332, 5/9, d1=0.699, d2=0.601 g=0.910\n",
      ">332, 6/9, d1=0.666, d2=0.618 g=0.875\n",
      ">332, 7/9, d1=0.634, d2=0.634 g=0.892\n",
      ">332, 8/9, d1=0.609, d2=0.633 g=0.913\n",
      ">332, 9/9, d1=0.624, d2=0.602 g=0.905\n",
      ">333, 1/9, d1=0.688, d2=0.615 g=0.910\n",
      ">333, 2/9, d1=0.600, d2=0.617 g=0.923\n",
      ">333, 3/9, d1=0.620, d2=0.593 g=0.936\n",
      ">333, 4/9, d1=0.565, d2=0.610 g=0.942\n",
      ">333, 5/9, d1=0.571, d2=0.608 g=0.931\n",
      ">333, 6/9, d1=0.565, d2=0.593 g=0.954\n",
      ">333, 7/9, d1=0.590, d2=0.632 g=0.933\n",
      ">333, 8/9, d1=0.655, d2=0.583 g=0.919\n",
      ">333, 9/9, d1=0.647, d2=0.625 g=0.902\n",
      ">334, 1/9, d1=0.587, d2=0.593 g=0.891\n",
      ">334, 2/9, d1=0.596, d2=0.640 g=0.914\n",
      ">334, 3/9, d1=0.611, d2=0.641 g=0.898\n",
      ">334, 4/9, d1=0.584, d2=0.678 g=0.913\n",
      ">334, 5/9, d1=0.576, d2=0.711 g=0.883\n",
      ">334, 6/9, d1=0.599, d2=0.716 g=0.876\n",
      ">334, 7/9, d1=0.595, d2=0.734 g=0.859\n",
      ">334, 8/9, d1=0.585, d2=0.654 g=0.848\n",
      ">334, 9/9, d1=0.594, d2=0.656 g=0.856\n",
      ">335, 1/9, d1=0.584, d2=0.651 g=0.842\n",
      ">335, 2/9, d1=0.624, d2=0.689 g=0.924\n",
      ">335, 3/9, d1=0.637, d2=0.686 g=0.935\n",
      ">335, 4/9, d1=0.616, d2=0.700 g=0.882\n",
      ">335, 5/9, d1=0.579, d2=0.706 g=0.917\n",
      ">335, 6/9, d1=0.661, d2=0.680 g=0.993\n",
      ">335, 7/9, d1=0.661, d2=0.576 g=1.014\n",
      ">335, 8/9, d1=0.707, d2=0.583 g=1.049\n",
      ">335, 9/9, d1=0.673, d2=0.544 g=1.089\n",
      ">336, 1/9, d1=0.625, d2=0.549 g=1.116\n",
      ">336, 2/9, d1=0.663, d2=0.574 g=1.059\n",
      ">336, 3/9, d1=0.591, d2=0.668 g=1.003\n",
      ">336, 4/9, d1=0.636, d2=0.609 g=0.980\n",
      ">336, 5/9, d1=0.617, d2=0.628 g=0.975\n",
      ">336, 6/9, d1=0.610, d2=0.665 g=0.933\n",
      ">336, 7/9, d1=0.738, d2=0.629 g=0.953\n",
      ">336, 8/9, d1=0.720, d2=0.600 g=1.064\n",
      ">336, 9/9, d1=0.719, d2=0.537 g=1.061\n",
      ">337, 1/9, d1=0.754, d2=0.584 g=1.043\n",
      ">337, 2/9, d1=0.714, d2=0.613 g=0.999\n",
      ">337, 3/9, d1=0.660, d2=0.666 g=0.934\n",
      ">337, 4/9, d1=0.740, d2=0.737 g=0.875\n",
      ">337, 5/9, d1=0.762, d2=0.692 g=0.882\n",
      ">337, 6/9, d1=0.762, d2=0.652 g=0.907\n",
      ">337, 7/9, d1=0.740, d2=0.655 g=0.914\n",
      ">337, 8/9, d1=0.727, d2=0.632 g=0.896\n",
      ">337, 9/9, d1=0.664, d2=0.623 g=0.925\n",
      ">338, 1/9, d1=0.641, d2=0.578 g=0.959\n",
      ">338, 2/9, d1=0.645, d2=0.594 g=1.005\n",
      ">338, 3/9, d1=0.710, d2=0.560 g=1.018\n",
      ">338, 4/9, d1=0.647, d2=0.555 g=1.031\n",
      ">338, 5/9, d1=0.598, d2=0.481 g=1.046\n",
      ">338, 6/9, d1=0.555, d2=0.520 g=1.076\n",
      ">338, 7/9, d1=0.499, d2=0.543 g=1.008\n",
      ">338, 8/9, d1=0.562, d2=0.561 g=1.001\n",
      ">338, 9/9, d1=0.603, d2=0.558 g=0.997\n",
      ">339, 1/9, d1=0.550, d2=0.564 g=0.957\n",
      ">339, 2/9, d1=0.553, d2=0.597 g=0.916\n",
      ">339, 3/9, d1=0.515, d2=0.651 g=0.900\n",
      ">339, 4/9, d1=0.610, d2=0.682 g=0.922\n",
      ">339, 5/9, d1=0.556, d2=0.991 g=0.795\n",
      ">339, 6/9, d1=0.539, d2=0.729 g=0.909\n",
      ">339, 7/9, d1=0.687, d2=0.806 g=0.970\n",
      ">339, 8/9, d1=0.738, d2=0.559 g=1.141\n",
      ">339, 9/9, d1=0.651, d2=0.464 g=1.308\n",
      ">340, 1/9, d1=0.600, d2=0.389 g=1.383\n",
      ">340, 2/9, d1=0.538, d2=0.471 g=1.252\n",
      ">340, 3/9, d1=0.500, d2=0.526 g=1.070\n",
      ">340, 4/9, d1=0.509, d2=0.733 g=0.938\n",
      ">340, 5/9, d1=0.605, d2=0.778 g=0.866\n",
      ">340, 6/9, d1=0.567, d2=0.787 g=0.836\n",
      ">340, 7/9, d1=0.678, d2=0.699 g=0.922\n"
     ]
    }
   ],
   "source": [
    "# example of a dcgan on cifar10\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.cifar10 import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "import image_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator32(in_shape=(32,32,3)):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator64v1(in_shape=(64,64,3)):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    model.add(Conv2D(128, (3,3), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(512, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.00005, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator64(in_shape=(64,64,3)):\n",
    "    model = Sequential()\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "    \n",
    "    # normal\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=init, input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), kernel_initializer=init, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (3,3), strides=(2,2), kernel_initializer=init, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(512, (3,3), strides=(2,2), kernel_initializer=init, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.00005, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator32(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator64(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator64v2(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 1024 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 1024)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator64v3(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 1024 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(ReLU())\n",
    "    model.add(Reshape((4, 4, 1024)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator64v4(latent_dim):\n",
    "    model = Sequential()\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "    \n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 1024 * 4 * 4\n",
    "    model.add(Dense(n_nodes,  kernel_initializer=init, input_dim=latent_dim))\n",
    "    model.add(ReLU())\n",
    "    model.add(Reshape((4, 4, 1024)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(512, (4,4), strides=(2,2), kernel_initializer=init, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), kernel_initializer=init, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), kernel_initializer=init, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), kernel_initializer=init, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    " \n",
    "    \n",
    "# load and prepare cifar10 training images\n",
    "def load_real_samples32():\n",
    "    # load cifar10 dataset\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    \n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "def trim_classes(images, labels, max_classes=10):\n",
    "    return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "def resize_images(images, size=(28,28)):\n",
    "    temp = []\n",
    "    for img in images:\n",
    "        temp.append(np.array(Image.fromarray(img).resize(size, Image.ANTIALIAS)))\n",
    "    return np.asarray(temp)\n",
    "\n",
    "# load and prepare cifar10 training images\n",
    "def load_real_samples64(n_classes=1):\n",
    "    # load imagenet64 dataset\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, 1)\n",
    "    (trainX, _), (_, _) = image_utils.split_data(images, labels, train=1)\n",
    "#     trainX = resize_images(trainX, size=(32,32))\n",
    "    \n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    " \n",
    "# example of smoothing class=1 to [0.7, 1.2]\n",
    "def smooth_positive_labels(y):\n",
    "    return y - 0.3 + (random(y.shape) * 0.5)\n",
    "    \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "#     y = smooth_positive_labels(y)\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# create and save a plot of generated images\n",
    "def save_plot(examples, epoch, n=7):\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(examples[i])\n",
    "    # save plot to file\n",
    "    filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "\n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'generator_model_%03d.h5' % (epoch+1)\n",
    "    g_model.save(filename)\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10000, n_batch=100):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "                (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator64()\n",
    "# create the generator\n",
    "g_model = define_generator64(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples64()\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)\n",
    "\n",
    "\n",
    "# images, labels = image_utils.load_images(path_root=\"\")\n",
    "# images, labels = trim_classes(images, labels, 1)\n",
    "# (trainX, _), (_, _) = image_utils.split_data(images, labels, train=1)\n",
    "# trainX = resize_images(trainX, size=(32,32))\n",
    "\n",
    "# print(trainX.shape)\n",
    "# pyplot.imshow(trainX[899])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVLLUIjU20wy"
   },
   "source": [
    "### ThoughtVIZ's  Baseline AC-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YwfGtq24GBZ"
   },
   "source": [
    "**Download model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ogKkn3n3Wus"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ThoughtViz/models\n",
    "!wget -q https://raw.githubusercontent.com/ptirupat/ThoughtViz/master/training/models/ac_gan.py -P ThoughtViz/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U47otpX3_V5"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-aC8Z_H2_OL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import image_utils\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "from ThoughtViz.models import ac_gan\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3e-xtTX4t-",
    "outputId": "0d2d4a97-8284-47cb-ad4b-fdfc7132d01c"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "MODEL_DIR = '/tmp/imagenet'\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "softmax = None\n",
    "\n",
    "\n",
    "# Call this function with list of images. Each of elements should be a\n",
    "# numpy array with values ranging from 0 to 255.\n",
    "def get_inception_score(images, splits=10):\n",
    "  assert(type(images) == list)\n",
    "  assert(type(images[0]) == np.ndarray)\n",
    "  assert(len(images[0].shape) == 3)\n",
    "  assert(np.max(images[0]) > 10)\n",
    "  assert(np.min(images[0]) >= 0.0)\n",
    "  inps = []\n",
    "  for img in images:\n",
    "    img = img.astype(np.float32)\n",
    "    inps.append(np.expand_dims(img, 0))\n",
    "  bs = 1\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    preds = []\n",
    "    n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n",
    "    for i in range(n_batches):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n",
    "        inp = np.concatenate(inp, 0)\n",
    "        pred = sess.run(softmax, {'ExpandDims:0': inp})\n",
    "        preds.append(pred)\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "      part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
    "      kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "      kl = np.mean(np.sum(kl, 1))\n",
    "      scores.append(np.exp(kl))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "# This function is called automatically.\n",
    "def _init_inception():\n",
    "  global softmax\n",
    "  if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(MODEL_DIR, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  tarfile.open(filepath, 'r:gz').extractall(MODEL_DIR)\n",
    "  with tf.compat.v1.gfile.FastGFile(os.path.join(\n",
    "      MODEL_DIR, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "  # Works with an arbitrary minibatch size.\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    pool3 = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "    ops = pool3.graph.get_operations()\n",
    "    for op_idx, op in enumerate(ops):\n",
    "        for o in op.outputs:\n",
    "            shape = o.get_shape()\n",
    "            shape = [s for s in shape]\n",
    "            new_shape = []\n",
    "            for j, s in enumerate(shape):\n",
    "                if s == 1 and j == 0:\n",
    "                    new_shape.append(None)\n",
    "                else:\n",
    "                    new_shape.append(s)\n",
    "            o.set_shape(tf.TensorShape(new_shape))\n",
    "    w = sess.graph.get_operation_by_name(\"softmax/logits/MatMul\").inputs[1]\n",
    "    logits = tf.matmul(tf.squeeze(pool3, [1, 2]), w)\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "if softmax is None:\n",
    "  _init_inception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrAlxWDP4QuZ"
   },
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKNRqAFF4TEe"
   },
   "outputs": [],
   "source": [
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "def combine_rgb_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1], 3),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1], :] = img[:, :, :]\n",
    "    return image\n",
    "\n",
    "def load_data(num_classes):\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, num_classes)\n",
    "    (x_train, y_train), (x_test, y_test) = image_utils.split_data(images, labels)\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "    x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def train_gan(input_noise_dim, batch_size, epochs, model_save_dir, output_dir, num_classes):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = load_data(num_classes)\n",
    "    print(y_train.shape)\n",
    "    adam_lr = 0.001\n",
    "    adam_beta_1 = 0.5\n",
    "\n",
    "    d = ac_gan.discriminator_model_rgb((64,64), num_classes)\n",
    "    d_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "    d.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=d_optim)\n",
    "    d.trainable = True\n",
    "\n",
    "    g = ac_gan.generator_model_rgb(input_noise_dim + num_classes)\n",
    "    g_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "    g.compile(loss='categorical_crossentropy', optimizer=g_optim)\n",
    "\n",
    "    d_on_g = ac_gan.generator_containing_discriminator(input_noise_dim + num_classes, g, d)\n",
    "    d_on_g.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=g_optim)\n",
    "\n",
    "    g.summary()\n",
    "    d.summary()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is \", epoch)\n",
    "\n",
    "        print(\"Number of batches\", int(x_train.shape[0]/batch_size))\n",
    "\n",
    "        for index in range(int(x_train.shape[0]/batch_size)):\n",
    "            # generate noise from a normal distribution\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "            random_labels = [randint(0, 9) for i in range(batch_size)]\n",
    "\n",
    "            one_hot_vectors = [to_categorical(label, 10) for label in random_labels]\n",
    "\n",
    "            conditioned_noise = []\n",
    "            for i in range(batch_size):\n",
    "                conditioned_noise.append(np.append(noise[i], one_hot_vectors[i]))\n",
    "            conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "            # get real images and corresponding labels\n",
    "            real_images = x_train[index * batch_size:(index + 1) * batch_size]\n",
    "            real_labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            # generate fake images using the generator\n",
    "            generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "            # discriminator loss of real images\n",
    "            d_loss_real = d.train_on_batch(real_images, [np.array([1] * batch_size), np.array(real_labels)])\n",
    "            # discriminator loss of fake images\n",
    "            d_loss_fake = d.train_on_batch(generated_images, [np.array([0] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
    "            d_loss = (d_loss_fake[0] + d_loss_real[0]) * 0.5\n",
    "\n",
    "            # save generated images at intermediate stages of training\n",
    "            if index % 250 == 0:\n",
    "                image = combine_rgb_images(generated_images)\n",
    "                image = image * 255.0\n",
    "                img_save_path = os.path.join(output_dir, str(epoch) + \"_g_\" + str(index) + \".png\")\n",
    "                Image.fromarray(image.astype(np.uint8)).save(img_save_path)\n",
    "\n",
    "            d.trainable = False\n",
    "            # generator loss\n",
    "            g_loss = d_on_g.train_on_batch(conditioned_noise, [np.array([1] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
    "            d.trainable = True\n",
    "\n",
    "        # test_image_count = 50000\n",
    "        test_image_count = x_train.shape[0]\n",
    "        test_noise = np.random.uniform(-1, 1, (test_image_count, input_noise_dim))\n",
    "        test_labels = [randint(0, 9) for i in range(test_image_count)]\n",
    "        one_hot_vectors_test = [to_categorical(label, 10) for label in test_labels]\n",
    "\n",
    "        conditioned_noise_test = []\n",
    "        for i in range(test_image_count):\n",
    "            conditioned_noise_test.append(np.append(test_noise[i], one_hot_vectors_test[i]))\n",
    "        conditioned_noise_test = np.array(conditioned_noise_test)\n",
    "\n",
    "        test_images = g.predict(conditioned_noise_test, verbose=0)\n",
    "        test_images = test_images * 255.0\n",
    "        \n",
    "        if epoch % 50 == 0:        \n",
    "            inception_score = get_inception_score([test_image for test_image in test_images], splits=10)\n",
    "\n",
    "        print(\"Epoch %d d_loss : %f\" % (epoch, d_loss))\n",
    "        print(\"Epoch %d g_loss : %f\" % (epoch, g_loss[0]))\n",
    "        print(\"Epoch %d inception_score : %f\" % (epoch, inception_score[0]))\n",
    "\n",
    "        # save generator and discriminator models along with the weights\n",
    "        g.save(os.path.join(model_save_dir, 'generator_' + str(epoch)), overwrite=True, include_optimizer=True)\n",
    "        d.save(os.path.join(model_save_dir, 'discriminator_' + str(epoch)), overwrite=True, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqtGoD-g4VtZ"
   },
   "source": [
    "**Train GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_5zBRwzZ4cEw",
    "outputId": "456f933e-87a8-404c-969c-ba8fc3a32341"
   },
   "outputs": [],
   "source": [
    "batch_size =50\n",
    "run_id = 1\n",
    "n_epochs = 500\n",
    "input_dim = 100\n",
    "n_classes=10\n",
    "\n",
    "model_save_dir = os.path.join('./saved_models/baseline_acgan/', 'run_' + str(run_id))\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "output_dir = os.path.join('./outputs/baseline_acgan/', 'run_' + str(run_id))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train_gan(input_dim, batch_size, n_epochs, model_save_dir, output_dir, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imWPfhDd9pca"
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "def resize_images(images, size=(28,28)):\n",
    "  temp = []\n",
    "  for img in images:\n",
    "    temp.append(np.array(Image.fromarray(img).resize(size, Image.ANTIALIAS)))\n",
    "  return np.asarray(temp)\n",
    "\n",
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRZLkkFj9fDJ",
    "outputId": "7e6e606f-22d7-4a21-e1ec-3a09f61b7490"
   },
   "outputs": [],
   "source": [
    "images, labels = image_utils.load_images()\n",
    "images = rgb2gray(images)\n",
    "print(images.shape)\n",
    "images = resize_images(images)\n",
    "print(images.shape)\n",
    "(x_train, y_train), (x_test, y_test) = image_utils.split_data(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXagMgJP5RqG"
   },
   "source": [
    "**Download Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQf2da6q5RIY"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/models/ThoughtViz\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/generator_100/saved_model.pb  /content/models/ThoughtViz/\n",
    "!mv /content/models/ThoughtViz/saved_model.pb /content/models/ThoughtViz/generator.pb\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/discriminator_100/saved_model.pb  /content/models/ThoughtViz/\n",
    "!mv /content/models/ThoughtViz/saved_model.pb /content/models/ThoughtViz/discriminator.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AlRT0ZE9QO4"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/models/ThoughtViz\n",
    "!cp -r /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/generator_100 /content/models/ThoughtViz/\n",
    "!cp -r /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/discriminator_100 /content/models/ThoughtViz/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J8CDTsq6mtU"
   },
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlLpMWkz7u_6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "WllfTTeJ_NI4",
    "outputId": "f37faaff-24e2-4542-d245-5a4955e28dce"
   },
   "outputs": [],
   "source": [
    "K.set_learning_phase(False)\n",
    "\n",
    "input_noise_dim = 100\n",
    "batch_size = 50\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "conditioned_noise = []\n",
    "for i in range(batch_size):\n",
    "    conditioned_noise.append(np.append(noise[i], to_categorical(0, 10)))\n",
    "conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "g = load_model(\"/content/models/ThoughtViz/generator_100\")\n",
    "\n",
    "# generate images using the generator\n",
    "generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "image = combine_rgb_images(generated_images)\n",
    "image = image * 127.5 + 127.5\n",
    "plt.imshow(image)\n",
    "img = Image.fromarray(image.astype(np.uint8))\n",
    "img.show()\n",
    "img.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmZV825L9kv1",
    "outputId": "268a9579-52af-49b7-811e-fd88451ed846"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from layers.mog_layer import *\n",
    "from utils.image_utils import *\n",
    "\n",
    "\n",
    "class Tests():\n",
    "\n",
    "    def test_deligan_baseline(self, generator_model):\n",
    "        K.set_learning_phase(False)\n",
    "\n",
    "        input_noise_dim = 100\n",
    "        batch_size = 50\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "        random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "        conditioned_noise = []\n",
    "        for i in range(batch_size):\n",
    "            conditioned_noise.append(np.append(noise[i], to_categorical(random_labels[i], 10)))\n",
    "        conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
    "\n",
    "        # generate images using the generator\n",
    "        generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "        image = combine_rgb_images(generated_images)\n",
    "        image = image * 127.5 + 127.5\n",
    "        img = Image.fromarray(image.astype(np.uint8))\n",
    "        img.show()\n",
    "\n",
    "    def test_deligan_final(self, generator_model, classifier_model, eeg_pkl_file):\n",
    "        K.set_learning_phase(False)\n",
    "\n",
    "        # load EEG data\n",
    "        eeg_data = pickle.load(open(eeg_pkl_file, \"rb\"), encoding='bytes')\n",
    "        classifier = load_model(classifier_model)\n",
    "\n",
    "        x_test = eeg_data[b'x_test']\n",
    "        y_test = eeg_data[b'y_test']\n",
    "        y_test = [np.argmax(y) for y in y_test]\n",
    "        layer_index = 9\n",
    "\n",
    "        # keras way of getting the output from an intermediate layer\n",
    "        get_nth_layer_output = K.function([classifier.layers[0].input], [classifier.layers[layer_index].output])\n",
    "\n",
    "        layer_output = get_nth_layer_output([x_test])[0]\n",
    "\n",
    "        input_noise_dim = 100\n",
    "        batch_size = 50\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "        random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "        eeg_feature_vectors = [layer_output[random.choice(np.where(y_test == random_label)[0])] for random_label in random_labels]\n",
    "\n",
    "        noises, conditionings = [], []\n",
    "        for i in range(batch_size):\n",
    "            noises.append(noise[i])\n",
    "            conditionings.append(eeg_feature_vectors[i])\n",
    "\n",
    "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
    "\n",
    "        # generate images using the generator\n",
    "        generated_images = g.predict([np.array(noises), np.array(conditionings)], verbose=0)\n",
    "\n",
    "        image = combine_rgb_images(generated_images)\n",
    "        image = image * 127.5 + 127.5\n",
    "        img = Image.fromarray(image.astype(np.uint8))\n",
    "        img.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tests = Tests()\n",
    "    #tests.test_deligan_baseline('../models/gan_models/baseline/deligan/image/generator.model')\n",
    "    tests.test_deligan_final('../models/gan_models/final/image/generator.model',\n",
    "                       '../models/eeg_models/image/run_final.h5',\n",
    "                       '../data/eeg/image/data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUOI8Pnt1pzr"
   },
   "source": [
    "# ThoughtViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTpkEupEF4S4"
   },
   "source": [
    "**Complete the following steps before running the cells below**\n",
    "\n",
    "1.\n",
    "Change runtime to GPU \n",
    "\n",
    ">Runtime > Change runtime type > GPU\n",
    "\n",
    "2.\n",
    "For each of the following links, add a Google Drive shortcut to your Drive root folder(My Drive):\n",
    "\n",
    ">https://drive.google.com/file/d/1atP9CsjWIT-hg3fX--fcC1hg0uvg9bEH/view\n",
    "\n",
    ">https://drive.google.com/file/d/1x32IulYeBVmkshEKweijMX3DK1Wu8odx/view\n",
    "\n",
    ">https://drive.google.com/file/d/1cq8RTBiwqO-Jo0TZjBNlRHZEhjKDknKP/view\n",
    "\n",
    ">https://drive.google.com/file/d/1U9qtN1SlOS3dzd2BwWWHhJiMz_0yNW9U/view\n",
    "\n",
    ">https://drive.google.com/file/d/1uFFhvTsU2nmdaecR2WPWsiGJfgI3as1_/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TItDNCT0N9q"
   },
   "source": [
    "**Mount drive and download project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUz-xkGirmGA"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfeFyd97TzfV"
   },
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/ptirupat/ThoughtViz.git\" \"/content/ThoughtViz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXJSE0mZGRI7"
   },
   "source": [
    "**Unzip project files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo9VRjm_soez"
   },
   "outputs": [],
   "source": [
    "!unzip -q \"/content/gdrive/My Drive/data.zip\" -d \"/content/ThoughtViz\" \n",
    "!unzip -q \"/content/gdrive/My Drive/images.zip\" -d \"/content/ThoughtViz/training\"\n",
    "!unzip -q \"/content/gdrive/My Drive/eeg_models.zip\" -d \"/content/ThoughtViz/models\" \n",
    "!unzip -q \"/content/gdrive/My Drive/trained_classifier_models.zip\" -d \"/content/ThoughtViz/training\"\n",
    "!unzip -q \"/content/gdrive/My Drive/gan_models.zip\" -d \"/content/ThoughtViz/models\"\n",
    "!echo \"Download Finished!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a79LZR2kDeaJ"
   },
   "source": [
    "**Update outdated module calls and make code compatible with colab environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8qiLZ7nApzh"
   },
   "outputs": [],
   "source": [
    "# Update Keras module structure\n",
    "!sed -i \"s/keras.layers/keras/\" ThoughtViz/layers/mog_layer.py\n",
    "\n",
    "# Change System Path\n",
    "!sed -i \"1i import sys\\nsys.path.insert(0,'/content/ThoughtViz')\\n\" /content/ThoughtViz/testing/test.py\n",
    "!sed -i \"s/\\..\\//\\/content\\/ThoughtViz\\//\" ThoughtViz/testing/test.py\n",
    "\n",
    "# Rename GAN model directory to match code\n",
    "!mv /content/ThoughtViz/models/gan_models/thoughtviz/ /content/ThoughtViz/models/gan_models/final/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3Hr0y71HRvh"
   },
   "source": [
    "**Test code**\n",
    "\n",
    "Note: Images are not saved by default. \n",
    ">To save the output image, you must edit the *ThoughtViz/testing/test.py* file and add the line **img.save('output.png')** after *img.show()* in the *test_deligan_final* function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov05zjNP70zG"
   },
   "outputs": [],
   "source": [
    "!python3 ThoughtViz/testing/test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtGN9xbZ39lE"
   },
   "source": [
    "# EEG decoding to images - Converting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoP7kwRQKHCm"
   },
   "source": [
    "**Convert to NPY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAbmc-734gXr"
   },
   "source": [
    "Code provided by: *https://github.com/perceivelab/eeg_visual_classification/blob/main/eeg_signal_classification.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fB7_4GgI4UkY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pth_to_npy(path):\n",
    "      torch_file = torch.load(path)\n",
    "\n",
    "      torch_dataset = torch_file[\"dataset\"]\n",
    "      torch_labels = torch_file[\"labels\"]\n",
    "      torch_images = torch_file[\"images\"]\n",
    "      torch_size = len(torch_dataset)\n",
    "\n",
    "      return torch_dataset, torch_labels, torch_images, torch_size\n",
    "\n",
    "torch_dataset, torch_labels, torch_images, torch_size = pth_to_npy(data_path)\n",
    "# # Dataset class\n",
    "# class EEGDataset:\n",
    "    \n",
    "#     # Constructor\n",
    "#     def __init__(self, eeg_signals_path):\n",
    "#         # Load EEG signals\n",
    "#         loaded = torch.load(eeg_signals_path)\n",
    "#         if opt.subject!=0:\n",
    "#             self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==opt.subject]\n",
    "#         else:\n",
    "#             self.data=loaded['dataset']        \n",
    "#         self.labels = loaded[\"labels\"]\n",
    "#         self.images = loaded[\"images\"]\n",
    "        \n",
    "#         # Compute size\n",
    "#         self.size = len(self.data)\n",
    "\n",
    "#     # Get size\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     # Get item\n",
    "#     def __getitem__(self, i):\n",
    "#         # Process EEG\n",
    "#         eeg = self.data[i][\"eeg\"].float().t()\n",
    "#         eeg = eeg[opt.time_low:opt.time_high,:]\n",
    "\n",
    "#         if opt.model_type == \"model10\":\n",
    "#             eeg = eeg.t()\n",
    "#             eeg = eeg.view(1,128,opt.time_high-opt.time_low)\n",
    "#         # Get label\n",
    "#         label = self.data[i][\"label\"]\n",
    "#         # Return\n",
    "#         return eeg, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hb5xFyEpQBaj",
    "outputId": "55157747-30fa-4d7c-9d04-49861e6fdd94"
   },
   "outputs": [],
   "source": [
    "print(\"size:\", torch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRPIXFBGdVpp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjWLx1doROMW",
    "outputId": "c50823c4-7ab9-4e88-aec4-d44d4af8d873"
   },
   "outputs": [],
   "source": [
    "# print(\"dataset 0:\", torch_dataset[0])\n",
    "eeg = torch_dataset[0][\"eeg\"]\n",
    "image_num = torch_dataset[0][\"image\"]\n",
    "label = torch_dataset[0][\"label\"]\n",
    "subject = torch_dataset[0][\"subject\"]\n",
    "\n",
    "print(\"eeg data:\", eeg)\n",
    "print(\"image number:\", image_num)\n",
    "print(\"image label:\", label)\n",
    "print(\"participant number:\", subject)\n",
    "\n",
    "trimmed_eeg = eeg.float().t()[40:460]\n",
    "print(\"eeg data trimmed:\", trimmed_eeg)\n",
    "\n",
    "numpy_eeg = trimmed_eeg.numpy()\n",
    "print(\"eeg data trimmed+numpy:\", numpy_eeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uI9xMywlQx7o",
    "outputId": "84bf189c-cbd1-4c98-9b06-d47cef86fe37"
   },
   "outputs": [],
   "source": [
    "print(\"label 0:\", torch_labels[0])\n",
    "print(\"label 1:\", torch_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVKjoP9MQ7eO",
    "outputId": "12cabec5-9fde-4b64-c2a7-9998174e2139"
   },
   "outputs": [],
   "source": [
    "print(\"image 0:\", torch_images[0])\n",
    "print(\"image 1:\", torch_images[1])\n",
    "\n",
    "for i in range(0,10):\n",
    "  print(torch_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kwdKppi4K0Z"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlOHKmR49Psy"
   },
   "source": [
    "**Download and Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0hHATgNyvPC",
    "outputId": "aca4bd0a-ecf2-4e07-9047-cfbcf9f57109"
   },
   "outputs": [],
   "source": [
    "!pip install -U mne\n",
    "!pip install pyEDFlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zs8lBY6j9B5_"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "from pyedflib import highlevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fZb1Gzn9UzE"
   },
   "source": [
    "**Define Visualization Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-BOnV8T8lmx"
   },
   "outputs": [],
   "source": [
    "def create_edf(npy_path, name):\n",
    "  data = np.load(npy_path)\n",
    "  # [128,440,11965]\n",
    "  signals = list(data.T[:,:,:])\n",
    "  channel_names = list('ch'+str(i) for i in range(0,128))\n",
    "  signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "  header = highlevel.make_header()\n",
    "  highlevel.write_edf(name, signals, signal_headers, header)\n",
    "\n",
    "def create_edf2(npy_path, name):\n",
    "  data = np.load(npy_path)\n",
    "  # [128,11965,440]\n",
    "  signals = list(data.transpose(2,0,1)[0:ch,0:100,:])\n",
    "  channel_names = list('ch'+str(i) for i in range(0,128))\n",
    "  signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "  header = highlevel.make_header()\n",
    "  highlevel.write_edf(name, signals, signal_headers, header)\n",
    "\n",
    "def visualizeData(path):\n",
    "  raw = mne.io.read_raw_edf(path,preload=True);\n",
    "  print(raw)\n",
    "  raw.plot_psd(area_mode='range', fmin=15, fmax=70.0, show=False, average=True);\n",
    "  raw.plot(duration=2, scalings=0.000002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EL43khtN0b-"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/gdrive/MyDrive/EEG2Image/data/eeg_14_70.npy'\n",
    "test_path = '/content/test_file.edf'\n",
    "rand_path = '/content/edf_file.edf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AAoglAZY3JR",
    "outputId": "b5272e3b-7fa6-47ea-e476-2c2a734fc415"
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data[::1,1].shape)\n",
    "print(data[::1,2].shape)\n",
    "print(data[1::].shape)\n",
    "print(data[2::].shape)\n",
    "print(data.T[0,0].shape)\n",
    "print(data.T.shape)\n",
    "print(data.T[:,:,0:10].shape)\n",
    "print(data.T[:,:,0:10].reshape(128,440*10).shape)\n",
    "print(data.transpose(2,0,1).shape)\n",
    "print(data.transpose(2,0,1)[:,0:10,:].shape)\n",
    "data.transpose(2,0,1)[0:10,0:10,:].reshape((128,440*10)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "eepeZbubOJVk",
    "outputId": "37f4d124-3512-47f7-c397-c5f1322f75fc"
   },
   "outputs": [],
   "source": [
    "# write an edf file\n",
    "ch = 5\n",
    "signals = list(data.T[0:ch,:,0:100].reshape(ch,440*100))\n",
    "channel_names = list('ch'+str(i) for i in range(0,ch))\n",
    "signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "header = highlevel.make_header(patientname='patient_x', gender='Female')\n",
    "highlevel.write_edf('test_file.edf', signals, signal_headers, header)\n",
    "\n",
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "J0m0kkUuNIXb",
    "outputId": "d78821b7-7916-47c4-cdd5-0cff76fc4c70"
   },
   "outputs": [],
   "source": [
    "# write an edf file\n",
    "ch = 5\n",
    "signals = list(data.transpose(2,0,1)[0:ch,0:100,:])\n",
    "channel_names = list('ch'+str(i) for i in range(0,ch))\n",
    "signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "header = highlevel.make_header()\n",
    "highlevel.write_edf('test_file.edf', signals, signal_headers, header)\n",
    "\n",
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "atyHW3BWnS9J",
    "outputId": "d32f1654-12cb-4e54-a1e6-f0ddf0e45817"
   },
   "outputs": [],
   "source": [
    "# write an edf file\n",
    "ch = 50\n",
    "signals = list(data.transpose(2,0,1)[0:ch,0:100,:])\n",
    "channel_names = list('ch'+str(i) for i in range(0,ch))\n",
    "signal_headers = highlevel.make_signal_headers(channel_names, sample_rate=1000)\n",
    "header = highlevel.make_header()\n",
    "highlevel.write_edf('test_file.edf', signals, signal_headers, header)\n",
    "\n",
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "NOQdtLEINQGy",
    "outputId": "56dcc711-ef5c-4d58-def9-6dc01dc45546"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "0wsi8UGJlflJ",
    "outputId": "62b590c0-0d04-45ce-ecec-af507f177a3e"
   },
   "outputs": [],
   "source": [
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "xZzJCRhgnV1d",
    "outputId": "6d9597e4-97d0-4973-c916-bfc0ad877648"
   },
   "outputs": [],
   "source": [
    "visualizeData(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC2ITzzILMDk"
   },
   "source": [
    "**LSTM stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxsizMvPLUUQ"
   },
   "outputs": [],
   "source": [
    "# Code goes here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz0lue81LaON"
   },
   "source": [
    "**GAN stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJIEr4gVLgCY"
   },
   "outputs": [],
   "source": [
    "# Code goes here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXagbmapLnsY"
   },
   "source": [
    "**Test Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIc1nb7BLpsC"
   },
   "outputs": [],
   "source": [
    "# Code goes here #"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPqkXqJUAQ2NK5Q/RgvUhXg",
   "collapsed_sections": [
    "dsBmC9XVLTvz",
    "mUOI8Pnt1pzr",
    "LtGN9xbZ39lE",
    "_kwdKppi4K0Z"
   ],
   "include_colab_link": true,
   "name": "CSE 247 Project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
