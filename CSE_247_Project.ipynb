{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlexanderCardarasUCSC/EEG-Decoding-to-Images/blob/main/CSE_247_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7ZhR8a4LkDU"
   },
   "source": [
    "**Complete the following steps before running the cells below**\n",
    "\n",
    "1.\n",
    "Change runtime to GPU \n",
    "\n",
    ">Runtime > Change runtime type > GPU\n",
    "\n",
    "2.\n",
    "For each of the following link, add a Google Drive shortcut to your Drive root folder(My Drive):\n",
    "\n",
    ">https://drive.google.com/drive/folders/1z8wcew5R7GCeu5s7SSsWw8bUcx1urVmk?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR4Zp5-xYElX"
   },
   "source": [
    "# Load data - PRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6luLgH9YEla"
   },
   "outputs": [],
   "source": [
    "# !pip -q install keras\n",
    "!pip -q install google-cloud-storage\n",
    "!pip -q install google-api-python-client oauth2client\n",
    "!pip -q install tensorflow-addons\n",
    "!pip -q install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoKm9mG3YElb"
   },
   "outputs": [],
   "source": [
    "from apiclient import discovery\n",
    "from httplib2 import Http\n",
    "import oauth2client\n",
    "from oauth2client import file, client, tools\n",
    "\n",
    "obj = lambda: None\n",
    "lmao = {\"auth_host_name\":'localhost', 'noauth_local_webserver':'store_true', \n",
    "        'auth_host_port':[8080, 8090], 'logging_level':'ERROR'}\n",
    "for k, v in lmao.items():\n",
    "    setattr(obj, k, v)\n",
    "    \n",
    "# authorization boilerplate code\n",
    "SCOPES = 'https://www.googleapis.com/auth/drive.readonly'\n",
    "store = file.Storage('token.json')\n",
    "creds = store.get()\n",
    "# The following will give you a link if token.json does not exist, the link allows the\n",
    "# user to give this app permission\n",
    "if not creds or creds.invalid:\n",
    "    flow = client.flow_from_clientsecrets('client_secret.json', SCOPES)\n",
    "    creds = tools.run_flow(flow, store, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyUcPULtYElc"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "DRIVE = discovery.build('drive', 'v3', http=creds.authorize(Http()))\n",
    "\n",
    "def download_file(file_id, filename):\n",
    "    # if you get the shareable link, the link contains this id, replace the file_id below\n",
    "    request = DRIVE.files().get_media(fileId=file_id)\n",
    "    # replace the filename and extension in the first field below\n",
    "    fh = io.FileIO(filename, mode='w')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTYek1YQYEle"
   },
   "outputs": [],
   "source": [
    "# Download Image data\n",
    "!mkdir -p data/images\n",
    "!wget -q https://raw.githubusercontent.com/AlexanderCardarasUCSC/EEG-Decoding-to-Images/main/image_utils.py \n",
    "download_file(\"1-2WFgtBn5WdZCnDlJkwaswTTrRtVNrS0\", \"images.npy\")\n",
    "download_file(\"1Lijm9A5Kq4EMub5jO0Bzz3c2VJLm-WH6\", \"image_labels.npy\")\n",
    "\n",
    "!sudo mv images.npy data/images\n",
    "!sudo mv image_labels.npy data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUWoxMgn9TOB"
   },
   "source": [
    "# Load data - Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JenuAt1ALqvt"
   },
   "source": [
    "**Mount drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVsVV6U5LtiA",
    "outputId": "a82b98d6-30a8-410e-f490-0da0cf78fb11"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNDc6jwSzYjf"
   },
   "source": [
    "*55-95Hz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nrNCoFSzNbX"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/data/eeg\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/eeg_54_95_std.pth /content/data/eeg/\n",
    "eeg_path = \"/content/data/eeg/eeg_54_95_std.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pzsz5CBrLJQJ"
   },
   "source": [
    "#### Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwepmfU39z3E"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/images\n",
    "!wget -q https://raw.githubusercontent.com/AlexanderCardarasUCSC/EEG-Decoding-to-Images/main/image_utils.py /content\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/data/images.npy /content/images\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/data/image_labels.npy /content/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ7shO6sIN5l"
   },
   "source": [
    "# 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from PIL import Image\n",
    "import math\n",
    "import image_utils\n",
    "from tensorflow_addons.layers import SpectralNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import cv2\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "#         losses.append((gen_loss, disc_loss))\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "    \n",
    "def train(dataset, epochs, losses):\n",
    "    for epoch in range(epochs):\n",
    "        gen_losses = []\n",
    "        disc_losses = []\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "            \n",
    "            gen_losses.append(gen_loss.numpy())\n",
    "            disc_losses.append(disc_loss.numpy())\n",
    "        \n",
    "        losses.append([sum(gen_losses)/len(gen_losses), sum(disc_losses)/len(disc_losses)])\n",
    "        # Produce images for the GIF as you go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = generator(seed, training=False)\n",
    "    predictions = ((predictions + 1)/2.0*255).numpy().astype(int)\n",
    "\n",
    "    # plot the result\n",
    "    f, axs = plt.subplots(4,4,figsize=(8,8))\n",
    "    # plot images\n",
    "    for i in range(16):\n",
    "        axs[i//4,i%4].imshow(predictions[i])\n",
    "        axs[i//4,i%4].axis(\"off\")\n",
    "\n",
    "    plt.savefig('images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "def resize_images(images, size=(28,28)):\n",
    "    temp = []\n",
    "    for img in images:\n",
    "        temp.append(np.array(Image.fromarray(img).resize(size, Image.ANTIALIAS)))\n",
    "    return np.asarray(temp)\n",
    "\n",
    "def augment_images(images):\n",
    "    augmented = []\n",
    "    for image in images:\n",
    "        flipped = tf.image.flip_left_right(image)\n",
    "        \n",
    "        augmented.append(image)\n",
    "        augmented.append(flipped)\n",
    "        \n",
    "    return np.asarray(augmented)\n",
    "\n",
    "def trim_classes(images, labels, max_classes=10, images_per_class=900):\n",
    "    return images[0:images_per_class*max_classes], labels[0:images_per_class*max_classes]\n",
    "\n",
    "# load and prepare imnet64 training images\n",
    "def load_real_samples(n_classes=1):\n",
    "    # load imagenet64 dataset\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images = augment_images(images)\n",
    "#     images = resize_images(images)\n",
    "#     images = rgb2gray(images)\n",
    "    images, labels = trim_classes(images, labels, n_classes,images_per_class=1800)\n",
    "    (trainX, trainY), (_, _) = image_utils.split_data(images, labels, train=1)\n",
    "    \n",
    "\n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    \n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    print(X.shape, trainY.shape)\n",
    "    return [X, trainY]\n",
    "\n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('images/image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(16*16*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((16, 16, 256)))\n",
    "    assert model.output_shape == (None, 16, 16, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    assert model.output_shape == (None, 16, 16, 128)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    assert model.output_shape == (None, 32, 32, 64)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 64, 64, 3)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 3]))\n",
    "    \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "n_classes = 1\n",
    "# BUFFER_SIZE = 60000\n",
    "BUFFER_SIZE = 1800*n_classes\n",
    "# BATCH_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1000\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "train_images = load_real_samples(n_classes=n_classes)[0]\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "generator = make_generator_model()\n",
    "# noise = tf.random.normal([1, 100])\n",
    "# generated_image = generator(noise, training=False)\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "# decision = discriminator(generated_image)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "train(train_dataset, EPOCHS, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan-64x64-1800-1-class-SN_DG.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('images/image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = None\n",
    "# for i in range(0,5):\n",
    "for i in range(5,10):\n",
    "    img = cv2.imread(\"images/image_at_epoch_%.4d\"%((i+1)*100)+\".png\")[64:-64,30:-30]\n",
    "    \n",
    "    if pic is None:\n",
    "        pic = img\n",
    "    else:\n",
    "        pic = np.hstack((pic,img))\n",
    "    print(pic.shape)\n",
    "\n",
    "cv2.imwrite(\"test.png\",pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Generator Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot([i for i in range(len(losses))], np.array(losses)[:,0], color=\"green\")\n",
    "plt.show()\n",
    "plt.savefig(\"Generator_Loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Discriminator Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot([i for i in range(len(losses))], np.array(losses)[:,1], color=\"blue\")\n",
    "plt.show()\n",
    "plt.savefig(\"Discriminator_Loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(16*16*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((16, 16, 256)))\n",
    "\n",
    "    model.add(SpectralNormalization(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same',\n",
    "                                                           use_bias=False)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(SpectralNormalization(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', \n",
    "                                                           use_bias=False)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(SpectralNormalization(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                  input_shape=[64, 64, 3])))\n",
    "    \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(SpectralNormalization(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2hmYEUh2q5R"
   },
   "source": [
    "### Jason's AC-GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-w0-RTJYEl2"
   },
   "outputs": [],
   "source": [
    "!mkdir images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZLFNBTbdkrK"
   },
   "outputs": [],
   "source": [
    "# example of fitting an auxiliary classifier gan (ac-gan) on fashion mnsit\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow_addons.layers import SpectralNormalization\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXllC6Y0PiRj"
   },
   "source": [
    "**Discriminator architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUwEvQbeFk-2"
   },
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminatorv1(in_shape=(64,64,3), n_classes=10):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    # downsample to 14x14\n",
    "    fe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Dropout(0.5)(fe)\n",
    "    # normal\n",
    "    fe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Dropout(0.5)(fe)\n",
    "    # downsample to 7x7\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = LeakyReLU( alpha=0.2)(fe)\n",
    "    fe = Dropout(0.5)(fe)\n",
    "    # normal\n",
    "    fe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Dropout(0.5)(fe)\n",
    "    # flatten feature maps\n",
    "    fe = Flatten()(fe)\n",
    "    # real/fake output\n",
    "    out1 = Dense(1, activation='sigmoid')(fe)\n",
    "    # class label output\n",
    "    out2 = Dense(n_classes, activation='softmax')(fe)\n",
    "    # define model\n",
    "    model = Model(in_image, [out1, out2])\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# # define the standalone discriminator model\n",
    "# def define_discriminator(in_shape=(64,64,3)):\n",
    "#     model = Sequential()\n",
    "#     # normal\n",
    "#     model.add(Conv2D(128, (4,4), padding='same', input_shape=in_shape))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "#     # downsample\n",
    "#     model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "#     # downsample\n",
    "#     model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "#     # downsample\n",
    "#     model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "#     # classifier\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     # compile model\n",
    "#     opt = Adam(lr=0.00005, beta_1=0.5)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape, n_classes):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    \n",
    "    # normal\n",
    "    fe = SpectralNormalization(Conv2D(128, (4,4), padding='same', kernel_initializer=init))(in_image)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "\n",
    "    # downsample to 32x32\n",
    "    fe = SpectralNormalization(Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init))(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    \n",
    "    # downsample to 16x16\n",
    "    fe = SpectralNormalization(Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init))(fe)\n",
    "    fe = LeakyReLU( alpha=0.2)(fe)\n",
    "        \n",
    "    # downsample to 8x8\n",
    "    fe = SpectralNormalization(Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init))(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    \n",
    "\n",
    "    # flatten feature maps\n",
    "    fe = Flatten()(fe)\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    \n",
    "    # real/fake output\n",
    "    out1 = Dense(1, activation='sigmoid')(fe)\n",
    "    \n",
    "    # class label output\n",
    "    out2 = Dense(n_classes, activation='softmax')(fe)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(in_image, [out1, out2])\n",
    "   \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.00005, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true*y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9yPgBfEPsdM"
   },
   "source": [
    "**Generator architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtkgzUM3PuXp"
   },
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generatorv1(latent_dim, n_classes=10):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    # li = Embedding(n_classes, 50)(in_label)\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # linear multiplication\n",
    "    # n_nodes = 7 * 7\n",
    "    n_nodes = 16 * 16 * 3\n",
    "    li = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "    # reshape to additional channel\n",
    "    # li = Reshape((7, 7, 1))(li)\n",
    "    li = Reshape((16, 16, 3))(li)\n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    # foundation for 7x7 image\n",
    "    # n_nodes = 384 * 7 * 7\n",
    "    n_nodes = 384 * 16 * 16\n",
    "    gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "    gen = Activation('relu')(gen)\n",
    "    # gen = Reshape((7, 7, 384))(gen)\n",
    "    gen = Reshape((16, 16, 384))(gen)\n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([gen, li])\n",
    "    # upsample to 14x14\n",
    "    gen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "    gen = BatchNormalization()(gen)\n",
    "    gen = Activation('relu')(gen)\n",
    "    # upsample to 28x28\n",
    "    # gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = Conv2DTranspose(3, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    out_layer = Activation('tanh')(gen)\n",
    "    # define model\n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    \n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    \n",
    "    # linear multiplication\n",
    "    n_nodes = 4 * 4 * 3\n",
    "    li = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "    \n",
    "    # reshape to additional channel\n",
    "    li = Reshape((4, 4, 3))(li)\n",
    "    \n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(latent_dim,)\n",
    "                  )\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4*4\n",
    "    lat = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "    lat = Reshape((4, 4, 256))(lat)\n",
    "    \n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([lat, li])\n",
    "    \n",
    "    # upsample to 8x8\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "#     gen = BatchNormalization(momentum=0.3)(gen)\n",
    "#     gen = Dropout(0.5)(gen)\n",
    "    \n",
    "    # upsample to 16x16\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "#     gen = BatchNormalization(momentum=0.3)(gen)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "#     gen = BatchNormalization(momentum=0.3)(gen)\n",
    "#     gen = Dropout(0.5)(gen)\n",
    "    \n",
    "    # upsample to 64x64\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "#     gen = BatchNormalization(momentum=0.3)(gen)\n",
    "        \n",
    "    gen = Conv2D(3, (3,3), padding='same', kernel_initializer=init)(gen)\n",
    "    out_layer = Activation('tanh')(gen)\n",
    "    # define model\n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbtLd_QgP6T9"
   },
   "source": [
    "**GAN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUP86d-eP6jc"
   },
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # connect the outputs of the generator to the inputs of the discriminator\n",
    "    gan_output = d_model(g_model.output)\n",
    "    # define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "    model = Model(g_model.input, gan_output)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHNh232fQxaC"
   },
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t29JjwvnQnGz"
   },
   "outputs": [],
   "source": [
    "def trim_classes(images, labels, max_classes=10):\n",
    "    return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "# load images\n",
    "def load_real_samples(n_classes):\n",
    "    # load dataset\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, n_classes)\n",
    "    (trainX, trainY), (testX, testY) = image_utils.split_data(images, labels, train=1)\n",
    "    # expand to 3d, e.g. add channels\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # convert from ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    print(X.shape, trainY.shape)\n",
    "    return [X, trainY]\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples, n_classes)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y\n",
    "\n",
    "def generate_fake_class_sample(generator, class_label, latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    labels_input = np.asarray([class_label for i in range(n_samples)])\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y\n",
    "\n",
    " \n",
    "# create and save a plot of generated images\n",
    "def save_plot(examples, epoch, n=5):\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "    # plot the result\n",
    "    f, axs = pyplot.subplots(n,n,figsize=((n*n)-1,(n*n)-1))\n",
    "    # plot images\n",
    "    for i in range(n*n):\n",
    "        axs[i//n,i%n].imshow(examples[i])\n",
    "        axs[i//n,i%n].axis(\"off\")\n",
    "    # save plot to file\n",
    "    filename = 'images/generated_plot_e%03d.png' % (epoch+1)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "    # prepare fake examples\n",
    "    [X, _], _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    save_plot(X, step)\n",
    "    print('>Saved')\n",
    "    \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_classes=10):\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # get randomly selected 'real' samples\n",
    "        [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "        # update discriminator model weights\n",
    "        _,d_r1,d_r2 = d_model.train_on_batch(X_real, [y_real, labels_real])\n",
    "        # generate 'fake' examples\n",
    "        [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator model weights\n",
    "        _,d_f,d_f2 = d_model.train_on_batch(X_fake, [y_fake, labels_fake])\n",
    "        # prepare points in latent space as input for the generator\n",
    "        [z_input, z_labels] = generate_latent_points(latent_dim, n_batch, n_classes)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        _,g_1,g_2 = gan_model.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % ((i//bat_per_epo)+1, d_r1,d_r2, d_f,d_f2, g_1,g_2))\n",
    "        # evaluate the model performance every 'epoch'\n",
    "        if (i+1) % (bat_per_epo * 10) == 0:\n",
    "            summarize_performance(i//(bat_per_epo), g_model, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-HQexL2SEaB"
   },
   "source": [
    "**Train GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZTVMnDL0REYL",
    "outputId": "bb8eb22f-4ba1-4a9d-8440-b11b057fb006"
   },
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "# latent_dim = 100\n",
    "latent_dim = 100\n",
    "n_classes=10\n",
    "n_epochs = 5000\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator((64,64,3), n_classes)\n",
    "discriminator.summary()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim, n_classes)\n",
    "generator.summary()\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "gan_model.summary()\n",
    "# load image data\n",
    "dataset = load_real_samples(n_classes)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=n_epochs, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVFroeQlgCZ1"
   },
   "outputs": [],
   "source": [
    "[X, _], _ = generate_fake_class_sample(generator, class_label=0, latent_dim=latent_dim, n_samples=16)\n",
    "# scale from [-1,1] to [0,1]\n",
    "X = (X + 1) / 2.0\n",
    "\n",
    "f, axs = plt.subplots(4,4,figsize=(15,15))\n",
    "# plot images\n",
    "for i in range(16):\n",
    "    axs[i//4,i%4].imshow(X[i])\n",
    "    axs[i//4,i%4].axis(\"off\")\n",
    "f.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHRHGSU2MOHA"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(generator, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(discriminator, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReRlWMz_YEmB"
   },
   "source": [
    "# Jason RGB GAN + DC GAN + GAN Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ri-UWOjYEmD"
   },
   "source": [
    "Used\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "\n",
    "https://towardsdatascience.com/gan-ways-to-improve-gan-performance-acf37f9f59b\n",
    "\n",
    "More Details\n",
    "\n",
    "https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "https://sthalles.github.io/advanced_gans/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDo3Jnt3YEmD"
   },
   "source": [
    "**Create dir to store images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf8hYafKYEmF"
   },
   "outputs": [],
   "source": [
    "!mkdir images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyxe_t3jYEmJ"
   },
   "source": [
    "**GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY4iS5P4YEmK"
   },
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.cifar10 import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.models import save_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "import keras.backend as K\n",
    "from matplotlib import pyplot\n",
    "import image_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "def conv2d( x, filters, shape=(4, 4)) :\n",
    "    '''\n",
    "    I don't want to write lengthy parameters so I made a short hand function.\n",
    "    '''\n",
    "    x.add(Conv2D( filters, shape, strides=(2, 2), padding='same',\n",
    "               kernel_initializer=RandomNormal(stddev=0.02)))\n",
    "    #x.add(MaxPooling2D())\n",
    "    x.add(BatchNormalization(momentum = 0.3))\n",
    "    x.add(LeakyReLU(alpha=0.2))\n",
    "    return x\n",
    "    \n",
    "def deconv2d( x, filters, shape=(4, 4) ) :\n",
    "    '''\n",
    "    Conv2DTransposed gives me checkerboard artifact...\n",
    "    Select one of the 3.\n",
    "    '''\n",
    "    # Simpe Conv2DTranspose\n",
    "    # Not good, compared to upsample + conv2d below.\n",
    "    x.add(Conv2DTranspose( filters, shape, padding='same',\n",
    "        strides=(2, 2), kernel_initializer=RandomNormal(stddev=0.02)))\n",
    "\n",
    "    # simple and works\n",
    "#     x.add(UpSampling2D( (2, 2) ))\n",
    "#     x.add(Conv2D( filters, shape, padding='same' ))\n",
    "\n",
    "    # Bilinear2x... Not sure if it is without bug, not tested yet.\n",
    "    # Tend to make output blurry though\n",
    "    #x.add(bilinear2x( x, filters ))\n",
    "    #x.add(Conv2D( filters, shape, padding='same' ))\n",
    "\n",
    "    x.add(BatchNormalization(momentum=0.3))\n",
    "    x.add(LeakyReLU(alpha=0.2))\n",
    "    return x\n",
    "    \n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminatorv1(in_shape=(64,64,3)):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    model.add(Conv2D(128, (4,4), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.00005, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(64,64,3)):\n",
    "    model = Sequential()\n",
    "    # normal\n",
    "    model.add(Conv2D(8, (4,4), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    conv2d(model, 16)\n",
    "    # downsample\n",
    "    conv2d(model, 32)\n",
    "    # downsample\n",
    "    conv2d(model, 64)\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0001, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generatorv1(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    # upsample to 8x8\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 64 * 8 * 8\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((8, 8, 64)))\n",
    "#     upsample to 8x8\n",
    "#     model = deconv2d(model, 32)\n",
    "    # upsample to 16x16\n",
    "    model = deconv2d(model, 32)\n",
    "    # upsample to 32x32\n",
    "    model = deconv2d(model, 16)\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(8, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output layer\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "#     d_model.trainable = False\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    d_model.trainable = True\n",
    "    return model\n",
    "\n",
    "def trim_classes(images, labels, n_classes):\n",
    "    return images[0:900*n_classes], labels[0:900*n_classes]\n",
    "\n",
    "# load and prepare imnet64 training images\n",
    "def load_real_samples(n_classes=1):\n",
    "    # load imagenet64 dataset\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, n_classes)\n",
    "    (trainX, _), (_, _) = image_utils.split_data(images, labels, train=1)\n",
    "\n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# create and save a plot of generated images\n",
    "def save_plot(examples, epoch, n=6):\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "    # plot the result\n",
    "    f, axs = pyplot.subplots(n,n,figsize=((n*n)-1,(n*n)-1))\n",
    "    # plot images\n",
    "    for i in range(n*n):\n",
    "        axs[i//n,i%n].imshow(examples[i])\n",
    "        axs[i//n,i%n].axis(\"off\")\n",
    "    # save plot to file\n",
    "    filename = 'images/generated_plot_e%03d.png' % (epoch+1)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "    \n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, gan_model, dataset, latent_dim, n_samples=150):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    \n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "#     save(gan_model, g_model, d_model)\n",
    "    \n",
    "    \n",
    "def save(gan, generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    save_model(gan, 'gan')\n",
    "    discriminator.trainable = True\n",
    "    save_model(generator, 'generator')\n",
    "    save_model(discriminator, 'discriminator')\n",
    "\n",
    "\n",
    "def load():\n",
    "    discriminator = load_model('discriminator')\n",
    "#     discriminator.trainiable=True\n",
    "    generator = load_model('generator', compile=False)\n",
    "    gan = load_model('gan',)\n",
    "    discriminator.trainiable=False\n",
    "\n",
    "#     gan.summary()\n",
    "#     discriminator.summary()\n",
    "#     generator.summary()\n",
    "\n",
    "    return gan, generator, discriminator\n",
    "\n",
    "    \n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1000, n_batch=100, starting_epoch=0):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(starting_epoch, n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            d_model.trainable = False\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "                (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "            d_model.trainable = True\n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, gan_model, dataset, latent_dim)\n",
    "            \n",
    "def augment_images(images):\n",
    "    augmented = []\n",
    "    for image in images:\n",
    "        flipped = tf.image.flip_left_right(image)\n",
    "        \n",
    "        augmented.append(image)\n",
    "        augmented.append(flipped)\n",
    "        \n",
    "    return np.asarray(augmented)\n",
    "\n",
    "\n",
    "\n",
    "def create_and_train(n_classes=1):\n",
    "    # size of the latent space\n",
    "    latent_dim = 100\n",
    "    # create the discriminator\n",
    "    d_model = define_discriminator()\n",
    "#     d_model.summary()\n",
    "    \n",
    "    # create the generator\n",
    "    g_model = define_generator(latent_dim)\n",
    "#     g_model.summary()\n",
    "    \n",
    "    # create the gan\n",
    "    gan_model = define_gan(g_model, d_model)\n",
    "#     gan_model.summary()\n",
    "    \n",
    "    # load image data\n",
    "    dataset = load_real_samples(n_classes)\n",
    "    dataset = augment_images(dataset)\n",
    "    # train model\n",
    "    train(g_model, d_model, gan_model, dataset, latent_dim)\n",
    "    \n",
    "\n",
    "def load_and_train(model_name, n_classes, starting_epoch):\n",
    "    # size of the latent space\n",
    "    latent_dim = 100\n",
    "\n",
    "    gan_model, g_model, d_model = load()\n",
    "    \n",
    "    # load image data\n",
    "    dataset = load_real_samples(n_classes)\n",
    "    dataset = augment_images(dataset)\n",
    "    \n",
    "    \n",
    "    # train model\n",
    "    train(g_model, d_model, gan_model, dataset, latent_dim, starting_epoch=starting_epoch)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXCKM__NYEmM"
   },
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "create_and_train(n_classes)\n",
    "# load_and_train(\"010\", n_classes, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAojWfHxYEmM"
   },
   "source": [
    "**C-GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_TAKFAUYEmN"
   },
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.cifar10 import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import save_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "import keras.backend as K\n",
    "from matplotlib import pyplot\n",
    "import image_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape, n_classes):\n",
    "    # input\n",
    "    i_model = Input(shape=in_shape)\n",
    "    # normal\n",
    "    model = Conv2D(128, (4,4), input_shape=in_shape, padding='same')(i_model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # downsample\n",
    "    model = Conv2D(256, (4,4), strides=(2,2), padding='same')(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # downsample\n",
    "    model = Conv2D(256, (4,4), strides=(2,2), padding='same')(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # downsample\n",
    "    model = Conv2D(512, (4,4), strides=(2,2), padding='same')(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # classifier\n",
    "    model = Flatten()(model)\n",
    "    model = Dropout(0.4)(model)\n",
    "    \n",
    "    # output T/F\n",
    "    o_model = Dense(1, activation='sigmoid')(model)\n",
    "    \n",
    "    # output class label\n",
    "    c_model = Dense(n_classes, activation=\"softmax\")(model)\n",
    "    \n",
    "    model = Model(i_model, [o_model, c_model])\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.00005, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', \"sparse_categorical_crossentropy\"], optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes):\n",
    "    # input label\n",
    "    in_label = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    c_model = Embedding(n_classes, 50)(in_label)\n",
    "    n_nodes = 4 * 4 * 32\n",
    "    c_model = Dense(n_nodes)(c_model)\n",
    "    c_model = Reshape((4, 4, 32))(c_model)\n",
    "    \n",
    "    # input noise\n",
    "    in_latent = Input(shape=(latent_dim,))\n",
    "    # foundation for 4x4 image\n",
    "    n_nodes = 256 * 4 * 4\n",
    "    model = Dense(n_nodes, input_dim=latent_dim)(in_latent)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    model = Reshape((4, 4, 256))(model)\n",
    "    # upsample to 8x8\n",
    "    model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # upsample to 16x16\n",
    "    model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # upsample to 32x32\n",
    "    model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # upsample to 64x64\n",
    "    model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "    # output layer\n",
    "    o_model = Conv2D(3, (3,3), activation='tanh', padding='same')(model)\n",
    "    \n",
    "    model = Model([in_latent, in_label], o_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect the outputs of the generator to the inputs of the discriminator\n",
    "    o_model = d_model(g_model.output)\n",
    "    #define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "    model = Model(g_model.input, o_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy','sparse_categorical_crossentropy'], optimizer=opt)\n",
    "    d_model.trainable = True\n",
    "    return model\n",
    "\n",
    "def trim_classes(images, labels, n_classes):\n",
    "    return images[0:900*n_classes], labels[0:900*n_classes]\n",
    "\n",
    "# load and prepare imnet64 training images\n",
    "def load_real_samples(n_classes=1):\n",
    "    # load imagenet64 dataset\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, n_classes)\n",
    "    (trainX, trainY), (_, _) = image_utils.split_data(images, labels, train=1)\n",
    "\n",
    "    # convert from unsigned ints to floats\n",
    "    X = trainX.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    print(X.shape, trainY.shape)\n",
    "    return [X, trainY]\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [x_input, labels]\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples, n_classes):\n",
    "    # generate points in latent space\n",
    "    x_input, labels_input = generate_latent_points(latent_dim, n_samples, n_classes)\n",
    "    # predict outputs\n",
    "    images = g_model.predict([x_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y\n",
    "\n",
    " \n",
    "# create and save a plot of generated images\n",
    "def save_plot(examples, epoch, n_classes, n=5):\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "    # plot the result\n",
    "    f, axs = pyplot.subplots(n,n,figsize=((n*n)-1,(n*n)-1))\n",
    "    # plot images\n",
    "    for i in range(n*n):\n",
    "        axs[i//n,i%n].imshow(examples[i])\n",
    "        axs[i//n,i%n].axis(\"off\")\n",
    "    # save plot to file\n",
    "    filename = 'images/generated_plot_e%03d.png' % (epoch+1)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "    \n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, gan_model, dataset, latent_dim, n_classes, n_samples=150):\n",
    "    # prepare real samples\n",
    "    [X_real, labels_real], y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, tt, _, _, _ = d_model.evaluate(X_real, [y_real, labels_real], verbose=0)\n",
    "    _ = d_model.evaluate(X_real, [y_real, labels_real], verbose=0)\n",
    "\n",
    "    # prepare fake examples\n",
    "    [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, n_samples, n_classes)\n",
    "\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, ff, _, _, _ = d_model.evaluate(X_fake, [y_fake, labels_fake], verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (tt*100, ff*100))\n",
    "    \n",
    "    # save plot\n",
    "    save_plot(X_fake, epoch, n_classes)\n",
    "#     save(gan_model, g_model, d_model)\n",
    "    \n",
    "    \n",
    "def save(gan, generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    save_model(gan, 'gan')\n",
    "    discriminator.trainable = True\n",
    "    save_model(generator, 'generator')\n",
    "    save_model(discriminator, 'discriminator')\n",
    "\n",
    "\n",
    "def load():\n",
    "    discriminator = load_model('discriminator')\n",
    "#     discriminator.trainiable=True\n",
    "    generator = load_model('generator', compile=False)\n",
    "    gan = load_model('gan',)\n",
    "    discriminator.trainiable=False\n",
    "\n",
    "#     gan.summary()\n",
    "#     discriminator.summary()\n",
    "#     generator.summary()\n",
    "\n",
    "    return gan, generator, discriminator\n",
    "\n",
    "    \n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_classes, n_epochs=1000, n_batch=100, starting_epoch=0):\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(starting_epoch, n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights\n",
    "            dr,_,_,_,_  = d_model.train_on_batch(X_real, [y_real, labels_real])\n",
    "            \n",
    "            # generate 'fake' examples\n",
    "            [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch, n_classes)\n",
    "            # update discriminator model weights\n",
    "            df,_,_,_,_ = d_model.train_on_batch(X_fake, [y_fake, labels_fake])\n",
    "        \n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch, n_classes)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            d_model.trainable = False\n",
    "            # update the generator via the discriminator's error\n",
    "            g,_,_ = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, dr[%.3f], df[%.3f], g[%.3f]' % (i+1, dr, df, g))\n",
    "            \n",
    "            d_model.trainable = True\n",
    "#             summarize_performance(i, g_model, d_model, gan_model, dataset, latent_dim, n_classes)\n",
    "            \n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 5 == 0:\n",
    "            summarize_performance(i, g_model, d_model, gan_model, dataset, latent_dim, n_classes)\n",
    "            \n",
    "def augment_images(images):\n",
    "    augmented = []\n",
    "    for image in images:\n",
    "        flipped = tf.image.flip_left_right(image)\n",
    "        \n",
    "        augmented.append(image)\n",
    "        augmented.append(flipped)\n",
    "        \n",
    "    return np.asarray(augmented)\n",
    "\n",
    "\n",
    "\n",
    "def create_and_train(n_classes):\n",
    "    # size of the latent space\n",
    "    latent_dim = 100\n",
    "    # create the discriminator\n",
    "    d_model = define_discriminator((64,64,3), n_classes)\n",
    "    d_model.summary()\n",
    "    \n",
    "    # create the generator\n",
    "    g_model = define_generator(latent_dim, n_classes)\n",
    "    g_model.summary()\n",
    "    \n",
    "    # create the gan\n",
    "    gan_model = define_gan(g_model, d_model)\n",
    "    gan_model.summary()\n",
    "    \n",
    "    # load image data\n",
    "    dataset = load_real_samples(n_classes)\n",
    "#     dataset = augment_images(dataset)\n",
    "    # train model\n",
    "    train(g_model, d_model, gan_model, dataset, latent_dim, n_classes)\n",
    "    \n",
    "\n",
    "def load_and_train(model_name, n_classes, starting_epoch):\n",
    "    # size of the latent space\n",
    "    latent_dim = 100\n",
    "\n",
    "    gan_model, g_model, d_model = load()\n",
    "    \n",
    "    # load image data\n",
    "    dataset = load_real_samples(n_classes)\n",
    "#     dataset = augment_images(dataset)\n",
    "    \n",
    "    \n",
    "    # train model\n",
    "    train(g_model, d_model, gan_model, dataset, latent_dim, n_classes, starting_epoch=starting_epoch)\n",
    "    \n",
    "    \n",
    "\n",
    "n_classes = 3\n",
    "create_and_train(n_classes)\n",
    "# load_and_train(\"010\", n_classes, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVLLUIjU20wy"
   },
   "source": [
    "### ThoughtVIZ's  Baseline AC-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YwfGtq24GBZ"
   },
   "source": [
    "**Download model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ogKkn3n3Wus"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ThoughtViz/models\n",
    "!wget -q https://raw.githubusercontent.com/ptirupat/ThoughtViz/master/training/models/ac_gan.py -P ThoughtViz/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U47otpX3_V5"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-aC8Z_H2_OL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import image_utils\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "from ThoughtViz.models import ac_gan\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T3e-xtTX4t-"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "MODEL_DIR = '/tmp/imagenet'\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "softmax = None\n",
    "\n",
    "\n",
    "# Call this function with list of images. Each of elements should be a\n",
    "# numpy array with values ranging from 0 to 255.\n",
    "def get_inception_score(images, splits=10):\n",
    "  assert(type(images) == list)\n",
    "  assert(type(images[0]) == np.ndarray)\n",
    "  assert(len(images[0].shape) == 3)\n",
    "  assert(np.max(images[0]) > 10)\n",
    "  assert(np.min(images[0]) >= 0.0)\n",
    "  inps = []\n",
    "  for img in images:\n",
    "    img = img.astype(np.float32)\n",
    "    inps.append(np.expand_dims(img, 0))\n",
    "  bs = 1\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    preds = []\n",
    "    n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n",
    "    for i in range(n_batches):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n",
    "        inp = np.concatenate(inp, 0)\n",
    "        pred = sess.run(softmax, {'ExpandDims:0': inp})\n",
    "        preds.append(pred)\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "      part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
    "      kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "      kl = np.mean(np.sum(kl, 1))\n",
    "      scores.append(np.exp(kl))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "# This function is called automatically.\n",
    "def _init_inception():\n",
    "  global softmax\n",
    "  if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(MODEL_DIR, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  tarfile.open(filepath, 'r:gz').extractall(MODEL_DIR)\n",
    "  with tf.compat.v1.gfile.FastGFile(os.path.join(\n",
    "      MODEL_DIR, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "  # Works with an arbitrary minibatch size.\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    pool3 = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "    ops = pool3.graph.get_operations()\n",
    "    for op_idx, op in enumerate(ops):\n",
    "        for o in op.outputs:\n",
    "            shape = o.get_shape()\n",
    "            shape = [s for s in shape]\n",
    "            new_shape = []\n",
    "            for j, s in enumerate(shape):\n",
    "                if s == 1 and j == 0:\n",
    "                    new_shape.append(None)\n",
    "                else:\n",
    "                    new_shape.append(s)\n",
    "            o.set_shape(tf.TensorShape(new_shape))\n",
    "    w = sess.graph.get_operation_by_name(\"softmax/logits/MatMul\").inputs[1]\n",
    "    logits = tf.matmul(tf.squeeze(pool3, [1, 2]), w)\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "if softmax is None:\n",
    "  _init_inception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrAlxWDP4QuZ"
   },
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKNRqAFF4TEe"
   },
   "outputs": [],
   "source": [
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]\n",
    "\n",
    "def combine_rgb_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1], 3),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1], :] = img[:, :, :]\n",
    "    return image\n",
    "\n",
    "def load_data(num_classes):\n",
    "    images, labels = image_utils.load_images(path_root=\"\")\n",
    "    images, labels = trim_classes(images, labels, num_classes)\n",
    "    (x_train, y_train), (x_test, y_test) = image_utils.split_data(images, labels)\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "    x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def train_gan(input_noise_dim, batch_size, epochs, model_save_dir, output_dir, num_classes):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = load_data(num_classes)\n",
    "    print(y_train.shape)\n",
    "    adam_lr = 0.001\n",
    "    adam_beta_1 = 0.5\n",
    "\n",
    "    d = ac_gan.discriminator_model_rgb((64,64), num_classes)\n",
    "    d_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "    d.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=d_optim)\n",
    "    d.trainable = True\n",
    "\n",
    "    g = ac_gan.generator_model_rgb(input_noise_dim + num_classes)\n",
    "    g_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "    g.compile(loss='categorical_crossentropy', optimizer=g_optim)\n",
    "\n",
    "    d_on_g = ac_gan.generator_containing_discriminator(input_noise_dim + num_classes, g, d)\n",
    "    d_on_g.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=g_optim)\n",
    "\n",
    "    g.summary()\n",
    "    d.summary()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is \", epoch)\n",
    "\n",
    "        print(\"Number of batches\", int(x_train.shape[0]/batch_size))\n",
    "\n",
    "        for index in range(int(x_train.shape[0]/batch_size)):\n",
    "            # generate noise from a normal distribution\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "            random_labels = [randint(0, 9) for i in range(batch_size)]\n",
    "\n",
    "            one_hot_vectors = [to_categorical(label, 10) for label in random_labels]\n",
    "\n",
    "            conditioned_noise = []\n",
    "            for i in range(batch_size):\n",
    "                conditioned_noise.append(np.append(noise[i], one_hot_vectors[i]))\n",
    "            conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "            # get real images and corresponding labels\n",
    "            real_images = x_train[index * batch_size:(index + 1) * batch_size]\n",
    "            real_labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            # generate fake images using the generator\n",
    "            generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "            # discriminator loss of real images\n",
    "            d_loss_real = d.train_on_batch(real_images, [np.array([1] * batch_size), np.array(real_labels)])\n",
    "            # discriminator loss of fake images\n",
    "            d_loss_fake = d.train_on_batch(generated_images, [np.array([0] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
    "            d_loss = (d_loss_fake[0] + d_loss_real[0]) * 0.5\n",
    "\n",
    "            # save generated images at intermediate stages of training\n",
    "            if index % 250 == 0:\n",
    "                image = combine_rgb_images(generated_images)\n",
    "                image = image * 255.0\n",
    "                img_save_path = os.path.join(output_dir, str(epoch) + \"_g_\" + str(index) + \".png\")\n",
    "                Image.fromarray(image.astype(np.uint8)).save(img_save_path)\n",
    "\n",
    "            d.trainable = False\n",
    "            # generator loss\n",
    "            g_loss = d_on_g.train_on_batch(conditioned_noise, [np.array([1] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
    "            d.trainable = True\n",
    "\n",
    "        # test_image_count = 50000\n",
    "        test_image_count = x_train.shape[0]\n",
    "        test_noise = np.random.uniform(-1, 1, (test_image_count, input_noise_dim))\n",
    "        test_labels = [randint(0, 9) for i in range(test_image_count)]\n",
    "        one_hot_vectors_test = [to_categorical(label, 10) for label in test_labels]\n",
    "\n",
    "        conditioned_noise_test = []\n",
    "        for i in range(test_image_count):\n",
    "            conditioned_noise_test.append(np.append(test_noise[i], one_hot_vectors_test[i]))\n",
    "        conditioned_noise_test = np.array(conditioned_noise_test)\n",
    "\n",
    "        test_images = g.predict(conditioned_noise_test, verbose=0)\n",
    "        test_images = test_images * 255.0\n",
    "        \n",
    "        if epoch % 50 == 0:        \n",
    "            inception_score = get_inception_score([test_image for test_image in test_images], splits=10)\n",
    "\n",
    "        print(\"Epoch %d d_loss : %f\" % (epoch, d_loss))\n",
    "        print(\"Epoch %d g_loss : %f\" % (epoch, g_loss[0]))\n",
    "        print(\"Epoch %d inception_score : %f\" % (epoch, inception_score[0]))\n",
    "\n",
    "        # save generator and discriminator models along with the weights\n",
    "        g.save(os.path.join(model_save_dir, 'generator_' + str(epoch)), overwrite=True, include_optimizer=True)\n",
    "        d.save(os.path.join(model_save_dir, 'discriminator_' + str(epoch)), overwrite=True, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqtGoD-g4VtZ"
   },
   "source": [
    "**Train GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5zBRwzZ4cEw"
   },
   "outputs": [],
   "source": [
    "batch_size =50\n",
    "run_id = 1\n",
    "n_epochs = 500\n",
    "input_dim = 100\n",
    "n_classes=10\n",
    "\n",
    "model_save_dir = os.path.join('./saved_models/baseline_acgan/', 'run_' + str(run_id))\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "output_dir = os.path.join('./outputs/baseline_acgan/', 'run_' + str(run_id))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train_gan(input_dim, batch_size, n_epochs, model_save_dir, output_dir, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imWPfhDd9pca"
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "def resize_images(images, size=(28,28)):\n",
    "  temp = []\n",
    "  for img in images:\n",
    "    temp.append(np.array(Image.fromarray(img).resize(size, Image.ANTIALIAS)))\n",
    "  return np.asarray(temp)\n",
    "\n",
    "def trim_classes(images, labels, max_classes=10):\n",
    "  return images[0:900*max_classes], labels[0:900*max_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRZLkkFj9fDJ"
   },
   "outputs": [],
   "source": [
    "images, labels = image_utils.load_images()\n",
    "images = rgb2gray(images)\n",
    "print(images.shape)\n",
    "images = resize_images(images)\n",
    "print(images.shape)\n",
    "(x_train, y_train), (x_test, y_test) = image_utils.split_data(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXagMgJP5RqG"
   },
   "source": [
    "**Download Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQf2da6q5RIY"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/models/ThoughtViz\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/generator_100/saved_model.pb  /content/models/ThoughtViz/\n",
    "!mv /content/models/ThoughtViz/saved_model.pb /content/models/ThoughtViz/generator.pb\n",
    "!cp /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/discriminator_100/saved_model.pb  /content/models/ThoughtViz/\n",
    "!mv /content/models/ThoughtViz/saved_model.pb /content/models/ThoughtViz/discriminator.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AlRT0ZE9QO4"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/models/ThoughtViz\n",
    "!cp -r /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/generator_100 /content/models/ThoughtViz/\n",
    "!cp -r /content/gdrive/MyDrive/EEG2Image/models/ThoughtViz/baseline\\ acgan/run_1/discriminator_100 /content/models/ThoughtViz/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J8CDTsq6mtU"
   },
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlLpMWkz7u_6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WllfTTeJ_NI4"
   },
   "outputs": [],
   "source": [
    "K.set_learning_phase(False)\n",
    "\n",
    "input_noise_dim = 100\n",
    "batch_size = 50\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "conditioned_noise = []\n",
    "for i in range(batch_size):\n",
    "    conditioned_noise.append(np.append(noise[i], to_categorical(0, 10)))\n",
    "conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "g = load_model(\"/content/models/ThoughtViz/generator_100\")\n",
    "\n",
    "# generate images using the generator\n",
    "generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "image = combine_rgb_images(generated_images)\n",
    "image = image * 127.5 + 127.5\n",
    "plt.imshow(image)\n",
    "img = Image.fromarray(image.astype(np.uint8))\n",
    "img.show()\n",
    "img.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmZV825L9kv1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from layers.mog_layer import *\n",
    "from utils.image_utils import *\n",
    "\n",
    "\n",
    "class Tests():\n",
    "\n",
    "    def test_deligan_baseline(self, generator_model):\n",
    "        K.set_learning_phase(False)\n",
    "\n",
    "        input_noise_dim = 100\n",
    "        batch_size = 50\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "        random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "        conditioned_noise = []\n",
    "        for i in range(batch_size):\n",
    "            conditioned_noise.append(np.append(noise[i], to_categorical(random_labels[i], 10)))\n",
    "        conditioned_noise = np.array(conditioned_noise)\n",
    "\n",
    "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
    "\n",
    "        # generate images using the generator\n",
    "        generated_images = g.predict(conditioned_noise, verbose=0)\n",
    "\n",
    "        image = combine_rgb_images(generated_images)\n",
    "        image = image * 127.5 + 127.5\n",
    "        img = Image.fromarray(image.astype(np.uint8))\n",
    "        img.show()\n",
    "\n",
    "    def test_deligan_final(self, generator_model, classifier_model, eeg_pkl_file):\n",
    "        K.set_learning_phase(False)\n",
    "\n",
    "        # load EEG data\n",
    "        eeg_data = pickle.load(open(eeg_pkl_file, \"rb\"), encoding='bytes')\n",
    "        classifier = load_model(classifier_model)\n",
    "\n",
    "        x_test = eeg_data[b'x_test']\n",
    "        y_test = eeg_data[b'y_test']\n",
    "        y_test = [np.argmax(y) for y in y_test]\n",
    "        layer_index = 9\n",
    "\n",
    "        # keras way of getting the output from an intermediate layer\n",
    "        get_nth_layer_output = K.function([classifier.layers[0].input], [classifier.layers[layer_index].output])\n",
    "\n",
    "        layer_output = get_nth_layer_output([x_test])[0]\n",
    "\n",
    "        input_noise_dim = 100\n",
    "        batch_size = 50\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
    "\n",
    "        random_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "        eeg_feature_vectors = [layer_output[random.choice(np.where(y_test == random_label)[0])] for random_label in random_labels]\n",
    "\n",
    "        noises, conditionings = [], []\n",
    "        for i in range(batch_size):\n",
    "            noises.append(noise[i])\n",
    "            conditionings.append(eeg_feature_vectors[i])\n",
    "\n",
    "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
    "\n",
    "        # generate images using the generator\n",
    "        generated_images = g.predict([np.array(noises), np.array(conditionings)], verbose=0)\n",
    "\n",
    "        image = combine_rgb_images(generated_images)\n",
    "        image = image * 127.5 + 127.5\n",
    "        img = Image.fromarray(image.astype(np.uint8))\n",
    "        img.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tests = Tests()\n",
    "    #tests.test_deligan_baseline('../models/gan_models/baseline/deligan/image/generator.model')\n",
    "    tests.test_deligan_final('../models/gan_models/final/image/generator.model',\n",
    "                       '../models/eeg_models/image/run_final.h5',\n",
    "                       '../data/eeg/image/data.pkl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CSE_247_Project(2)(1)(1)(2).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
