
\documentclass[sigplan,screen]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\usepackage{subfigure}

\begin{document}

\title{EEG to Image Generation Experiment Report}


\author{Robert Sato}
\authornote{Both authors contributed equally to this research.}
\email{rssato@ucsc.edu}
\author{Alexander Cardaras}
\authornotemark[1]
\email{acardara.ucsc.edu}
\affiliation{%
  \institution{University of California, Santa Cruz}
  \streetaddress{1156, High St}
  \city{Santa Cruz}
  \state{California}
  \country{USA}
  \postcode{95064}
}

\begin{abstract}
  This work aims to study the effectiveness of using electroencephalograph (EEG) readings to generate images. We explore the latent sp
  This work aims to recreate images using electroencephalograph (EEG) readings as inputs. We explore the application of recent advancements in both EEG classification and image generation. Once an initial pipeline is established, we will explore its applications in a restricted data environment.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%%\begin{CCSXML}
%%<ccs2012>
%% <concept>
%%  <concept_id>10010520.10010553.10010562</concept_id>
%%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%%  <concept_significance>500</concept_significance>
%% </concept>
%%\end{CCSXML}

%%\ccsdesc[500]{Computer systems organization~Embedded systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{EEG, Generative Adversarial Networks, Deep Learning}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Recent works have leveraged deep learning models to categorize EEG signals evoked by sensory stimuli. These works utilize supervised learning to create an EEG-label feature map for conditioning a downstream generative model. It has been shown that with such a generative model it is possible to create images that are semantically coherent with the original stimuli. This architecture is typically broken down into two parts; a discriminator for EEG classification and a generator for image creation. Best practices for the implementation of both the discriminator and generator are under debate and image generation from EEG signals is a current research topic.

It has been shown in \cite{spampinato2019deep} that LSTMs can be used to effectively classify event-related potential amplitudes of visual stimuli recorded through EEG. The authors proposed three different image classification models and concluded that a simple LSTM coupled with two dense layers had superior classification accuracy. They attribute the lower accuracy of older, CNN based models to the temporal dynamics of the brain which contains fundamental information for understanding how thoughts develop and evolve. LSTMs have been shown to excel at modeling these time dependent systems due to their retention of information from previous time steps.

In Brain2Image \cite{10.1145/3123266.3127907}, Kavasidis et al. showed that both VAEs and GANs are capable of generating images from learned latent features of brain signals. In their work, they condition two separate generative networks using an intermediate layer of their discriminator, which is assumed to carry latent information about image features. They show qualitatively, that GANs are capable of generating sharper, higher quality images when compared to VAEs. Averaged over 40 classes, their generators produced an Inception Score of 4.49 and 5.07 for VAEs and GANs respectively. 

In an effort to create a generalized model for image generation from EEG signals, Tirupattur et al. proposed ThoughtViz \cite{10.1145/3240508.3240641}, a CNN based architecture that is capable of creating high quality images when trained on EEG signals of three different image datasets. They show that with no fine tuning of hyperparameters, a single model can learn to generate images of numbers, letters, and objects of higher quality than previously achieved. They found that applying 1D convolutions along the channel and time axes could increase classification accuracy of EEG signals. They showed quantitatively that conditioning generators with intermediate discriminator features as opposed to one-hot encoded class labels yields higher quality images. Their model achieved an average Inception score of 5.439 over 10 object classes and an image classification accuracy of over 92\% for numbers and letters.  


\section{Description of the Data}
Our EEG dataset was provided by PeRCeiVe Lab \cite{10.1145/3240508.3240641, 9097411}, where six subjects were shown visual stimuli of objects while their brain waves were being recorded. The images shown were from 40 ImageNet classes with 50 images per class. This totals 2,000 images with each image being shown for 0.5 seconds. The EEG readings were sampled at a 1KHz frequency giving 500 readings per image with a data resolution of 16 bits. To prevent overlap of brain cognition signals between images, the first 40 ms were discarded from each image showing. The EEG readings were filtered using a notch filter (49-51 Hz) and a second-order band-pass Butterworth filter. The low cutoff frequency of the band-pass filter was set to 14 Hz, while the high cutoff frequency was set to 71 Hz. 

To clean the data, we explored normalizing the EEG data in a couple of ways. On a per channel basis, we normalized the EEG signal values by dividing all the values by their mean and then passing them through a sigmoid activation function. We also tried reducing all values linearly to the range 0-1. Neither of these approaches gave us a better classification accuracy. Following \cite{Craik_2019}, the most effective approach for EEG classification on image cognition is by using the raw EEG data or converting it to power spectral density so we opted to stay with the raw EEG data.

While the data was not normalized, EEG readings restricted to the 55-95 Hz range were found to be the most effective, supporting what was found in \cite{10.1145/3123266.3127907}. We tested the classification accuracy using different restricted frequency bands and achieved similar results, thus we chose to use the 55-95 Hz range for the rest of our experiments. 

\section{Baseline Classifier and Image Generator}
Our pipeline consists of a feature extractor network applied to the input EEG signals and a generative model that takes the feature vector as input and outputs generated images. Therefore, we have one network trained to classify EEG readings into the correct class, and another network trained on generating realistic images. We then tested a number of different classifier networks. While \cite{10.1145/3240508.3240641} argues that the best network architecture consists of a single LSTM layer and two subsequent fully connected layers, we found that a CNN based model had better performance in agreement with \cite{spampinato2019deep}. 

This CNN based model is the same model constructed in the ThoughtViz \cite{spampinato2019deep} paper. It consists of 1D convolutions in the time axis, 1D convolutions across all channels, and 2 subsequent 2D convolution layers. The classification is done by 2 fully connected layers, the last one using a softmax activation function.

Throughout testing, we have experimented with VAE, GANs, conditional GANs, and auxiliary control GANs (AC-GANs) \cite{odena2017conditional}. While the project requires a condition vector be supplied to the GAN to incorporate the EEG feature vector, we are focusing on generating acceptable images before integrating the EEG feature vectors. The current AC-GAN implementation uses transposed convolution layers to upsample images drawn from a 100 latent vector uniform distribution. Following the tricks in \cite{radford2016unsupervised}, we arrived at an architecture similar to DCGAN. The generator uses ReLU activation function at each layer except the output layer which uses tanh. We are currently testing different kernel sizes and strides for the transposed convolution layers. The discriminator uses 2D convolution layers with a kernel size of 4x4 and strides of 2x2. Following the same paper \cite{radford2016unsupervised}, we use LeakyReLU with an alpha of 0.2, call batch normalization at each convolution layer, apply a 50\% dropout at each convolution layer, and initialize the convolution kernels drawing from a normal distribution with a standard deviation of 0.02. The discriminator uses binary cross-entropy for deciding between real and fake images. In parallel, it uses categorical cross-entropy for determining the class of the given image.

\section{Results}
\subsection{Architectural Tests}
In the early stages of this project, we stripped our baseline GAN of all of its components to see what each one did. The baseline GAN was derived from the DCGAN architecture. It consisted of 3 upsample and 3 downsample layers with increasing and decreasing channels per layer respectively. This GAN was unconditioned and trained on the ImageNet horse class at 64x64 resolution.

From nothing but convolution and transposed convolution layers, we add nonlinearity with ReLU, test different upsampling techniques, channel widths and depths, kernel sizes, and batch normalization. This section outlines the progression we took in experimenting with different GAN architectures. Further sections will introduce other techniques that were unrelated to model architecture or required their own section to explain.

\subsubsection{Nonlinearity}

Figure 1 shows the barebones model that will be added to.

%% base 4x4 model summary
\begin{figure}
\includegraphics[width=0.4\textwidth]{base_4x4_no_ReLU_summary.png}
\caption{Baseline model with no ReLU}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{base_3x3_no_ReLU.png}
\caption{Baseline GAN attempts at generating horses}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{base_3x3_no_ReLU_loss.png}
\caption{3x3 Loss}
\end{figure}

Before adding nonlinearity with ReLU, the output of the generator is complete garbage. The loss of the generator is considerably high while the discriminator loss is low. This shows that the discriminator easily differentiates between real and fake images during its training while the generator is unsuccessful in fooling the disciminator during its training.


Upon adding ReLU, we immediately see an improvement in image quality, as shown in Figure X

\begin{figure}
\includegraphics[width=0.4\textwidth]{base_3x3_w_ReLU_horses.png}
\caption{Baseline horses}
\end{figure}

\subsubsection{Upsampling Techniques}

Although most of our tests use transposed convolutions with a stride of 2x2 to upsample, we also tried upsampling using nearest neighbor and bilinear upsampling. We also tested different kernel sizes in the generator for the convolution immediately following the upsampling. These images are displayed in Figures X-Y.

\begin{figure}
\includegraphics[width=0.4\textwidth]{upsamp_bilinear_2x2_horses.png}
\caption{Bilinear interpolation followed by 2x2 kernel}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{upsamp_nn_2x2_horses.png}
\caption{Nearest neighbor interpolation followed by 2x2 kernel}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{upsamp_nn_3x3_horses.png}
\caption{Nearest neighbor interpolation followed by 3x3 kernel}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{upsamp_nn_5x5_horses.png}
\caption{Nearest neighbor interpolation followed by 5x5 kernel}
\end{figure}

\subsubsection{Width vs depth}
\subsubsection{Kernel size}
\subsubsection{Batch Normalization}

We consistently found that adding batch normalization ruined the GAN's performance. Figure X shows images generated using the baseline GAN (used for Figure Y) with batch normalization added between layers.

\begin{figure}
\includegraphics[width=0.4\textwidth]{batch_norm_3x3_images.png}
\caption{Adding batch normalization}
\end{figure}

We also found the addition of batch normalization to hurt GAN performance during our testing of the flowers dataset going from an FID of 142.8 before batch normalization to an FID of 178.0. Image quality is considerably worse and some mode collapse is present (Figure X, Y)

\begin{figure}
\includegraphics[width=0.4\textwidth]{before_bn_flowers_images.png}
\caption{Generated flowers before batch normalization}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{after_bn_flowers_images.png}
\caption{Generated flowers after batch normalization}
\end{figure}


\subsection{Learning Rate and Scheduling Tests}
\subsection{Noisy Label and Label Smoothing Tests}
\subsection{Class Label to Noise Channel Ratio Tests}
\subsection{Spectral Normalization}

Interpolation
Classification accuracy






%% Old stuff

\section{Preliminary Results}
\subsection{EEG Classification}
Initial EEG classification was done following the LSTM model used in Brain2Image \cite{10.1145/3240508.3240641}. This consisted of one LSTM layer and two full connected layers. This model was trained using the Adam optimizer for 20 epochs. We then trained it using the 5-95 Hz, 14-70 Hz, and 55-95 Hz frequency restricted data. The 55-95 Hz data performed the best as shown in Table 1. The highers accuracy model was found using the CNN architecture from ThoughtViz \cite{spampinato2019deep}.

\begin{table}
  \label{tab:freq}
  \caption{EEG Classification Accuracy}
  \caption*{\footnotesize All models were trained with a patience of 10 epochs.}
  \begin{tabular}{ccl}
    \toprule
    Frequency&Accuracy&Model\\
    \midrule
    5-95 Hz & 16\% & LSTM\\
    14-70 Hz & 20\% & LSTM\\
    55-95 Hz & 23\% & LSTM\\
    55-95 Hz & 31\% & CNN\\
  \bottomrule
\end{tabular}
\end{table}

Before implementing the CNN model, other approaches were tested to try and improve the LSTM model. These tests are shown in Table 2 with their results.
\begin{table}
  \label{tab:freq}
  \caption{Changes to LSTM Training}
  \caption*{\footnotesize The 55-95 Hz frequency range data was used during all of these experiments.}
  \begin{tabular}{cccl}
    \toprule
    Accuracy & Optimizer & Regularization & Normalization\\
    \midrule
    23\% & Adam & No & No\\
    22\% & Adam & No & Mean 0, Sigmoid\\
    24\% & SGD & No & Mean 0, Sigmoid\\
    2\% & Adam & No & Linear to 0-1\\
    25\% & Adam & L1=0.001 & No\\
    22\% & Adam & L1=0.0001 & No\\
    25\% & Adam & L2=0.01 & No\\
    26\% & Adam & L2=0.001 & No\\
    23\% & Adam & L2=0.0001 & No\\
  \bottomrule
\end{tabular}
\end{table}


\subsection{Image Generator}
Our first conditional GANs used a naive architecture which suffered from pattern repetition, low detail and occasional mode collapse. Figure 1 shows a generated set of images sampled from the latent space of the horse object class. As shown in the figure, these images suffer from a unique repetition of patterns that greatly reduces the quality of the image. We first focused on resolving this issue as it closely resembled a well documented problem in image generation using GANs, the checkerboard artifact \cite{odena2016deconvolution}. At its core, the checkerboard artifact is caused by uneven overlap of the transposed convolution kernel as it makes passes over features. By replacing all transposed convolution operations with 2D convolutions followed by up-sampling with bilinear interpolation we can essentially avoid this issue all together. 

Images generated with this updated model no longer suffered from the checkerboard artifact, but their overall quality dropped immensely. As shown in Figure 2, these images are unintelligible and no longer retain any semblance of the class they were conditioned on. Because of this, we reverted back to using transposed convolution for image upscaling and attempted to minimize the overlap of kernel sweeps by ensuring that the kernel size of the convolution is a factor of the stride. This patch by itself does not completely remove checkerboard artifacting, additionally, it has no effect on  mode collapse, so we also implemented some best practices mentioned in \cite{radford2016unsupervised} for building a GAN. These additions are detailed in the Baseline Classifier and Image Generator section above. The final baseline model produced images shown in Figure 3, which still suffer from artifacting, but more closely resemble the object class than previous GAN versions. 

\begin{figure}
\includegraphics[width=0.4\textwidth]{figure1b.png}
\caption{Horses generated with Conv2DTranspose up-pooling}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{figure1a.png}
\caption{Horses generated with Conv2D and bilinear up-sampling}
\end{figure}

\begin{figure}
\includegraphics[width=0.4\textwidth]{figure1c.png}
\caption{Horses generated with Conv2DTranspose up-pooling and DCGAN best practices}
\end{figure}


\section{Contributions}

\subsection{Robert's Contributions}
At the start of this project, the focus was data pre-processing and the EEG classifier. I wrote code for converting and saving data files, helper functions for reformatting images, and implemented the base LSTM. To improve this baseline, I tested different normalization techniques as detailed in the data pre-processing section. I added result analysis code including plots of loss, accuracy and confusion matrix calculation. While testing the LSTM on different normalization techniques, I also tested the different frequency restricted data (5-95 Hz, 14-70 Hz, 55-95 Hz), and other classifier models. I tested two LSTM models from the Brain2Image paper. 

Finally, I imported and tested the ThoughtViz classifier which we are using now. All of the results from these tests are shown in Table 1 and 2.
I also contributed to the GAN implementation. Alex and I have implemented and tested different GANs on our own. I explored a few different GAN examples including traditional GANs, conditional GANs and AC-GANs. To improve performance, I tested different upsampling techniques such as bilinear upsampling with 2D convolutions. Lastly, I wrote helper code to automatically generate a directory on each new GAN test run that saved generated images, model descriptions, loss plots, and model artifacts throughout the training process to streamline the testing of different GAN architectures, hyperparameters and other incremental changes.


\subsection{Alex's Contributions}
I began work on this project by implementing and verifying the results of both the Brain2Image and ThoughtViz papers. I reran each of their tests using the exact datasets and architecture design described in their works, using some of their provided source code. After some analysis I concluded that the results in the Brain2Image paper were not representative of their model's performance. As a result, we adopted the ThoughtViz architecture. I attempted to salvage the Brain2Image model, which was prone to overfitting by adding Gaussian noise to the training data, but the results from ThoughtViz were still superior. 

During this time, I also wrote some basic code for EEG visualization. While Robert worked on implementing and tuning the ThoughtViz architecture to run on our dataset, I acquired Images for training the generative part of our project. I wrote a web-scraper for downloading ImageNet images from the web and acquired the ImageNet64 dataset. I then focused on implementing a wide range of of image generators. I started with a VAE, modified it into a conditional VAE, and then began building GANs. VAEs were not mentioned in detail in this paper due to the fact that their results were lackluster and I quickly moved on to implementing GANs. Independent from Robert, I wrote my own GAN, conditional GAN and AC-GAN. 

I also wrote code for getting the inception score of generated images but haven't thoroughly benchmarked my generators with it yet. Instead, I focused on generating the highest quality image, through qualitative analysis, that I could. Similar to Robert, I tried bilinear upsampling with 2D convolutions on my models but did not observe and advantage from doing so. I implemented and tested a wide variety of techniques as described at the end of the Baseline Classifier and Image Generator section and many more from other papers. I finally settled with the good practices for building GANs highlighted in DCGAN \cite{radford2016unsupervised}. The final contributions I made to this project thus far are the tuning of hyperparameters of the final DCGAN inspired generator and some scripts and documentation to allow for the easy transition from google colab to PRP.  


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}


\end{document}
\endinput
%%
%% End of file `project-progress-report.tex'.

